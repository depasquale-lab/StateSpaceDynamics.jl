var documenterSearchIndex = {"docs":
[{"location":"MixtureModels/#Mixture-Models","page":"Mixture Models","title":"Mixture Models","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"CollapsedDocStrings = true","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"A mixture model is a probability distribution which, given a finite k  0, samples from k different distributions f_i(x)  i in 1k randomly, where the probability of sampling from f_i(x) is pi_i. Generally, a mixture model is written in the form of:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"f_mix(x Theta pi) = sum_k=1^K pi_k f_k(x)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where f_i(x) is called the ith component and pi_i is called the ith mixing coeffiecent.","category":"page"},{"location":"MixtureModels/#Gaussian-Mixture-Model","page":"Mixture Models","title":"Gaussian Mixture Model","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"GaussianMixtureModel\nGaussianMixtureModel(k::Int, data_dim::Int)\nfit!(gmm::GaussianMixtureModel, data::Matrix{<:Real}; maxiter::Int=50, tol::Float64=1e-3, initialize_kmeans::Bool=false)\nlog_likelihood(gmm::GaussianMixtureModel, data::Matrix{<:Real})\nsample(gmm::GaussianMixtureModel, n::Int)","category":"page"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel\n\nA Gaussian Mixture Model for clustering and density estimation.\n\nFields\n\nk::Int: Number of clusters.\nμₖ::Matrix{<:Real}: Means of each cluster (dimensions: data_dim x k).\nΣₖ::Array{Matrix{<:Real}, 1}: Covariance matrices of each cluster.\nπₖ::Vector{Float64}: Mixing coefficients for each cluster.\n\nExamples\n\ngmm = GaussianMixtureModel(3, 2) # Create a Gaussian Mixture Model with 3 clusters and 2-dimensional data\nfit!(gmm, data)\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel-Tuple{Int64, Int64}","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel(k::Int, data_dim::Int)\n\nConstructor for GaussianMixtureModel. Initializes Σₖ's covariance matrices to the  identity, πₖ to a uniform distribution, and μₖ's means to zeros.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(gmm::GaussianMixtureModel, data::Matrix{<:Real}; <keyword arguments>)\n\nFits a Gaussian Mixture Model (GMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\ngmm::GaussianMixtureModel: The Gaussian Mixture Model to be fitted.\ndata::Matrix{<:Real}: The dataset on which the model will be fitted, where each row represents a data point.\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the GMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\nExample\n\ndata = rand(2, 100)  # Generate some random data\ngmm = GaussianMixtureModel(k=3, d=2)  # Initialize a GMM with 3 components and 2-dimensional data\nclass_probabilities = fit!(gmm, data, maxiter=100, tol=1e-4, initialize_kmeans=true)\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.log_likelihood-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.log_likelihood","text":"log_likelihood(gmm::GaussianMixtureModel, data::Matrix{<:Real})\n\nCompute the log-likelihood of the data given the Gaussian Mixture Model (GMM). The data matrix should be of shape (# observations, # features).\n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.sample-Tuple{GaussianMixtureModel, Int64}","page":"Mixture Models","title":"StateSpaceDynamics.sample","text":"Draw 'n' samples from gmm. Returns a Matrix{<:Real}, where each row is a data point.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#Poisson-Mixture-Model","page":"Mixture Models","title":"Poisson Mixture Model","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"PoissonMixtureModel\nPoissonMixtureModel(k::Int)\nfit!(pmm::PoissonMixtureModel, data::Matrix{Int64}; maxiter::Int=50, tol::Float64=1e-3,initialize_kmeans::Bool=false)\nlog_likelihood(pmm::PoissonMixtureModel, data::Matrix{Int64})\nsample(pmm::PoissonMixtureModel, n::Int)","category":"page"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel\n\nA Poisson Mixture Model for clustering and density estimation.\n\nFields\n\nk::Int: Number of poisson-distributed clusters.\nλₖ::Vector{Float64}: Means of each cluster.\nπₖ::Vector{Float64}: Mixing coefficients for each cluster.\n\nExamples\n\njulia pmm = PoissonMixtureModel(3) # 3 clusters, 2-dimensional data fit!(pmm, data)\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel-Tuple{Int64}","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel(k::Int)\n\nConstructor for PoissonMixtureModel. Initializes λₖ's means to  ones and πₖ to a uniform distribution.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(pmm::PoissonMixtureModel, data::Matrix{Int}; <keyword arguments>)\n\nFits a Poisson Mixture Model (PMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\npmm::PoissonMixtureModel: The Poisson Mixture Model to be fitted.\ndata::Matrix{Int}: The dataset on which the model will be fitted, where each row represents a data point.\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the PMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\nExample\n\ndata = rand(1:10, 100, 1)  # Generate some random integer data\npmm = PoissonMixtureModel(k=3)  # Initialize a PMM with 3 components\nclass_probabilities = fit!(pmm, data, maxiter=100, tol=1e-4, initialize_kmeans=true)\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.log_likelihood-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.log_likelihood","text":"log_likelihood(pmm::PoissonMixtureModel, data::Matrix{Int})\n\nCompute the log-likelihood of the data given the Poisson Mixture Model (PMM). The data matrix should be of shape (# observations, # features).\n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.sample-Tuple{PoissonMixtureModel, Int64}","page":"Mixture Models","title":"StateSpaceDynamics.sample","text":"sample(pmm::PoissonMixtureModel, n)\n\nDraw 'n' samples from pmm. Returns a Vector{Int} of length n.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#Misc","page":"Miscellaneous","title":"Misc","text":"","category":"section"},{"location":"Misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Modules = [StateSpaceDynamics]\nPages   = [\n    \"GlobalTypes.jl\", \n    \"Preprocessing.jl\",\n    \"HMMConstructors.jl\",\n    \"Utilities.jl\"]","category":"page"},{"location":"Misc/#StateSpaceDynamics.FilterSmooth","page":"Miscellaneous","title":"StateSpaceDynamics.FilterSmooth","text":"\"     FilterSmooth{T<:Real}\n\nA mutable structure for storing smoothed estimates and associated covariance matrices in a filtering or smoothing algorithm.\n\nType Parameters\n\nT<:Real: The numerical type used for all fields (e.g., Float64, Float32).\n\nFields\n\nx_smooth::Matrix{T}   The matrix containing smoothed state estimates over time. Each column typically represents the state vector at a given time step.\np_smooth::Array{T, 3}   The posterior covariance matrices with dimensions (latentdim, latentdim, time_steps)\nE_z::Array{T, 3}   The expected latent states, size (statedim, T, ntrials).\nE_zz::Array{T, 4}   The expected value of zt * zt', size (statedim, statedim, T, n_trials).\nE_zz_prev::Array{T, 4}   The expected value of zt * z{t-1}', size (statedim, statedim, T, n_trials).\n\nExample\n\n```julia\n\nInitialize a FilterSmooth object with Float64 type\n\nfilter = FilterSmooth{Float64}(     xsmooth = zeros(10, 100),     psmooth = zeros(10, 10, 100),     Ez = zeros(10, 5, 100),     Ezz = zeros(10, 10, 5, 100),     Ezzprev = zeros(10, 10, 5, 100) )\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ForwardBackward","page":"Miscellaneous","title":"StateSpaceDynamics.ForwardBackward","text":"ForwardBackward{T<:Real}\n\nA mutable struct that encapsulates the forward–backward algorithm outputs for a hidden Markov model (HMM).\n\nFields\n\nloglikelihoods::Matrix{T}: Matrix of log-likelihoods for each observation and state.\nα::Matrix{T}: The forward probabilities (α) for each time step and state.\nβ::Matrix{T}: The backward probabilities (β) for each time step and state.\nγ::Matrix{T}: The state occupancy probabilities (γ) for each time step and state.\nξ::Array{T,3}: The pairwise state occupancy probabilities (ξ) for consecutive time steps and state pairs.\n\nTypically, α and β are computed by the forward–backward algorithm to find the likelihood of an observation sequence. γ and ξ are derived from these calculations to estimate how states transition over time.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ProbabilisticPCA","page":"Miscellaneous","title":"StateSpaceDynamics.ProbabilisticPCA","text":"mutable struct ProbabilisticPCA\n\nProbabilistic PCA model from Bishop's Pattern Recognition and Machine Learning.\n\nFields:\n\nW: Weight matrix that maps from latent space to data space.\nσ²: Noise variance\nμ: Mean of the data\nk: Number of latent dimensions\nD: Number of features\nz: Latent variables\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ProbabilisticPCA-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.ProbabilisticPCA","text":"ProbabilisticPCA(;W::Matrix{<:AbstractFloat}, σ²:: <: AbstractFloat, μ::Matrix{<:AbstractFloat}, k::Int, D::Int)\n\nConstructor for ProbabilisticPCA model.\n\n# Args:\n\n- W::Matrix{<:AbstractFloat}: Weight matrix that maps from latent space to data space.\n\n- σ²:: <: AbstractFloat: Noise variance\n\n- μ::Matrix{<:AbstractFloat}: Mean of the data\n\n- k::Int: Number of latent dimensions\n\n- D::Int: Number of features\n\n# Example:\n\n```julia\n\n# PPCA with unknown parameters\n\nppca = ProbabilisticPCA(k=1, D=2)\n\n# PPCA with known parameters\n\nppca = ProbabilisticPCA(W=rand(2, 1), σ²=0.1, μ=rand(2), k=1, D=2)\n\n```\n\n\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.E_Step-Tuple{ProbabilisticPCA, Matrix{<:Real}}","page":"Miscellaneous","title":"StateSpaceDynamics.E_Step","text":"E_Step(ppca::ProbabilisticPCA, X::Matrix{<:AbstractFloat})\n\nExpectation step of the EM algorithm for PPCA. See Bishop's Pattern Recognition and Machine Learning for more details.\n\nArgs:\n\nppca::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nE_Step(ppca, rand(10, 2))\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.M_Step!-Tuple{ProbabilisticPCA, Matrix{<:Real}, AbstractArray, AbstractArray}","page":"Miscellaneous","title":"StateSpaceDynamics.M_Step!","text":"M_Step!(model::ProbabilisticPCA, X::Matrix{<:AbstractFloat}, E_z::Matrix{<:AbstractFloat}, E_zz::Array{<:AbstractFloat, 3}\n\nMaximization step of the EM algorithm for PPCA. See Bishop's Pattern Recognition and Machine Learning for more details.\n\nArgs:\n\nmodel::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\nE_z::Matrix{<:AbstractFloat}: E[z]\nE_zz::Matrix{<:AbstractFloat}: E[zz']\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nE_z, E_zz = E_Step(ppca, rand(10, 2))\nM_Step!(ppca, rand(10, 2), E_z, E_zzᵀ)\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.fit!","page":"Miscellaneous","title":"StateSpaceDynamics.fit!","text":"fit!(model::ProbabilisticPCA, X::Matrix{<:AbstractFloat}, max_iter::Int=100, tol::AbstractFloat=1e-6)\n\nFit the PPCA model to the data using the EM algorithm.\n\nArgs:\n\nmodel::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\nmax_iter::Int: Maximum number of iterations\ntol::AbstractFloat: Tolerance for convergence\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nfit!(ppca, rand(10, 2))\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.loglikelihood-Tuple{ProbabilisticPCA, Matrix{<:Real}}","page":"Miscellaneous","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::ProbabilisticPCA, X::Matrix{<:AbstractFloat})\n\nCalculate the log-likelihood of the data given the PPCA model.\n\nArgs:\n\nmodel::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nloglikelihood(ppca, rand(10, 2))\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.GaussianHMM-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.GaussianHMM","text":"GaussianHMM(; K::Int, output_dim::Int, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Hidden Markov Model with Gaussian Emissions\n\nArguments\n\nK::Int: The number of hidden states\noutput_dim::Int: The dimensionality of the observation\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization)\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization)\n\nReturns\n\n::HiddenMarkovModel: Hidden Markov Model Object with Gaussian Emissions\n\n```\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingBernoulliRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingBernoulliRegression","text":"SwitchingBernoulliRegression(; K::Int, input_dim::Int, include_intercept::Bool=true, β::Vector{<:Real}=if include_intercept zeros(input_dim + 1) else zeros(input_dim) end, λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching Bernoulli Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input data.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model (defaults to true).\nβ::Vector{<:Real}: The regression coefficients (defaults to zeros). \nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization).\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Bernoulli Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingGaussianRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingGaussianRegression","text":"SwitchingGaussianRegression(; \n    K::Int,\n    input_dim::Int,\n    output_dim::Int,\n    include_intercept::Bool = true,\n    β::Matrix{<:Real} = if include_intercept\n        zeros(input_dim + 1, output_dim)\n    else\n        zeros(input_dim, output_dim)\n    end,\n    Σ::Matrix{<:Real} = Matrix{Float64}(I, output_dim, output_dim),\n    λ::Float64 = 0.0,\n    A::Matrix{<:Real} = initialize_transition_matrix(K),\n    πₖ::Vector{Float64} = initialize_state_distribution(K)\n)\n\nCreate a Switching Gaussian Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input features.\noutput_dim::Int: The dimensionality of the output predictions.\ninclude_intercept::Bool: Whether to include an intercept in the regression model (default is true).\nβ::Matrix{<:Real}: The regression coefficients (defaults to zeros based on input_dim and output_dim).\nΣ::Matrix{<:Real}: The covariance matrix of the Gaussian emissions (defaults to an identity matrix).\nλ::Float64: The regularization parameter for the regression (default is 0.0).\nA::Matrix{<:Real}: The transition matrix of the Hidden Markov Model (defaults to random initialization).\nπₖ::Vector{Float64}: The initial state distribution of the Hidden Markov Model (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Gaussian Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridgm-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Array{Matrix{T}, 1}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridgm","text":"block_tridgm(main_diag::Vector{Matrix{T}}, upper_diag::Vector{Matrix{T}}, lower_diag::Vector{Matrix{T}}) where {T<:Real}\n\nConstruct a block tridiagonal matrix from three vectors of matrices.\n\nArguments\n\nmain_diag::Vector{Matrix{T}}: Vector of matrices for the main diagonal.\nupper_diag::Vector{Matrix{T}}: Vector of matrices for the upper diagonal.\nlower_diag::Vector{Matrix{T}}: Vector of matrices for the lower diagonal.\n\nReturns\n\nA sparse matrix representing the block tridiagonal matrix.\n\nThrows\n\nErrorException if the lengths of upper_diag and lower_diag are not one less than the length of main_diag.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Array{Matrix{T}, 1}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse","text":"block_tridiagonal_inverse(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix.\n\nArguments\n\nA: Lower diagonal blocks.\nB: Main diagonal blocks.\nC: Upper diagonal blocks.\n\nReturns\n\nλii: Diagonal blocks of the inverse.\nλij: Off-diagonal blocks of the inverse.\n\nNotes: This implementation is from the paper:\n\n\"An Accelerated Lambda Iteration Method for Multilevel Radiative Transfer” Rybicki, G.B., and Hummer, D.G., Astronomy and Astrophysics, 245, 171–181 (1991), Appendix B.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse_static-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Array{Matrix{T}, 1}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse_static","text":"block_tridiagonal_inverse_static(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix using static matrices. See block_tridiagonal_inverse for details.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.euclidean_distance-Tuple{AbstractVector{Float64}, AbstractVector{Float64}}","page":"Miscellaneous","title":"StateSpaceDynamics.euclidean_distance","text":"euclidean_distance(a::AbstractVector{Float64}, b::AbstractVector{Float64})\n\nCalculate the Euclidean distance between two points.\n\nArguments\n\na::AbstractVector{Float64}: The first point.\nb::AbstractVector{Float64}: The second point.\n\nReturns\n\nThe Euclidean distance between a and b.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.gaussian_entropy-Union{Tuple{LinearAlgebra.Symmetric{T, S} where S<:(AbstractMatrix{<:T})}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.gaussian_entropy","text":"gaussian_entropy(H::Symmetric{T}) where T <: Real\n\nCalculate the entropy of a Gaussian distribution with Hessian (i.e. negative precision) matrix H.\n\nArguments\n\nH::Symmetric{T}: The Hessian matrix.\n\nReturns\n\nThe entropy of the Gaussian distribution.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::Matrix{<:Real}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6)\n\nPerform K-means clustering on the input data.\n\nArguments\n\ndata::Matrix{<:Real}: The input data matrix where each row is a data point.\nk_means::Int: The number of clusters.\nmax_iters::Int=100: Maximum number of iterations.\ntol::Float64=1e-6: Convergence tolerance.\n\nReturns\n\nA tuple containing the final centroids and cluster labels for each data point.\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering-2","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::Vector{Float64}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6)\n\nPerform K-means clustering on vector data.\n\nArguments\n\ndata::Vector{Float64}: The input data vector.\nk_means::Int: The number of clusters.\nmax_iters::Int=100: Maximum number of iterations.\ntol::Float64=1e-6: Convergence tolerance.\n\nReturns\n\nA tuple containing the final centroids and cluster labels for each data point.\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Tuple{Matrix{<:Real}, Int64}","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::Matrix{<:Real}, k_means::Int)\n\nPerform K-means++ initialization for cluster centroids.\n\nArguments\n\ndata::Matrix{<:Real}: The input data matrix where each row is a data point.\nk_means::Int: The number of clusters.\n\nReturns\n\nA matrix of initial centroids for K-means clustering.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Tuple{Vector{Float64}, Int64}","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::Vector{Float64}, k_means::Int)\n\nPerform K-means++ initialization for cluster centroids on vector data.\n\nArguments\n\ndata::Vector{Float64}: The input data vector.\nk_means::Int: The number of clusters.\n\nReturns\n\nA matrix of initial centroids for K-means clustering.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.logistic-Tuple{Real}","page":"Miscellaneous","title":"StateSpaceDynamics.logistic","text":"logistic(x::Real)\n\nCalculate the logistic function in a numerically stable way.\n\nArguments\n\nx::Real: The input value.\n\nReturns\n\nThe result of the logistic function applied to x.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.make_posdef!-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.make_posdef!","text":"make_posdef!(A::Matrix{T}) where {T}\n\nEnsure that a matrix is positive definite by adjusting its eigenvalues.\n\nArguments\n\nA::Matrix{T}: The input matrix.\n\nReturns\n\nA positive definite matrix derived from A.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.row_matrix-Tuple{AbstractVector}","page":"Miscellaneous","title":"StateSpaceDynamics.row_matrix","text":"row_matrix(x::AbstractVector)\n\nConvert a vector to a row matrix.\n\nArguments\n\nx::AbstractVector: The input vector.\n\nReturns\n\nA row matrix (1 × n) containing the elements of x.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.stabilize_covariance_matrix-Tuple{Matrix{<:Real}}","page":"Miscellaneous","title":"StateSpaceDynamics.stabilize_covariance_matrix","text":"stabilize_covariance_matrix(Σ::Matrix{<:Real})\n\nStabilize a covariance matrix by ensuring it is symmetric and positive definite.\n\nArguments\n\nΣ::Matrix{<:Real}: The input covariance matrix.\n\nReturns\n\nA stabilized version of the input covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#What-is-a-Hidden-Markov-Model?","page":"Hidden Markov Models","title":"What is a Hidden Markov Model?","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model (HMM) is a graphical model that describes how systems change over time. When modeling a time series with T observations using an HMM, we assume that the observed data y_1T depends on hidden states x_1T that are not observed. Specifically, an HMM is a type of state-space model in which the hidden states are discrete.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The three components of an HMM are as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"An initial state distribution (pi): which hidden states we are likely to start in.\nA transition matrix (A): how the hidden states evolve over time.\nAn emission model: how the hidden states generate the observed data.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is given by:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"$","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x1 \\sim \\text{Cat}(\\pi) \\\nxt \\mid x{t-1} \\sim \\text{Cat}(A{x{t-1}, :}) \\\nyt \\mid xt \\sim p(yt \\mid \\theta{xt}) $","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t\nis the hidden (discrete) state at time t\ny_t\nis the observed data at time t\npi\nis the initial state distribution\nA\nis the state transition matrix\ntheta_x_t\nare the parameters of the emission distribution for state x_t","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The emission model can take many forms: Gaussian, Poisson, Bernoulli, categorical, etc... In the case of a Gaussian emission distribution, this becomes:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"$","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"yt \\mid (xt = k) \\sim \\mathcal{N}(\\muk, \\Sigmak) $","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"mu_k\nis the mean of the emission distribution for state k\nSigma_k\nis the covariance of the emission distribution for state k","category":"page"},{"location":"HiddenMarkovModels/#What-is-a-Generalized-Linear-Model-Hidden-Markov-Model","page":"Hidden Markov Models","title":"What is a Generalized Linear Model - Hidden Markov Model","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model - Generalized Linear Model (GLM-HMM) - also known as Switching Regression Model - is an extension to classic HMMs where the emission models are state-dependent GLMs that link an observed input to an observed output. This formulation allows each hidden state to define its own regression relationship between inputs and outputs, enabling the model to capture complex, state-dependent dynamics in the data. Currently, StateSpaceDynamics.jl support Gaussian, Bernoulli, Poisson, and Autoregressive GLMs as emission models.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"$","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x1 \\sim \\text{Cat}(\\pi) \\\nxt \\mid x{t-1} \\sim \\text{Cat}(A{x{t-1}, :}) \\\nyt \\mid xt, ut \\sim p(yt \\mid \\theta{xt}, ut) $","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t\nis the hidden (discrete) state at time t\ny_t\nis the observed output at time t\nu_t\nis the observed input (covariate) at time t\ntheta_x_t\nare the parameters of the GLM emission model for state x_t","category":"page"},{"location":"HiddenMarkovModels/#Example-Emission-Models","page":"Hidden Markov Models","title":"Example Emission Models","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For example, if the emission is a Gaussian GLM:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"$","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"yt \\mid (xt = k), ut \\sim \\mathcal{N}(\\muk + \\betak^\\top ut, \\sigma_k^2) $","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k\nare the regression weights for state k\nsigma_k^2\nis the state-dependent variance\nmu_k\nis the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"If the emission is Bernoulli (for binary outputs):","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"$","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"yt \\mid (xt = k), ut \\sim \\text{Bernoulli} \\left( \\sigma \\left( \\muk + \\betak^\\top ut \\right) \\right) $","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k\nare the regression weights for state k\nsigma(cdot)\nis the logistic sigmoid function for binary outputs\nmu_k\nis the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/#Learning-in-an-HMM","page":"Hidden Markov Models","title":"Learning in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"StateSpaceDynamics.jl implements Expectation-Maximization (EM) for parameter learning in both HMMs and GLM-HMMs. EM is an iterative method for finding maximum likelihood estimates of the parameters in graphical models with hidden variables. ","category":"page"},{"location":"HiddenMarkovModels/#Expectation-Step-(E-step)","page":"Hidden Markov Models","title":"Expectation Step (E-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the expectation step (E-step), we calculate the posterior distribution of the latent states given the current parameters of the model:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"$","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"p(X \\mid Y, \\theta_{\\text{old}}) $","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"We use dynamic programming to efficiently calculate this posterior using the forward and backward recursions for HMMs. This posterior is then used to construct the expectation of the complete data log-likelihood, also known as the Q-function:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"$","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Q(\\theta, \\theta{\\text{old}}) = \\sumX p(X \\mid Y, \\theta_{\\text{old}}) \\ln p(Y, X \\mid \\theta) $","category":"page"},{"location":"HiddenMarkovModels/#Maximization-Step-(M-step)","page":"Hidden Markov Models","title":"Maximization Step (M-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the maximization step (M-step), we maximize this expectation with respect to the parameters theta. Specifically:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For the initial state distribution and the transition matrix, we use analytical updates for the parameters, derived using Lagrange multipliers.\nFor emission models in the case of HMMs, we also implement analytical updates.\nIf the emission model is a GLM, we use Optim.jl to numerically optimize the objective function.","category":"page"},{"location":"HiddenMarkovModels/#Inference-in-an-HMM","page":"Hidden Markov Models","title":"Inference in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For state inference in Hidden Markov Models (HMMs), we implement two common algorithms:","category":"page"},{"location":"HiddenMarkovModels/#Forward-Backward-Algorithm","page":"Hidden Markov Models","title":"Forward-Backward Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Forward-Backward algorithm is used to compute the posterior state probabilities at each time step. Given the observed data, it calculates the probability of being in each possible hidden state at each time step, marginalizing over all possible state sequences.","category":"page"},{"location":"HiddenMarkovModels/#Viterbi-Algorithm","page":"Hidden Markov Models","title":"Viterbi Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Viterbi algorithm is used for best state sequence labeling. It finds the most likely sequence of hidden states given the observed data. This is done by dynamically computing the highest probability path through the state space, which maximizes the likelihood of the observed sequence.","category":"page"},{"location":"HiddenMarkovModels/#Reference","page":"Hidden Markov Models","title":"Reference","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For a complete mathematical formulation of the relevant HMM and HMM-GLM learning and inference algorithms, we recommend Pattern Recognition and Machine Learning, Chapter 13 by Christopher Bishop.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Modules = [StateSpaceDynamics]\nPages   = [\"HiddenMarkovModels.jl\"]","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.HiddenMarkovModel","page":"Hidden Markov Models","title":"StateSpaceDynamics.HiddenMarkovModel","text":"HiddenMarkovModel\n\nA Hidden Markov Model (HMM) with custom emissions.\n\nFields\n\nK::Int: Number of states.\nB::Vector=Vector(): Vector of emission models.\nemission=nothing: If B is missing emissions, clones of this model will be used to fill in the rest.\nA::Matrix{<:Real}: Transition matrix.\nπₖ::Vector{Float64}: Initial state distribution.\n\n\n\n\n\n","category":"type"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.class_probabilities","page":"Hidden Markov Models","title":"StateSpaceDynamics.class_probabilities","text":"function class_probabilities(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nCalculate the class probabilities at each time point using forward backward algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nclass_probabilities::Matrix{Float64}: The class probabilities at each timepoint\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.class_probabilities-2","page":"Hidden Markov Models","title":"StateSpaceDynamics.class_probabilities","text":"function class_probabilities(model::HiddenMarkovModel, Y::Vector{<:Matrix{<:Real}}, X::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing;)\n\nCalculate the class probabilities at each time point using forward backward algorithm on multiple trials of data\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Vectpr{<:Matrix{<:Real}}: The trials of emission data\nX::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing: Optional trials of input data for fitting Switching Regression Models\n\nReturns\n\nclass_probabilities::Vector{<:Matrix{Float64}}: Each trial's class probabilities at each timepoint\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.fit!","page":"Hidden Markov Models","title":"StateSpaceDynamics.fit!","text":"fit!(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real}, Nothing}=nothing; max_iters::Int=100, tol::Float64=1e-6)\n\nFit the Hidden Markov Model to multiple trials of data using the EM algorithm.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Vector{<:Matrix{<:Real}}: The trialized emission data.\nX::Union{Vector{<:Matrix{<:Real}}, Nothing}=nothing: Optional input data for fitting Switching Regression Models\nmax_iters::Int=100: The maximum number of iterations to run the EM algorithm.\ntol::Float64=1e-6: When the log likelihood is improving by less than this value, the algorithm will stop.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.fit!-2","page":"Hidden Markov Models","title":"StateSpaceDynamics.fit!","text":"fit!(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real}, Nothing}=nothing; max_iters::Int=100, tol::Float64=1e-6)\n\nFit the Hidden Markov Model using the EM algorithm.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data.\nX::Union{Matrix{<:Real}, Nothing}=nothing: Optional input data for fitting Switching Regression Models\nmax_iters::Int=100: The maximum number of iterations to run the EM algorithm.\ntol::Float64=1e-6: When the log likelihood is improving by less than this value, the algorithm will stop.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.sample","page":"Hidden Markov Models","title":"StateSpaceDynamics.sample","text":"sample(model::HiddenMarkovModel, data...; n::Int)\n\nGenerate n samples from a Hidden Markov Model. Returns a tuple of the state sequence and the observation sequence.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to sample from.\ndata...: The data to fit the Hidden Markov Model. Requires the same format as the emission model.\nn::Int: The number of samples to generate.\n\nReturns\n\nstate_sequence::Vector{Int}: The state sequence, where each element is an integer 1:K.\nobservation_sequence::Matrix{Float64}: The observation sequence. This takes the form of the emission model's output.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.viterbi","page":"Hidden Markov Models","title":"StateSpaceDynamics.viterbi","text":"viterbi(model::HiddenMarkovModel, Y::Vector{<:Matrix{<:Real}}, X::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing;)\n\nGet most likely class labels using the Viterbi algorithm for multiple trials of data\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Vectpr{<:Matrix{<:Real}}: The trials of emission data\nX::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing: Optional trials of input data for fitting Switching Regression Models\n\nReturns\n\nbest_path::Vector{<:Vector{Float64}}: Each trial's best state path\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.viterbi-2","page":"Hidden Markov Models","title":"StateSpaceDynamics.viterbi","text":"viterbi(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nGet most likely class labels using the Viterbi algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nbest_path::Vector{Float64}: The most likely state label at each timepoint\n\n\n\n\n\n","category":"function"},{"location":"SLDS/#Swiching-Linear-Dynamical-Systems","page":"Switching Linear Dynamical Systems","title":"Swiching Linear Dynamical Systems","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Modules = [StateSpaceDynamics]\nPages = [\"SLDS.jl\"]","category":"page"},{"location":"SLDS/#StateSpaceDynamics.SwitchingLinearDynamicalSystem","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.SwitchingLinearDynamicalSystem","text":"Switching Linear Dynamical System\n\n\n\n\n\n","category":"type"},{"location":"SLDS/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{SwitchingLinearDynamicalSystem, Matrix{T}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(slds::SwitchingLinearDynamicalSystem, y::Matrix{T}; \n     max_iter::Int=1000, \n     tol::Real=1e-12, \n     ) where {T<:Real}\n\nFit a Switching Linear Dynamical System using the variational Expectation-Maximization (EM) algorithm with Kalman smoothing.\n\nArguments\n\nslds::SwitchingLinearDynamicalSystem: The Switching Linear Dynamical System to be fitted.\ny::Matrix{T}: Observed data, size (obsdim, Tsteps).\n\nKeyword Arguments\n\nmax_iter::Int=1000: Maximum number of EM iterations.\ntol::Real=1e-12: Convergence tolerance for log-likelihood change.\n\nReturns\n\nmls::Vector{T}: Vector of log-likelihood values for each iteration.\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.hmm_elbo-Tuple{SwitchingLinearDynamicalSystem, StateSpaceDynamics.ForwardBackward}","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.hmm_elbo","text":"\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.initialize_slds-Tuple{}","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.initialize_slds","text":"Initialize a Switching Linear Dynamical System with random parameters.\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.mstep!-Union{Tuple{T}, Tuple{SwitchingLinearDynamicalSystem, Array{StateSpaceDynamics.FilterSmooth{T}, 1}, Matrix{T}, StateSpaceDynamics.ForwardBackward}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.mstep!","text":"\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.sample-Tuple{SwitchingLinearDynamicalSystem, Int64}","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.sample","text":"Generate synthetic data with switching LDS models\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.variational_expectation!-Union{Tuple{T}, Tuple{SwitchingLinearDynamicalSystem, Any, StateSpaceDynamics.ForwardBackward, Array{StateSpaceDynamics.FilterSmooth{T}, 1}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.variational_expectation!","text":"variational_expectation!(model::SwitchingLinearDynamicalSystem, y, FB, FS) -> Float64\n\nCompute the variational expectation (Evidence Lower Bound, ELBO) for a Switching Linear Dynamical System.\n\nArguments\n\nmodel::SwitchingLinearDynamicalSystem:   The switching linear dynamical system model containing parameters such as the number of regimes (K), system matrices (B), and observation models.\ny:   The observation data, typically a matrix where each column represents an observation at a specific time step.\nFB:   The forward-backward object that holds variables related to the forward and backward passes, including responsibilities (γ).\nFS:   An array of FilterSmooth objects, one for each regime, storing smoothed state estimates and covariances.\n\nReturns\n\nFloat64:   The total Evidence Lower Bound (ELBO) computed over all regimes and observations.\n\nDescription\n\nThis function performs the variational expectation step for a Switching Linear Dynamical System by executing the following operations:\n\nExtract Responsibilities:   Retrieves the responsibilities (γ) from the forward-backward object and computes their exponentials (hs).\nParallel Smoothing and Sufficient Statistics Calculation:   For each regime k from 1 to model.K, the function:\nPerforms smoothing using the smooth function to obtain smoothed states (x_smooth), covariances (p_smooth), inverse off-diagonal terms, and total entropy.\nComputes sufficient statistics (E_z, E_zz, E_zz_prev) from the smoothed estimates.\nCalculates the ELBO contribution for the current regime and accumulates it into ml_total.\nUpdate Variational Distributions:  \nComputes the variational distributions (qs) from the smoothed states, which are stored as log-likelihoods in FB.\nExecutes the forward and backward passes to update the responsibilities (γ) based on the new qs.\nRecalculates the responsibilities (γ) to reflect the updated variational distributions.\nReturn ELBO:   Returns the accumulated ELBO (ml_total), which quantifies the quality of the variational approximation.\n\nExample\n\n```julia\n\nAssume model is an instance of SwitchingLinearDynamicalSystem with K regimes\n\ny is the observation matrix of size (numfeatures, numtime_steps)\n\nFB is a pre-initialized ForwardBackward object\n\nFS is an array of FilterSmooth objects, one for each regime\n\nelbo = variational_expectation!(model, y, FB, FS) println(\"Computed ELBO: \", elbo)\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.variational_qs!-Union{Tuple{T}, Tuple{Array{StateSpaceDynamics.GaussianObservationModel{T}, 1}, StateSpaceDynamics.ForwardBackward, Any, Array{StateSpaceDynamics.FilterSmooth{T}, 1}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.variational_qs!","text":"variational_qs!(model::Vector{GaussianObservationModel{T}}, FB, y, FS) where {T<:Real}\n\nCompute the variational distributions (qs) and update the log-likelihoods for a set of Gaussian observation models within a Forward-Backward framework.\n\nArguments\n\nmodel::Vector{GaussianObservationModel{T}}   A vector of Gaussian observation models, where each model defines the parameters for a specific regime or state in a Switching Linear Dynamical System. Each GaussianObservationModel should contain fields such as the observation matrix C and the observation noise covariance R.\nFB   The Forward-Backward object that holds variables related to the forward and backward passes of the algorithm. It must contain a mutable field loglikelihoods, which is a matrix where each entry loglikelihoods[k, t] corresponds to the log-likelihood of the observation at time t under regime k.\ny   The observation data matrix, where each column represents an observation vector at a specific time step. The dimensions are typically (num_features, num_time_steps).\nFS   An array of FilterSmooth objects, one for each regime, that store smoothed state estimates (x_smooth) and their covariances (p_smooth). These are used to compute the expected sufficient statistics needed for updating the variational distributions.\n\nReturns\n\nNothing   The function performs in-place updates on the FB.loglikelihoods matrix. It does not return any value.\n\nDescription\n\nvariational_qs! updates the log-likelihoods for each Gaussian observation model across all time steps based on the current smoothed state estimates. This is a critical step in variational inference algorithms for Switching Linear Dynamical Systems, where the goal is to approximate the posterior distributions over latent variables.\n\nExample\n\n```julia\n\nDefine Gaussian observation models for each regime\n\nmodel = [     GaussianObservationModel(C = randn(5, 10), R = Matrix{Float64}(I, 5, 5)),     GaussianObservationModel(C = randn(5, 10), R = Matrix{Float64}(I, 5, 5)) ]\n\nInitialize ForwardBackward object with a preallocated loglikelihoods matrix\n\nFB = ForwardBackward(loglikelihoods = zeros(Float64, length(model), 100))\n\nGenerate synthetic observation data (5 features, 100 time steps)\n\ny = randn(5, 100)\n\nInitialize FilterSmooth objects for each regime\n\nFS = [     initialize_FilterSmooth(model[k], size(y, 2)) for k in 1:length(model) ]\n\nCompute variational distributions and update log-likelihoods\n\nvariational_qs!(model, FB, y, FS)\n\nAccess the updated log-likelihoods\n\nprintln(FB.loglikelihoods)\n\n\n\n\n\n","category":"method"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl is a comprehensive Julia package for state space modeling, designed specifically with neuroscientific applications in mind. The package provides efficient implementations of various state space models along with tools for parameter estimation, state inference, and model selection.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install StateSpaceDynamics.jl, start up Julia and type the following code-snipped into the REPL. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"StateSpaceDynamics\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"or alternatively, you can enter the package manager by typing ] and then run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add StateSpaceDynamics","category":"page"},{"location":"#What-are-State-Space-Models?","page":"Home","title":"What are State Space Models?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"State space models are a class of probabilistic models that describe the evolution of a system through two main components - a latent and observation process. The latent process is a stochastic process that is not directly observed, but is used to generate the observed data. The observation process is a conditional distribution that describes how the observed data is generated from the latent process.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In their most general form, state space models can be written as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim p(x_t+1  x_t) \n    y_t sim p(y_t  x_t)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x_t is the latent state at time t and y_t is the observed data at time t.","category":"page"},{"location":"#Example:-Linear-Dynamical-Systems","page":"Home","title":"Example: Linear Dynamical Systems","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A fundamental example is the Linear Dynamical System (LDS), which combines linear dynamics with Gaussian noise. The LDS can be expressed in two equivalent forms:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Equation form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 = A x_t + b + epsilon_t \n    y_t = C x_t + d + delta_t\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfA is the state transition matrix\nmathbfC is the observation matrix  \nmathbfb and mathbfd are bias terms\nboldsymbolepsilon_t and boldsymboldelta_t are Gaussian noise terms with covariances mathbfQ and mathbfR respectively","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distributional form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim mathcalN(A x_t + b Q) \n    y_t sim mathcalN(C x_t + d R)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where Q and R are the state and observation noise covariance matrices, respectively.","category":"page"},{"location":"#Models-Implemented","page":"Home","title":"Models Implemented","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl implements several types of state space models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Linear Dynamical Systems (LDS)\nGaussian LDS\nPoisson LDS\nHidden Markov Models (HMM)\nGaussian emissions\nRegression-based emissions\nGaussian regression\nBernoulli regression\nPoisson regression\nAutoregressive emissions","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a simple example using a Linear Dynamical System:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StateSpaceDynamics\n\n# Create a Gaussian LDS\nlds = GaussianLDS(\n    latent_dim=3,    # 3D latent state\n    obs_dim=10       # 10D observations\n)\n\n# Generate synthetic data\nx, y = sample(lds, 1000)  # 1000 timepoints\n\n# Fit the model\nfit!(lds, y)\n\n# Get smoothed state estimates\nx_smoothed = smooth(lds, y)","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you encounter a bug or would like to contribute to the package, come find us on Github.","category":"page"},{"location":"","page":"Home","title":"Home","text":"rsenne/ssm_julia","category":"page"},{"location":"#Citing-StateSpaceDynamics.jl","page":"Home","title":"Citing StateSpaceDynamics.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use StateSpaceDynamics.jl in your research, please cite the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"[Citation information to be added upon publication]","category":"page"},{"location":"EmissionModels/#Emission-Models","page":"EmissionModels","title":"Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"CurrentModule = StateSpaceDynamics","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The StateSpaceDynamics.jl package provides several emission models for state space modeling. These models define how observations are generated from latent states.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Modules = [StateSpaceDynamics]\nPages = [\"EmissionModels.jl\"]","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.AutoRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.AutoRegressionEmission","text":"AutoRegressionEmission <: EmissionModel\n\nA mutable struct representing an autoregressive emission model, which wraps around an AutoRegression model.\n\nFields\n\ninner_model::AutoRegression: The underlying autoregressive model used for the emissions.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.BernoulliRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.BernoulliRegressionEmission","text":"BernoulliRegressionEmission\n\nA Bernoulli regression model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\ninclude_intercept::Bool = true: Whether to include an intercept term.\nβ::Vector{<:Real} = if include_intercept zeros(input_dim + 1) else zeros(input_dim) end: Coefficients of the model. The first element is the intercept term, if included.\nλ::Float64 = 0.0: Regularization parameter.\n\n```\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianEmission","text":"mutable struct GaussianEmission <: EmissionModel\n\nGaussianEmission model with mean and covariance.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianEmission-Tuple{}","page":"EmissionModels","title":"StateSpaceDynamics.GaussianEmission","text":"function GaussianEmission(; output_dim::Int, μ::Vector{<:Real}=zeros(output_dim), Σ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim))\n\nFuncton to create a GaussianEmission with given output dimension, mean, and covariance.\n\nArguments\n\noutput_dim::Int: The output dimension of the emission\nμ::Vector{<:Real}=zeros(output_dim): The mean of the Gaussian\nΣ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim)): The covariance matrix of the Gaussian\n\nReturns\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianRegressionEmission","text":"GaussianRegressionEmission\n\nA Gaussian regression Emission model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\noutput_dim::Int: Dimension of the output data.\ninclude_intercept::Bool = true: Whether to include an intercept term; if true, the first column of β is assumed to be the intercept/bias.\nβ::Matrix{<:Real} = if include_intercept zeros(input_dim + 1, output_dim) else zeros(input_dim, output_dim) end: Coefficient matrix of the model. Shape inputdim by outputdim. The first row are the intercept terms, if included.\nΣ::Matrix{<:Real} = Matrix{Float64}(I, output_dim, output_dim): Covariance matrix of the model.\nλ::Float64 = 0.0: Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.PoissonRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.PoissonRegressionEmission","text":"PoissonRegressionEmission\n\nA Poisson regression model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\ninclude_intercept::Bool = true: Whether to include an intercept term.\nβ::Vector{<:Real} = if include_intercept zeros(input_dim + 1) else zeros(input_dim) end: Coefficients of the model. The first element is the intercept term, if included.\nλ::Float64 = 0.0: Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.RegressionOptimization","page":"EmissionModels","title":"StateSpaceDynamics.RegressionOptimization","text":"RegressionOptimization{T<:RegressionEmission}\n\nHolds the optimization problem data for regression emissions.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.SwitchingAutoRegression-Tuple{}","page":"EmissionModels","title":"StateSpaceDynamics.SwitchingAutoRegression","text":"SwitchingAutoRegression(; K::Int, output_dim::Int, order::Int, include_intercept::Bool=true, β::Matrix{<:Real}=if include_intercept zeros(output_dim * order + 1, output_dim) else zeros(output_dim * order, output_dim) end, Σ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim), λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching AutoRegression Model\n\nArguments\n\nK::Int: The number of hidden states.\noutput_dim::Int: The dimensionality of the output data.\norder::Int: The order of the autoregressive model.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model.\nβ::Matrix{<:Real}: The autoregressive coefficients (defaults to zeros).\nΣ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim): The covariance matrix for the autoregressive model (defaults to an identity matrix).\nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (Defaults to a random initialization). \nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (Defaults to a random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching AutoRegression Model\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.calc_regularization","page":"EmissionModels","title":"StateSpaceDynamics.calc_regularization","text":"calc_regularization(β::Matrix{<:Real}, λ::Float64, include_intercept::Bool)\n\nCalculate L2 regularization term for regression coefficients.\n\nArguments\n\nβ::Matrix{<:Real}: Coefficient matrix\nλ::Float64: Regularization parameter\ninclude_intercept::Bool: Whether to exclude the intercept term from regularization\n\nReturns\n\nFloat64: The regularization term value\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.calc_regularization_gradient","page":"EmissionModels","title":"StateSpaceDynamics.calc_regularization_gradient","text":"calc_regularization_gradient(β::Matrix{<:Real}, λ::Float64, include_intercept::Bool)\n\nCalculate gradient of L2 regularization term for regression coefficients.\n\nArguments\n\nβ::Matrix{<:Real}: Coefficient matrix\nλ::Float64: Regularization parameter\ninclude_intercept::Bool: Whether to exclude the intercept term from regularization\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.construct_AR_feature_matrix","page":"EmissionModels","title":"StateSpaceDynamics.construct_AR_feature_matrix","text":"construct_AR_feature_matrix(data::Matrix{Float64}, order::Int) -> Matrix{Float64}\n\nConstruct an autoregressive (AR) feature matrix from input time series data.\n\nArguments\n\ndata::Matrix{Float64}: A matrix of size (num_feats, T), where num_feats is the number of features, and T is the number of timepoints.\norder::Int: The autoregressive order, determining how many past timepoints are included for each time step.\n\nReturns\n\nMatrix{Float64}: A transformed feature matrix of size (num_feats * (order + 1), T - order), where each column contains stacked feature vectors from the current and past order timepoints.\n\nExample\n\n```julia data = rand(3, 10)  # 3 features, 10 timepoints order = 2 ARfeats = constructARfeaturematrix(data, order) size(AR_feats)  # (3 * (2 + 1), 10 - 2) => (9, 8)\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.construct_AR_feature_matrix-2","page":"EmissionModels","title":"StateSpaceDynamics.construct_AR_feature_matrix","text":"construct_AR_feature_matrix(data::Vector{Matrix{Float64}}, order::Int) -> Vector{Matrix{Float64}}\n\nConstructs autoregressive (AR) feature matrices for multiple trials of time series data. Each trial is represented as a matrix, and the function applies the same AR transformation to each trial independently.\n\nArguments\n\ndata::Vector{Matrix{Float64}}: A vector of matrices, where each matrix represents a trial of time series data with dimensions (num_feats, T), where num_feats is the number of features and T is the number of timepoints.\norder::Int: The autoregressive order, determining how many past timepoints are included for each time step.\n\nReturns\n\nVector{Matrix{Float64}}: A vector of transformed feature matrices, where each matrix has dimensions (num_feats * (order + 1), T - order), containing stacked feature vectors from the current and past order timepoints.\n\nExample\n\n```julia data = [rand(3, 10) for _ in 1:5]  # 5 trials, each with 3 features and 10 timepoints order = 2 ARfeatstrials = constructARfeaturematrix(data, order) size(ARfeats_trials[1])  # (9, 8), same transformation applied per trial\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.fit!","page":"EmissionModels","title":"StateSpaceDynamics.fit!","text":"fit!(model::GaussianEmission, Y::Matrix{<:Real}, w::Vector{Float64}=ones(size(Y, 1)))\n\nFit a GaussianEmission model to the data Y. \n\nArguments\n\nmodel::GaussianEmission: Gaussian model to fit.\nY::Matrix{<:Real}: Data to fit the model to. Should be a matrix of size (n, output_dim).\nw::Vector{Float64}=ones(size(Y, 1)): Weights for the data. Should be a vector of size n.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::PoissonRegressionEmission, Φ::Matrix{<:Real}, Y::Matrix{<:Real}, w::Vector{Float64}=ones(size(Y, 1)))\n\nCalculate the log-likelihood of a Poisson regression model.\n\nArguments\n\nmodel::PoissonRegressionEmission: Poisson regression model.\nΦ::Matrix{<:Real}: Design matrix of shape (n, input_dim).\nY::Matrix{<:Real}: Response matrix of shape (n, 1).\nw::Vector{Float64}: Weights of the data points. Should be a vector of size n.\n\nReturns\n\nloglikelihood::Float64: Log-likelihood of the model.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-2","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianRegressionEmission, Φ::Matrix{<:Real}, Y::Matrix{<:Real})\n\nCalculate the log likelihood of the data Y given the Gaussian regression emission model and the input features Φ.\n\nArguments\n\nmodel::GaussianRegressionEmission: The Gaussian regression emission model for which to calculate the log likelihood.\nΦ::Matrix{<:Real}: The input features matrix (Observations x Features).\nY::Matrix{<:Real}: The data matrix (Observations x Features).\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-3","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::BernoulliRegressionEmission, Φ::Matrix{<:Real}, Y::Matrix{<:Real}, w::Vector{Float64}=ones(size(Y, 1)))\n\nCalculate the log likelihood of the data Y given the Bernoulli regression emission model and the input features Φ. Optionally, a vector of weights w can be provided.\n\nArguments\n\nmodel::BernoulliRegressionEmission: The Bernoulli regression emission model for which to calculate the log likelihood.\nΦ::Matrix{<:Real}: The input features matrix (Observations x Features).\nY::Matrix{<:Real}: The data matrix (Observations x Features).\nw::Vector{Float64}: A vector of weights corresponding to each observation (defaults to a vector of ones).\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-4","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::AutoRegressionEmission, Y_prev::Matrix{<:Real}, Y::Matrix{<:Real})\n\nCalculate the log likelihood of the data Y given the autoregressive emission model and the previous observations Y_prev.\n\nArguments\n\nmodel::AutoRegressionEmission: The autoregressive emission model for which to calculate the log likelihood.\nY_prev::Matrix{<:Real}: The matrix of previous observations, where each row represents an observation.\nY::Matrix{<:Real}: The data matrix, where each row represents an observation.\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Tuple{GaussianEmission, Matrix{<:Real}}","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianEmission, Y::Matrix{<:Real})\n\nCalculate the log likelihood of the data Y given the Gaussian emission model.\n\nArguments\n\nmodel::GaussianEmission: The Gaussian emission model for which to calculate the log likelihood.\nY::Matrix{<:Real}: The data matrix, where each row represents an observation.\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.sample-Tuple{AutoRegressionEmission, Matrix{<:Real}}","page":"EmissionModels","title":"StateSpaceDynamics.sample","text":"sample(model::AutoRegressionEmission, Y_prev::Matrix{<:Real}; observation_sequence::Matrix{<:Real}=Matrix{Float64}(undef, 0, model.output_dim))\n\nGenerate a sample from the given autoregressive emission model using the previous observations Y_prev, and append it to the provided observation sequence.\n\nArguments\n\nmodel::AutoRegressionEmission: The autoregressive emission model to sample from.\nY_prev::Matrix{<:Real}: The matrix of previous observations, where each row represents an observation.\nobservation_sequence::Matrix{<:Real}: The sequence of observations to which the new sample will be appended (defaults to an empty matrix with appropriate dimensions).\n\nReturns\n\nMatrix{Float64}: The updated observation sequence with the new sample appended.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.sample-Tuple{BernoulliRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"StateSpaceDynamics.sample","text":"sample(model::BernoulliRegressionEmission, Φ::Matrix{<:Real}; n::Int=size(Φ, 1))\n\nGenerate n samples from a Bernoulli regression model. Returns a matrix of size (n, 1).\n\nArguments\n\nmodel::BernoulliRegressionEmission: Bernoulli regression model.\nΦ::Matrix{<:Real}: Design matrix of shape (n, input_dim).\nn::Int=size(Φ, 1): Number of samples to generate.\n\nReturns\n\nY::Matrix{<:Real}: Matrix of samples of shape (n, 1).\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.sample-Tuple{GaussianEmission}","page":"EmissionModels","title":"StateSpaceDynamics.sample","text":"sample(model::Gaussian; n::Int=1)\n\nGenerate n samples from a Gaussian model. Returns a matrix of size (n, output_dim).\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.sample-Tuple{GaussianRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"StateSpaceDynamics.sample","text":"sample(model::GaussianRegressionEmission, Φ::Matrix{<:Real}; n::Int=size(Φ, 1))\n\nGenerate n samples from a Gaussian regression model. Returns a matrix of size (n, output_dim).\n\nArguments\n\nmodel::GaussianRegressionEmission: Gaussian regression model.\nΦ::Matrix{<:Real}: Design matrix of shape (n, input_dim).\nn::Int=size(Φ, 1): Number of samples to generate.\n\nReturns\n\nY::Matrix{<:Real}: Matrix of samples of shape (n, output_dim).\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.sample-Tuple{PoissonRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"StateSpaceDynamics.sample","text":"sample(model::PoissonRegressionEmission, Φ::Matrix{<:Real}; n::Int=size(Φ, 1))\n\nGenerate n samples from a Poisson regression model. Returns a matrix of size (n, 1).\n\nArguments\n\nmodel::PoissonRegressionEmission: Poisson regression model.\nΦ::Matrix{<:Real}: Design matrix of shape (n, input_dim).\nn::Int=size(Φ, 1): Number of samples to generate.\n\nReturns\n\nY::Matrix{<:Real}: Matrix of samples of shape (n, 1).\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#What-is-a-Linear-Dynamical-System?","page":"Linear Dynamical Systems","title":"What is a Linear Dynamical System?","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A Linear Dynamical System (LDS) is a mathematical model used to describe how a system evolves over time. These systems are a subset of state-space models, where the hidden state dynamics are continuous. What makes these models linear is that the latent dynamics evolve according to a linear function of the previous state. The observations, however, can be related to the hidden state through a nonlinear link function.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"At its core, an LDS defines:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A state transition function: how the internal state evolves from one time step to the next.\nAn observation function: how the internal state generates the observed data.","category":"page"},{"location":"LinearDynamicalSystems/#The-Gaussian-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Gaussian Linear Dynamical System — typically just referred to as an LDS — is a specific type of linear dynamical system where both the state transition and observation functions are linear, and all noise is Gaussian.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The generative model is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"$","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"xt \\sim \\mathcal{N}(A x{t-1}, Q) \\\nyt \\sim \\mathcal{N}(C xt, R) $","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x_t\nis the hidden state at time t\ny_t\nis the observed data at time t\nA\nis the state transition matrix\nC\nis the observation matrix\nQ\nis the process noise covariance\nR\nis the observation noise covariance","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This can equivalently be written in equation form:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"$","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"xt = A x{t-1} + \\epsilont \\\nyt = C xt + \\etat $","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"epsilon_t sim mathcalN(0 Q)\nis the process noise\neta_t sim mathcalN(0 R)\nis the observation noise","category":"page"},{"location":"LinearDynamicalSystems/#The-Poisson-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Poisson Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Poisson Linear Dynamical System is a variant of the LDS where the observations are modeled as counts. This is useful in useful in fields like neuroscience where we are often interested in modeling spike count data. To relate the spiking data to the Gaussian latent variable, we use a nonlinear link function, specifically the exponential function. Thus our generative model is given by: ","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x_t sim mathcalN(A x_t-1 Q) \ny_t sim textPoisson(textexp(Cx_t + b))\n","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"b\nis a bias term","category":"page"},{"location":"LinearDynamicalSystems/#Inference-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Inference in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In StateSpaceDynamics.jl, we directly maximize the complete-data log-likelihood function with respect to the latent states given the data and the parameters of the model. In other words, the maximum a priori (MAP) estimate of the latent state path is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"undersetxtextargmax  left log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t) right","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This MAP estimation approach has the same computational complexity as traditional Kalman filtering and smoothing — $ \\mathcal{O}(T) $ — but is significantly more flexible. Notably, it can handle nonlinear observations and non-Gaussian noise while still yielding exact MAP estimates, unlike approximate techniques such as the Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF).","category":"page"},{"location":"LinearDynamicalSystems/#Newton's-Method-for-Latent-State-Optimization","page":"Linear Dynamical Systems","title":"Newton's Method for Latent State Optimization","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"To find the MAP trajectory, we iteratively optimize the latent states using Newton’s method. The update equation at each iteration is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x^(i+1) = x^(i) - left nabla^2 mathcalL(x^(i)) right^-1 nabla mathcalL(x^(i))","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x)\nis the complete-data log-likelihood:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x) = log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t)","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"nabla mathcalL(x)\nis the gradient of the full log-likelihood with respect to all latent states,\nnabla^2 mathcalL(x)\nis the Hessian of the full log-likelihood.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This update is performed over the entire latent state sequence $ x_{1:T} $, and repeated until convergence.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"For Gaussian models, $ \\mathcal{L}(x) $ is quadratic and Newton's method converges in a single step — recovering the exact Kalman smoother solution. But, for non-Gaussian models, the Hessian is not constant and the optimization is more complex. However, the MAP estimate can still be computed efficiently using the same approach as the optimization problem is still convex.","category":"page"},{"location":"LinearDynamicalSystems/#Laplace-Approximation-of-Posterior-for-Non-Conjugate-Observation-Models","page":"Linear Dynamical Systems","title":"Laplace Approximation of Posterior for Non-Conjugate Observation Models","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In the case of non-Gaussian observations, we can use a Laplace approximation to compute the posterior distribution of the latent states. Notably, in the case of Gaussian Observations (which is conjugate with the Gaussian state model), the posterior is also Gaussian, and is the exact posterior. However, for non-Gaussian observations, we can approximate the posterior using a Gaussian distribution centered at the MAP estimate of the latent states. This approximation is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"\np(x mid y) approx mathcalN(x^(*) -left nabla^2 mathcalL(x^(*)) right^-1)\n","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"$ x^{(*)}$ is the MAP estimate of the latent states\nnabla^2 mathcalL(x^(*))\nis the Hessian of the log-likelihood at the MAP estimate.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Despite, the requirement of inverting a Hessian of diomension (d x T) x (d x T), this is still computationally efficient, as the Markov structure of the model, renders the Hessian block-tridiagonal, and thus the inversion is not intractable.","category":"page"},{"location":"LinearDynamicalSystems/#Learning-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Learning in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Given the latent structure of state-space models, we must rely on either the Expectation-Maximization (EM) or Variational Inference (VI) approaches to learn the parameters of the model. StateSpaceDynamics.jl supports both EM and VI. For LDS models, we can use Laplace EM, where we approximate the posterior of the latent state path using the Laplace approximation as outlined above. Using these approximate posteriors (or exact ones in the Gaussian case), we can apply closed-form updates for the model parameters.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Modules = [StateSpaceDynamics]\nPages   = [\"LinearDynamicalSystems.jl\"]","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianObservationModel","text":"GaussianObservationModel{T<:Real} <: AbstractObservationModel\n\nRepresents the observation model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nC::Matrix{T}: Observation matrix\nR::Matrix{T}: Observation noise covariance\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianObservationModel-Union{Tuple{}, Tuple{T}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianObservationModel","text":"GaussianObservationModel(; C, R, obs_dim, latent_dim)\n\nConstruct a GaussianObservationModel with the given parameters or random initializations.\n\nArguments\n\nC::Matrix{T}=Matrix{T}(undef, 0, 0): Observation matrix\nR::Matrix{T}=Matrix{T}(undef, 0, 0): Observation noise covariance\nobs_dim::Int: Dimension of the observations (required if C or R is not provided.)\nlatent_dim::Int: Dimension of the latent state (required if C is not provided.)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianStateModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianStateModel","text":"GaussianStateModel{T<:Real} <: AbstractStateModel\n\nRepresents the state model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nA::Matrix{T}: Transition matrix\nQ::Matrix{T}: Process noise covariance\nx0::Vector{T}: Initial state\nP0::Matrix{T}: Initial state covariance\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianStateModel-Union{Tuple{}, Tuple{T}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianStateModel","text":"GaussianStateModel(; A, Q, x0, P0, latent_dim)\n\nConstruct a GaussianStateModel with the given parameters or random initializations.\n\nArguments\n\nA::Matrix{T}=Matrix{T}(undef, 0, 0): Transition matrix\nQ::Matrix{T}=Matrix{T}(undef, 0, 0): Process noise covariance\nx0::Vector{T}=Vector{T}(undef, 0): Initial state\nP0::Matrix{T}=Matrix{T}(undef, 0, 0): Initial state covariance\nlatent_dim::Int: Dimension of the latent state (required if any matrix is not provided.)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.LinearDynamicalSystem","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.LinearDynamicalSystem","text":"LinearDynamicalSystem{S<:AbstractStateModel, O<:AbstractObservationModel}\n\nRepresents a unified Linear Dynamical System with customizable state and observation models.\n\nFields\n\nstate_model::S: The state model (e.g., GaussianStateModel)\nobs_model::O: The observation model (e.g., GaussianObservationModel or PoissonObservationModel)\nlatent_dim::Int: Dimension of the latent state\nobs_dim::Int: Dimension of the observations\nfit_bool::Vector{Bool}: Vector indicating which parameters to fit during optimization\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.PoissonObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.PoissonObservationModel","text":"PoissonObservationModel{T<:Real} <: AbstractObservationModel\n\nRepresents the observation model of a Linear Dynamical System with Poisson observations.\n\nFields\n\nC::Matrix{T}: Observation matrix\nlog_d::Vector{T}: Mean firing rate vector (log space)\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.PoissonObservationModel-Union{Tuple{}, Tuple{T}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.PoissonObservationModel","text":"PoissonObservationModel(; C, log_d, obs_dim, latent_dim)\n\nConstruct a PoissonObservationModel with the given parameters or random initializations.\n\nArguments\n\nC::Matrix{T}=Matrix{T}(undef, 0, 0): Observation matrix\nlog_d::Vector{T}=Vector{T}(undef, 0): Mean firing rate vector (log space)\nobs_dim::Int: Dimension of the observations (required if any matrix is not provided.)\nlatent_dim::Int: Dimension of the latent state (required if C is not provided.)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianLDS-Union{Tuple{}, Tuple{T}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianLDS","text":"GaussianLDS(; A, C, Q, R, x0, P0, fit_bool, obs_dim, latent_dim)\n\nConstruct a Linear Dynamical System with Gaussian state and observation models.\n\nArguments\n\nA::Matrix{T}=Matrix{T}(undef, 0, 0): Transition matrix\nC::Matrix{T}=Matrix{T}(undef, 0, 0): Observation matrix\nQ::Matrix{T}=Matrix{T}(undef, 0, 0): Process noise covariance\nR::Matrix{T}=Matrix{T}(undef, 0, 0): Observation noise covariance\nx0::Vector{T}=Vector{T}(undef, 0): Initial state\nP0::Matrix{T}=Matrix{T}(undef, 0, 0): Initial state covariance\nfit_bool::Vector{Bool}=fill(true, 6): Vector indicating which parameters to fit during optimization\nobs_dim::Int: Dimension of the observations (required if C or R is not provided.)\nlatent_dim::Int: Dimension of the latent state (required if A, Q, x0, P0, or C is not provided.)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Gradient-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Matrix{T}, Matrix{T}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Matrix{T}, Matrix{T}, Vector{Float64}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.GaussianObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Gradient","text":"Gradient(lds::LinearDynamicalSystem{S,O}, y::AbstractMatrix{T}, x::AbstractMatrix{T}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nCompute the gradient of the log-likelihood with respect to the latent states for a linear dynamical system.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System.\ny::AbstractMatrix{T}: The observed data.\nx::AbstractMatrix{T}: The latent states.\nw::Vector{Float64}: coeffcients to weight the data.\n\nReturns\n\ngrad::Matrix{T}: Gradient of the log-likelihood with respect to the latent states.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Gradient-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Matrix{T}, Matrix{T}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Matrix{T}, Matrix{T}, Vector{T}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.PoissonObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Gradient","text":"Gradient(lds::LinearDynamicalSystem{S,O}, y::Matrix{T}, x::Matrix{T}) where {T<:Real, S<:GaussianStateModel{T}, O<:PoissonObservationModel{T}}\n\nCalculate the gradient of the log-likelihood of a Poisson Linear Dynamical System model for a single trial.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System model.\ny::Matrix{T}: The observed data. Dimensions: (obsdim, Tsteps)\nx::Matrix{T}: The latent state variables. Dimensions: (latentdim, Tsteps)\nw::Vector{T}: Weights for each observation in the log-likelihood calculation. Not currently used.\n\nReturns\n\ngrad::Matrix{T}: The gradient of the log-likelihood. Dimensions: (latentdim, Tsteps)\n\nNote\n\nThe gradient is computed with respect to the latent states x. Each row of the returned gradient corresponds to the gradient for a single time step.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Hessian-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{T}, AbstractMatrix{T}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{T}, AbstractMatrix{T}, Vector{Float64}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.GaussianObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Hessian","text":"Hessian(lds::LinearDynamicalSystem{S,O}, y::AbstractMatrix{T}, x::AbstractMatrix{T}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nConstruct the Hessian matrix of the log-likelihood of the LDS model given a set of observations.\n\nThis function is used for the direct optimization of the log-likelihood as advocated by Paninski et al. (2009).  The block tridiagonal structure of the Hessian is exploited to reduce the number of parameters that need to be computed, and to reduce the memory requirements. Together with the gradient, this allows for Kalman Smoothing to be performed  by simply solving a linear system of equations:\n\n̂xₙ₊₁ = ̂xₙ - H \\ ∇\n\nwhere ̂xₙ is the current smoothed state estimate, H is the Hessian matrix, and ∇ is the gradient of the log-likelihood.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System.\ny::AbstractMatrix{T}: Matrix of observations.\nx::AbstractMatrix{T}: Matrix of latent states.\nw::Vector{Float64}: coeffcients to weight the data.\n\nReturns\n\nH::Matrix{T}: Hessian matrix of the log-likelihood.\nH_diag::Vector{Matrix{T}}: Main diagonal blocks of the Hessian.\nH_super::Vector{Matrix{T}}: Super-diagonal blocks of the Hessian.\nH_sub::Vector{Matrix{T}}: Sub-diagonal blocks of the Hessian.\n\nNote\n\nx is not used in this function, but is required to match the function signature of other Hessian calculations e.g., in PoissonLDS.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Hessian-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{T}, AbstractMatrix{T}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{T}, AbstractMatrix{T}, Vector{T}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.PoissonObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Hessian","text":"Hessian(lds::LinearDynamicalSystem{S,O}, y::AbstractMatrix{T}, x::AbstractMatrix{T}) where {T<:Real, S<:GaussianStateModel{T}, O<:PoissonObservationModel{T}}\n\nCalculate the Hessian matrix of the log-likelihood for a Poisson Linear Dynamical System.\n\nThis function computes the Hessian matrix, which represents the second-order partial derivatives of the log-likelihood with respect to the latent states.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System with Poisson observations.\ny::AbstractMatrix{T}: The observed data. Dimensions: (obsdim, Tsteps)\nx::AbstractMatrix{T}: The current estimate of latent states. Dimensions: (latentdim, Tsteps)\nw::Vector{T}: Weights for each observation in the log-likelihood calculation. Not currently used.\n\nReturns\n\nH::Matrix{T}: The full Hessian matrix.\nH_diag::Vector{Matrix{T}}: The main diagonal blocks of the Hessian.\nH_super::Vector{Matrix{T}}: The super-diagonal blocks of the Hessian.\nH_sub::Vector{Matrix{T}}: The sub-diagonal blocks of the Hessian.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.PoissonLDS-Union{Tuple{}, Tuple{T}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.PoissonLDS","text":"PoissonLDS(; A, C, Q, log_d, x0, P0, refractory_period, fit_bool, obs_dim, latent_dim)\n\nConstruct a Linear Dynamical System with Gaussian state and Poisson observation models.\n\nArguments\n\nA::Matrix{T}=Matrix{T}(undef, 0, 0): Transition matrix\nC::Matrix{T}=Matrix{T}(undef, 0, 0): Observation matrix\nQ::Matrix{T}=Matrix{T}(undef, 0, 0): Process noise covariance\nlog_d::Vector{T}=Vector{T}(undef, 0): Mean firing rate vector (log space)\nx0::Vector{T}=Vector{T}(undef, 0): Initial state\nP0::Matrix{T}=Matrix{T}(undef, 0, 0): Initial state covariance\nrefractory_period::Int=1: Refractory period\nfit_bool::Vector{Bool}=fill(true, 7): Vector indicating which parameters to fit during optimization\nobs_dim::Int: Dimension of the observations (required if C, D, or log_d is not provided.)\nlatent_dim::Int: Dimension of the latent state (required if A, Q, x0, P0, or C is not provided.)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Q_function","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Q_function","text":"Q(A, Q, H, R, P0, x0, E_z, E_zz, E_zz_prev, y)\n\nCalculate the complete Q-function for the EM algorithm in a Linear Dynamical System.\n\nArguments\n\nA::Matrix{<:Real}: The state transition matrix.\nQ::AbstractMatrix{<:Real}: The process noise covariance matrix (or its Cholesky factor).\nH::Matrix{<:Real}: The observation matrix.\nR::AbstractMatrix{<:Real}: The observation noise covariance matrix (or its Cholesky factor).\nP0::AbstractMatrix{<:Real}: The initial state covariance matrix (or its Cholesky factor).\nx0::Vector{<:Real}: The initial state mean.\nE_z::Matrix{<:Real}: The expected latent states, size (state_dim, T).\nE_zz::Array{<:Real, 3}: The expected value of zt * zt', size (statedim, statedim, T).\nE_zz_prev::Array{<:Real, 3}: The expected value of zt * z{t-1}', size (statedim, statedim, T).\ny::Matrix{<:Real}: The observed data, size (obs_dim, T).\n\nReturns\n\nQ_val::Float64: The complete Q-function value.\n\n\n\n\n\n","category":"function"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Q_function-Union{Tuple{T}, Tuple{Matrix{T}, Matrix{T}, Matrix{T}, Vector{T}, Vector{T}, Matrix{T}, Array{T, 3}, Array{T, 4}, Array{T, 4}, Array{T, 4}, Array{T, 3}}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Q_function","text":"Q_function(A::Matrix{T}, Q::Matrix{T}, C::Matrix{T}, log_d::Vector{T}, x0::Vector{T}, P0::Matrix{T}, E_z::Matrix{T}, E_zz::Array{T, 3}, E_zz_prev::Array{T, 3}, P_smooth::Array{T, 3}, y::Matrix{T})\n\nCalculate the Q-function for the Linear Dynamical System.\n\nArguments\n\nA::Matrix{T}: The transition matrix.\nQ::Matrix{T}: The process noise covariance matrix.\nC::Matrix{T}: The observation matrix.\nlog_d::Vector{T}: The mean firing rate vector in log space.\nx0::Vector{T}: The initial state mean.\nP0::Matrix{T}: The initial state covariance matrix.\nE_z::Matrix{T}: The expected latent states.\nE_zz::Array{T, 3}: The expected latent states x the latent states.\nE_zz_prev::Array{T, 3}: The expected latent states x the previous latent states.\nP_smooth::Array{T, 3}: The smoothed state covariances.\ny::Matrix{T}: The observed data.\n\nReturns\n\nFloat64: The Q-function for the Linear Dynamical System.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Q_obs","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Q_obs","text":"Q_obs(H, R, E_z, E_zz, y)\n\nCalculate the observation component of the Q-function for the EM algorithm in a Linear Dynamical System.\n\nArguments\n\nH::Matrix{<:Real}: The observation matrix.\nR::AbstractMatrix{<:Real}: The observation noise covariance matrix (or its Cholesky factor).\nE_z::Matrix{<:Real}: The expected latent states, size (state_dim, T).\nE_zz::Array{<:Real, 3}: The expected value of zt * zt', size (statedim, statedim, T).\ny::Matrix{<:Real}: The observed data, size (obs_dim, T).\n\nReturns\n\nQ_val::Float64: The observation component of the Q-function.\n\n\n\n\n\n","category":"function"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Q_obs-Tuple{AbstractMatrix{<:Real}, AbstractVector{<:Real}, AbstractMatrix{<:Real}, AbstractVector{<:Real}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Q_obs","text":"Q_obs(H, R, E_z, E_zz, y)\n\nCalculate the a single time step observation component of the Q-function for the EM algorithm in a Linear Dynamical System before the R^-1 is accounted for.\n\nArguments\n\nH::Matrix{<:Real}: The observation matrix.\nR::AbstractMatrix{<:Real}: The observation noise covariance matrix (or its Cholesky factor).\nE_z::Vector{<:Real}: The expected latent states at time t, size (state_dim).\nE_zz::Matrix{<:Real}: The expected value of zt * zt' at time t, size (statedim, statedim).\ny::Vector{<:Real}: The observed data at time t, size (obs_dim).\n\nReturns\n\nq::Float64: The observation component at time t of the Q-function prior to R^-1.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Q_observation_model-Union{Tuple{U}, Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector{T}, AbstractArray{U, 3}, AbstractArray{U, 4}, Array{U, 3}}} where {T<:Real, U<:Real}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Q_observation_model","text":"Q_observation_model(C::Matrix{<:Real}, D::Matrix{<:Real}, log_d::Vector{<:Real}, E_z::Array{<:Real}, E_zz::Array{<:Real}, y::Array{<:Real})\n\nCalculate the Q-function for the observation model.\n\nArguments\n\nC::Matrix{<:Real}: The observation matrix.\nlog_d::Vector{<:Real}: The mean firing rate vector in log space.\nE_z::Array{<:Real}: The expected latent states.\nE_zz::Array{<:Real}: The expected latent states x the latent states.\ny::Array{<:Real}: The observed data.\n\nReturns\n\nFloat64: The Q-function for the observation model.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Q_state-Tuple{AbstractMatrix{<:Real}, AbstractMatrix{<:Real}, AbstractMatrix{<:Real}, AbstractVector{<:Real}, AbstractMatrix{<:Real}, AbstractArray{<:Real, 3}, AbstractArray{<:Real, 3}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Q_state","text":"Q_state(A, Q, P0, x0, E_z, E_zz, E_zz_prev)\n\nCalculate the state component of the Q-function for the EM algorithm in a Linear Dynamical System.\n\nArguments\n\nA::Matrix{<:Real}: The state transition matrix.\nQ::AbstractMatrix{<:Real}: The process noise covariance matrix (or its Cholesky factor).\nP0::AbstractMatrix{<:Real}: The initial state covariance matrix (or its Cholesky factor).\nx0::Vector{<:Real}: The initial state mean.\nE_z::Matrix{<:Real}: The expected latent states, size (state_dim, T).\nE_zz::Array{<:Real, 3}: The expected value of zt * zt', size (statedim, statedim, T).\nE_zz_prev::Array{<:Real, 3}: The expected value of zt * z{t-1}', size (statedim, statedim, T).\n\nReturns\n\nQ_val::Float64: The state component of the Q-function.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.Q_state-Union{Tuple{T}, Tuple{Matrix{T}, Matrix{T}, Matrix{T}, Vector{T}, Array{T, 3}, Array{T, 4}, Array{T, 4}}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.Q_state","text":"Q_state(A::Matrix{T}, Q::Matrix{T}, P0::Matrix{T}, x0::Vector{T}, E_z::Array{T, 3}, E_zz::Array{T, 4}, E_zz_prev::Array{T, 4}) where T<:Real\n\nCalculates the Q-function for the state model over multiple trials.\n\nArguments\n\nA::Matrix{T}: The transition matrix.\nQ::Matrix{T}: The process noise covariance matrix.\nP0::Matrix{T}: The initial state covariance matrix.\nx0::Vector{T}: The initial state mean.\nE_z::Array{T, 3}: The expected latent states.\nE_zz::Array{T, 4}: The expected latent states x the latent states.\nE_zz_prev::Array{T, 4}: The expected latent states x the previous latent states.\n\nReturns\n\nFloat64: The Q-function for the state model.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.calculate_elbo-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 4}, Array{T, 4}, Array{T, 3}, Float64}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 4}, Array{T, 4}, Array{T, 3}, Float64, Vector{Float64}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.GaussianObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.calculate_elbo","text":"calculate_elbo(lds::LinearDynamicalSystem{S,O}, E_z::Array{T,3}, E_zz::Array{T,4}, E_zz_prev::Array{T,4}, p_smooth::Array{T,4}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:AbstractObservationModel{T}}\n\nCalculate the Evidence Lower Bound (ELBO) for a Linear Dynamical System.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials, state_dim)\nE_zz_prev::Array{T,4}: Expected zt * z{t-1}', size (statedim, statedim, Tsteps, ntrials, state_dim)\np_smooth::Array{T,4}: Smoothed state covariances, size (statedim, statedim, Tsteps, ntrials, state_dim)\ny::Array{T,3}: Observed data, size (obsdim, Tsteps, n_trials)\n\nReturns\n\nelbo::T: The Evidence Lower Bound (ELBO) for the LDS.\n\nNote\n\nFor a GaussianLDS the ELBO is equivalent to the total marginal likelihood\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.calculate_elbo-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 4}, Array{T, 4}, Array{T, 3}, Float64}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.PoissonObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.calculate_elbo","text":"calculate_elbo(plds::LinearDynamicalSystem{S,O}, E_z::Array{T, 3}, E_zz::Array{T, 4}, \n               E_zz_prev::Array{T, 4}, P_smooth::Array{T, 4}, y::Array{T, 3}) where \n               {T<:Real, S<:GaussianStateModel{T}, O<:PoissonObservationModel{T}}\n\nCalculate the Evidence Lower Bound (ELBO) for a Poisson Linear Dynamical System (PLDS).\n\nArguments\n\nplds::LinearDynamicalSystem{S,O}: The PLDS model.\nE_z::Array{T, 3}: Expected values of latent states. Dimensions: (statedim, tsteps, n_trials).\nE_zz::Array{T, 4}: Expected values of latent state outer products. Dimensions: (statedim, statedim, tsteps, ntrials).\nE_zz_prev::Array{T, 4}: Expected values of latent state outer products with previous time step. Dimensions: (state dimension, state dimension, tsteps-1, ntrials).\nP_smooth::Array{T, 4}: Smoothed covariance matrices. Dimensions: (state dimension, state dimension, tsteps, ntrials).\ny::Array{T, 3}: Observed data. Dimensions: (obsdim, tsteps, n_trials).\n\nReturns\n\nelbo::Float64: The calculated Evidence Lower Bound.\n\nDescription\n\nThis function computes the ELBO for a PLDS model, which consists of two main components:\n\nThe expected complete log-likelihood (ECLL), calculated using the Q_function.\nThe entropy of the variational distribution, calculated using gaussian entropy.\n\nThe ELBO is then computed as: ELBO = ECLL - Entropy.\n\nNote\n\nEnsure that the dimensions of input arrays match the expected dimensions as described in the arguments section.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.estep-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.estep","text":"estep(lds::LinearDynamicalSystem{S,O}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:AbstractObservationModel{T}}\n\nPerform the E-step of the EM algorithm for a Linear Dynamical System, treating all input as multi-trial.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\ny::Array{T,3}: Observed data, size (obsdim, Tsteps, ntrials)   Note: For single-trial data, use y[1:1, :, :] to create a 3D array with ntrials = 1\n\nReturns\n\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials, state_dim)\nE_zz_prev::Array{T,4}: Expected zt * z{t-1}', size (statedim, statedim, Tsteps, ntrials, state_dim)\nx_smooth::Array{T,3}: Smoothed state estimates, size (statedim, statedim, Tsteps, ntrials)\np_smooth::Array{T,4}: Smoothed state covariances, size (statedim, statedim, Tsteps, ntrials, state_dim)\nml::T: Total marginal likelihood (log-likelihood) of the data across all trials\n\nNote\n\nThis function first smooths the data using the smooth function, then computes sufficient statistics.\nIt treats all input as multi-trial, with single-trial being a special case where n_trials = 1.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.fit!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(lds::LinearDynamicalSystem{S,O}, y::Matrix{T}; \n     max_iter::Int=1000, \n     tol::Real=1e-12, \n     ) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nFit a Linear Dynamical System using the Expectation-Maximization (EM) algorithm with Kalman smoothing.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System to be fitted.\ny::Matrix{T}: Observed data, size (obsdim, Tsteps).\n\nKeyword Arguments\n\nmax_iter::Int=1000: Maximum number of EM iterations.\ntol::Real=1e-12: Convergence tolerance for log-likelihood change.\n\nReturns\n\nmls::Vector{T}: Vector of log-likelihood values for each iteration.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.gradient_observation_model!-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractMatrix{T}, AbstractVector{T}, AbstractArray{T}, AbstractArray{T}, Array{T}}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.gradient_observation_model!","text":"gradient_observation_model!(grad::AbstractVector{T}, C::AbstractMatrix{T}, log_d::AbstractVector{T}, E_z::AbstractArray{T}, P_smooth::AbstractArray{T}, y::Array{T}) where T<:Real\n\nCompute the gradient of the Q-function with respect to the observation model parameters (C and log_d) for a Poisson Linear Dynamical System.\n\nArguments\n\ngrad::AbstractVector{T}: Pre-allocated vector to store the computed gradient.\nC::AbstractMatrix{T}: The observation matrix. Dimensions: (obsdim, latentdim)\nlog_d::AbstractVector{T}: The log of the baseline firing rates. Dimensions: (obs_dim,)\nE_z::AbstractArray{T}: The expected latent states. Dimensions: (latentdim, tsteps, n_trials)\nP_smooth::AbstractArray{T}: The smoothed state covariances. Dimensions: (latentdim, latentdim, tsteps, ntrials)\ny::Array{T}: The observed data. Dimensions: (obsdim, tsteps, N-trials)\n\nNote\n\nThis function modifies grad in-place. The gradient is computed for the negative Q-function, as we're minimizing -Q in optimization routines.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.initialize_FilterSmooth-Tuple{StateSpaceDynamics.LinearDynamicalSystem, Int64}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.initialize_FilterSmooth","text":"initialize_FilterSmooth(model::LinearDynamicalSystem, num_obs::Int) -> FilterSmooth{T}\n\nInitialize a FilterSmooth object for a given linear dynamical system model and number of observations.\n\nArguments\n\nmodel::LinearDynamicalSystem:   The linear dynamical system model containing system parameters, including the latent dimensionality (latent_dim).\nnum_obs::Int:   The number of observations (time steps) for which to initialize the smoothing filters.\n\nReturns\n\nFilterSmooth{T}:   A FilterSmooth instance with all fields initialized to zero arrays. The dimensions of the arrays are determined by the number of states (latent_dim) from the model and the specified number of observations (num_obs).\n\nExample\n\n```julia\n\nAssume model is an instance of LinearDynamicalSystem with latent_dim = 10\n\nnumobservations = 100 filtersmooth = initializeFilterSmooth(model, numobservations)\n\nfilter_smooth now contains zero-initialized arrays for smoothing operations\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.loglikelihood-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{Array{T, 3}, StateSpaceDynamics.LinearDynamicalSystem{O, S}, Array{T, 3}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.PoissonObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(x::Array{T, 3}, lds::LinearDynamicalSystem{S,O}, y::Array{T, 3}) where {T<:Real, S<:GaussianStateModel{T}, O<:PoissonObservationModel{T}}\n\nCalculate the complete-data log-likelihood of a Poisson Linear Dynamical System model for multiple trials.\n\nArguments\n\nx::Array{T, 3}: The latent state variables. Dimensions: (latentdim, TSteps, n_trials)\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System model.\ny::Array{T, 3}: The observed data. Dimensions: (obsdim, Tsteps, n_trials)\n\nReturns\n\nll::T: The log-likelihood value.\n\nExamples\n\nlds = PoissonLDS(obs_dim=4, latent_dim=3)\nx, y = sample(lds, 100, 10)  # 10 trials, 100 time steps each\nll = loglikelihood(x, lds, y)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.loglikelihood-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{U}, Tuple{AbstractMatrix{T}, StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{U}}, Tuple{AbstractMatrix{T}, StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{U}, Vector{U}}} where {U<:Real, T<:Real, S<:(StateSpaceDynamics.GaussianStateModel), O<:(StateSpaceDynamics.PoissonObservationModel)}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(x::Matrix{T}, lds::LinearDynamicalSystem{S,O}, y::Matrix{T}) where {T<:Real, S<:GaussianStateModel, O<:PoissonObservationModel}\n\nCalculate the complete-data log-likelihood of a Poisson Linear Dynamical System model for a single trial. \n\nArguments\n\nx::Matrix{T}: The latent state variables. Dimensions: (latentdim, Tsteps)\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System model.\ny::Matrix{T}: The observed data. Dimensions: (obsdim, Tsteps)\nw::Vector{T}: Weights for each observation in the log-likelihood calculation. Not currently used.\n\nReturns\n\nll::T: The log-likelihood value.\n\nExamples\n\nlds = PoissonLDS(obs_dim=4, latent_dim=3)\nx, y = sample(lds, 100, 1)  # 1 trial, 100 time steps\nll = loglikelihood(x, lds, y)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.loglikelihood-Union{Tuple{O}, Tuple{S}, Tuple{U}, Tuple{T}, Tuple{AbstractMatrix{T}, StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{U}}, Tuple{AbstractMatrix{T}, StateSpaceDynamics.LinearDynamicalSystem{S, O}, AbstractMatrix{U}, Vector{Float64}}} where {T<:Real, U<:Real, S<:(StateSpaceDynamics.GaussianStateModel), O<:(StateSpaceDynamics.GaussianObservationModel)}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(x::AbstractMatrix{T}, lds::LinearDynamicalSystem{S,O}, y::AbstractMatrix{T}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nCalculate the complete-data log-likelihood of a linear dynamical system (LDS) given the observed data.\n\nArguments\n\nx::AbstractMatrix{T}: The state sequence of the LDS.\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System.\ny::AbstractMatrix{T}: The observed data.\nw::Vector{Float64}: coeffcients to weight the data.\n\nReturns\n\nll::T: The complete-data log-likelihood of the LDS.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.mstep!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 4}, Array{T, 4}, Array{T, 3}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 4}, Array{T, 4}, Array{T, 3}, Vector{Float64}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.GaussianObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.mstep!","text":"mstep!(lds::LinearDynamicalSystem{S,O}, E_z::Array{T,3}, E_zz::Array{T,4}, E_zz_prev::Array{T,4}, p_smooth::Array{T, 4}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nPerform the M-step of the EM algorithm for a Linear Dynamical System with multi-trial data.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials)\nE_zz_prev::Array{T,4}: Expected zt * z{t-1}', size (statedim, statedim, Tsteps, ntrials)\np_smooth::Array{T,4}: Smoothed state covariances, size (statedim, statedim, Tsteps, ntrials) (not used)\ny::Array{T,3}: Observed data, size (obsdim, Tsteps, n_trials)\n\nNote\n\nThis function modifies lds in-place by updating all model parameters.\nUpdates are performed only for parameters where the corresponding fit_bool is true.\nAll update functions now handle multi-trial data.\nP_smooth is required but not used in the M-step so that the function signature matches the PoissonLDS version.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.mstep!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 4}, Array{T, 4}, Array{T, 3}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.PoissonObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.mstep!","text":"mstep!(plds::LinearDynamicalSystem{S,O}, E_z::Array{T,3}, E_zz::Array{T,4}, E_zz_Prev{T,4}, p_smooth{T,4}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:PoissonObservationModel{T}}\n\nPerform the M-step of the EM algorithm for a Poisson Linear Dynamical System with multi-trial data.\n\nArguments\n\nplds::LinearDynamicalSystem{S,O}: The Poisson Linear Dynamical System struct.\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials)\nE_zz_prev::Array{T,4}: Expected zt * z{t-1}', size (statedim, statedim, Tsteps, ntrials)\np_smooth::Array{T,4}: Smoothed state covariances, size (statedim, statedim, Tsteps, ntrials)\ny::Array{T,3}: Observed data, size (obsdim, Tsteps, n_trials)\n\nNote\n\nThis function modifies plds in-place by updating all model parameters.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.obsparams-Union{Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}}, Tuple{O}, Tuple{S}} where {S<:StateSpaceDynamics.AbstractStateModel, O<:StateSpaceDynamics.AbstractObservationModel}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.obsparams","text":"obsparams(lds::LinearDynamicalSystem{S,O}) where {S<:AbstractStateModel,O<:AbstractObservationModel}\n\nExtract the observation parameters from a Linear Dynamical System.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System.\n\nReturns\n\nparams::Vector{Vector{Real}}: Vector of observation parameters.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.sample-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Int64, Int64}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.GaussianObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.sample","text":"sample(lds::LinearDynamicalSystem{S,O}, T_steps::Int, n_trials::Int) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nSample from a Linear Dynamical System (LDS) model for multiple trials.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System model.\nT_steps::Int: The number of time steps to sample for each trial.\nn_trials::Int: The number of trials to sample.=\n\nReturns\n\nx::Array{T, 3}: The latent state variables. Dimensions: (latentdim, TSteps, n_trials)\ny::Array{T, 3}: The observed data. Dimensions: (obsdim, Tsteps, n_trials)\n\nExamples\n\nlds = GaussianLDS(obs_dim=4, latent_dim=3)\nx, y = sample(lds, 10, 100)  # 10 trials, 100 time steps each\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.sample-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Int64, Int64}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.PoissonObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.sample","text":"sample(lds::LinearDynamicalSystem{S,O}, T_steps::Int, n_trials::Int) where {T<:Real, S<:GaussianStateModel{T}, O<:PoissonObservationModel{T}}\n\nSample from a Poisson Linear Dynamical System (LDS) model for multiple trials.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System model.\nT_steps::Int: The number of time steps to sample for each trial.\nn_trials::Int: The number of trials to sample.\n\nReturns\n\nx::Array{T, 3}: The latent state variables. Dimensions: (latentdim, TSteps, n_trials)\ny::Array{Int, 3}: The observed data. Dimensions: (obsdim, Tsteps, n_trials)\n\nExamples\n\nlds = LinearDynamicalSystem(obs_dim=4, latent_dim=3)\nx, y = sample(lds, 100, 10)  # 10 trials, 100 time steps each\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.smooth-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.smooth","text":"smooth(lds::LinearDynamicalSystem{S,O}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nThis function performs direct smoothing for a linear dynamical system (LDS) given the system parameters and the observed data for multiple trials.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The LDS object representing the system parameters.\ny::Array{T,3}: The observed data array with dimensions (obsdim, tiemsteps, n_trials).\n\nReturns\n\nx::Array{T,3}: The optimal state estimates with dimensions (ntrials, timesteps, latent_dim).\np_smooth::Array{T,4}: The posterior covariance matrices with dimensions (latentdim, latentdim, timesteps, ntrials).\ninverse_offdiag::Array{T,4}: The inverse off-diagonal matrices with dimensions (latentdim, latentdim, timesteps, ntrials).\n\nExample\n\nlds = GaussianLDS(obs_dim=4, latent_dim=3)\ny = randn(5, 100, 4)  # 5 trials, 100 time steps, 4 observed dimension\nx, p_smooth, inverse_offdiag = smooth(lds, y)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.smooth-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Matrix{T}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Matrix{T}, Vector{Float64}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.smooth","text":"smooth(lds::LinearDynamicalSystem{S,O}, y::Matrix{T}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nThis function performs direct smoothing for a linear dynamical system (LDS) given the system parameters and the observed data.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The LDS object representing the system parameters.\ny::Matrix{T}: The observed data matrix.\nw::Vector{Float64}: coeffcients to weight the data.\n\nReturns\n\nx::Matrix{T}: The optimal state estimate.\np_smooth::Array{T, 3}: The posterior covariance matrix.\ninverse_offdiag::Array{T, 3}: The inverse off-diagonal matrix.\nQ_val::T: The Q-function value.\n\nExample\n\nlds = GaussianLDS(obs_dim=4, latent_dim=3)\ny = randn(100, 4)  # 100 time steps, 4 observed dimensions\nx, p_smooth, inverse_offdiag, Q_val = DirectSmoother(lds, y)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.stateparams-Union{Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}}, Tuple{O}, Tuple{S}} where {S<:StateSpaceDynamics.AbstractStateModel, O<:StateSpaceDynamics.AbstractObservationModel}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.stateparams","text":"stateparams(lds::LinearDynamicalSystem{S,O}) where {S<:AbstractStateModel,O<:AbstractObservationModel}\n\nExtract the state parameters from a Linear Dynamical System.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System.\n\nReturns\n\nparams::Vector{Vector{Real}}: Vector of state parameters.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.sufficient_statistics-Union{Tuple{T}, Tuple{Array{T, 3}, Array{T, 4}, Array{T, 4}}} where T<:Real","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.sufficient_statistics","text":"sufficient_statistics(x_smooth::Array{T,3}, p_smooth::Array{T,4}, p_smooth_t1::Array{T,4}) where T <: Real\n\nCompute sufficient statistics for the EM algorithm in a Linear Dynamical System.\n\nArguments\n\nx_smooth::Array{T,3}: Smoothed state estimates, size (statedim, statedim, Tsteps, ntrials)\np_smooth::Array{T,4}: Smoothed state covariances, size (statedim, statedim, Tsteps, ntrials, state_dim)\np_smooth_t1::Array{T,4}: Lag-one covariance smoother, size (statedim, statedim, Tsteps, ntrials, state_dim)\n\nReturns\n\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials, state_dim)\nE_zz_prev::Array{T,4}: Expected zt * z{t-1}', size (statedim, statedim, Tsteps, ntrials, state_dim)\n\nNote\n\nThe function computes the expected values for all trials.\nFor single-trial data, use inputs with n_trials = 1.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.update_C!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 3}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 3}, Vector{Float64}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.GaussianObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.update_C!","text":"update_C!(lds::LinearDynamicalSystem{S,O}, E_z::Array{T,3}, E_zz::Array{T,4}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nUpdate the observation matrix C of the Linear Dynamical System.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials)\ny::Array{T,3}: Observed data, size (obsdim, Tsteps, n_trials)\n\nNote\n\nThis function modifies lds in-place.\nThe update is only performed if lds.fit_bool[5] is true.\nThe result is averaged across all trials.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.update_Q!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 4}, Array{T, 4}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.update_Q!","text":"update_Q!(lds::LinearDynamicalSystem{S,O}, E_zz::Array{T, 4}, E_zz_prev::Array{T, 4}) where {T<:Real, S<:GaussianStateModel{T}, O<:AbstractObservationModel{T}}\n\nUpdate the process noise covariance matrix Q of the Linear Dynamical System.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\nE_zz::Array{T, 4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials)\nE_zz_prev::Array{T, 4}: Expected zt * z{t-1}', size (statedim, statedim, Tsteps, ntrials)\n\nNote\n\nThis function modifies lds in-place.\nThe update is only performed if lds.fit_bool[4] is true.\nThe result is averaged across all trials.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.update_R!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 3}}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 3}, Vector{Float64}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.GaussianObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.update_R!","text":"update_R!(lds::LinearDynamicalSystem{S,O}, E_z::Array{T,3}, E_zz::Array{T,4}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nUpdate the observation noise covariance matrix R of the Linear Dynamical System.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials)\ny::Array{T,3}: Observed data, size (obsdim, Tsteps, n_trials)\n\nNote\n\nThis function modifies lds in-place.\nThe update is only performed if lds.fit_bool[6] is true.\nThe result is averaged across all trials.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.update_initial_state_covariance!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.update_initial_state_covariance!","text":"update_initial_state_covariance!(lds::LinearDynamicalSystem{S,O}, E_z::Array{T,3}, E_zz::Array{T,4}) where {T<:Real, S<:GaussianStateModel{T}, O<:AbstractObservationModel{T}}\n\nUpdate the initial state covariance of the Linear Dynamical System using the average across all trials.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\nE_zz::Array{T,4}: Expected zt * zt', size (statedim, statedim, Tsteps, ntrials, state_dim)\n\nNote\n\nThis function modifies lds in-place.\nThe update is only performed if lds.fit_bool[2] is true.\nThe initial state covariance is computed as the average of the first time step across all trials.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.update_initial_state_mean!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.update_initial_state_mean!","text":"update_initial_state_mean!(lds::LinearDynamicalSystem{S,O}, E_z::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:AbstractObservationModel{T}}\n\nUpdate the initial state mean of the Linear Dynamical System using the average across all trials.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System struct\nE_z::Array{T,3}: Expected latent states, size (statedim, statedim, Tsteps, ntrials)\n\nNote\n\nThis function modifies lds in-place.\nThe update is only performed if lds.fit_bool[1] is true.\nThe initial state mean is computed as the average of the first time step across all trials.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.update_observation_model!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{StateSpaceDynamics.LinearDynamicalSystem{S, O}, Array{T, 3}, Array{T, 4}, Array{T, 3}}} where {T<:Real, S<:StateSpaceDynamics.GaussianStateModel{T}, O<:StateSpaceDynamics.PoissonObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.update_observation_model!","text":"update_observation_model!(plds::LinearDynamicalSystem{S,O}, E_z::Array{T, 3}, P_smooth::Array{T, 4}, y::Array{T, 3}) where {T<:Real, S<:GaussianStateModel{T}, O<:PoissonObservationModel{T}}\n\nUpdate the observation model parameters of a Poisson Linear Dynamical System using gradient-based optimization.\n\nArguments\n\nplds::LinearDynamicalSystem{S,O}: The Poisson Linear Dynamical System model.\nE_z::Array{T, 3}: The expected latent states. Dimensions: (latentdim, TSteps, n_trials)\nP_smooth::Array{T, 4}: The smoothed state covariances. Dimensions: (latentdim, TSteps, ntrials, latentdim)\ny::Array{T, 3}: The observed data. Dimensions: (obsdim, Tsteps, n_trials)\n\nNote\n\nThis function modifies plds in-place by updating the observation model parameters (C and logd). The optimization is performed only if `plds.fitbool[5]` is true.\n\n\n\n\n\n","category":"method"}]
}
