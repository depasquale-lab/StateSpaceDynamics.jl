{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using Random\n",
    "using Statistics\n",
    "using PythonCall\n",
    "using DataFrames\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using CSV\n",
    "using Distributions\n",
    "using HiddenMarkovModels\n",
    "using StateSpaceDynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HiddenMarkovModels as HMMs\n",
    "using StatsAPI\n",
    "using StableRNGs\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching zero(::Type{Vector{Float64}})\n\nClosest candidates are:\n  zero(!Matched::Type{Union{}}, Any...)\n   @ Base number.jl:310\n  zero(!Matched::Type{Pkg.Resolve.FieldValue})\n   @ Pkg C:\\Users\\zachl\\.julia\\juliaup\\julia-1.10.4+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\Pkg\\src\\Resolve\\fieldvalues.jl:38\n  zero(!Matched::Type{Dates.DateTime})\n   @ Dates C:\\Users\\zachl\\.julia\\juliaup\\julia-1.10.4+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\Dates\\src\\types.jl:438\n  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching zero(::Type{Vector{Float64}})\n",
      "\n",
      "Closest candidates are:\n",
      "  zero(!Matched::Type{Union{}}, Any...)\n",
      "   @ Base number.jl:310\n",
      "  zero(!Matched::Type{Pkg.Resolve.FieldValue})\n",
      "   @ Pkg C:\\Users\\zachl\\.julia\\juliaup\\julia-1.10.4+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\Pkg\\src\\Resolve\\fieldvalues.jl:38\n",
      "  zero(!Matched::Type{Dates.DateTime})\n",
      "   @ Dates C:\\Users\\zachl\\.julia\\juliaup\\julia-1.10.4+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\Dates\\src\\types.jl:438\n",
      "  ...\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] promote_objtype(::LBFGS{Nothing, LineSearches.InitialStatic{Float64}, LineSearches.HagerZhang{Float64, Base.RefValue{Bool}}, Optim.var\"#19#21\"}, ::Vector{Vector{Float64}}, ::Symbol, ::Bool, ::Function, ::Function)\n",
      "   @ Optim C:\\Users\\zachl\\.julia\\packages\\Optim\\fBdaz\\src\\multivariate\\optimize\\interface.jl:65\n",
      " [2] optimize(f::Function, g::Function, initial_x::Vector{Vector{Float64}}, method::LBFGS{Nothing, LineSearches.InitialStatic{Float64}, LineSearches.HagerZhang{Float64, Base.RefValue{Bool}}, Optim.var\"#19#21\"}, options::Optim.Options{Float64, Nothing}; inplace::Bool, autodiff::Symbol)\n",
      "   @ Optim C:\\Users\\zachl\\.julia\\packages\\Optim\\fBdaz\\src\\multivariate\\optimize\\interface.jl:154\n",
      " [3] optimize(f::Function, g::Function, initial_x::Vector{Vector{Float64}}, method::LBFGS{Nothing, LineSearches.InitialStatic{Float64}, LineSearches.HagerZhang{Float64, Base.RefValue{Bool}}, Optim.var\"#19#21\"}, options::Optim.Options{Float64, Nothing})\n",
      "   @ Optim C:\\Users\\zachl\\.julia\\packages\\Optim\\fBdaz\\src\\multivariate\\optimize\\interface.jl:151\n",
      " [4] fit_bern!(model::ControlledBernoulliHMM{Float64}, X::Matrix{Float64}, y::Matrix{Float64}, β_shape::Tuple{Int64, Int64}, β::Vector{Vector{Float64}}, w::Vector{Float64})\n",
      "   @ Main c:\\Users\\zachl\\OneDrive\\Documents\\GitHub\\StateSpaceDynamics.jl\\benchmarking\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:64\n",
      " [5] fit_bern!(model::ControlledBernoulliHMM{Float64}, X::Matrix{Float64}, y::Matrix{Float64}, β_shape::Tuple{Int64, Int64}, β::Vector{Vector{Float64}})\n",
      "   @ Main c:\\Users\\zachl\\OneDrive\\Documents\\GitHub\\StateSpaceDynamics.jl\\benchmarking\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:48\n",
      " [6] top-level scope\n",
      "   @ c:\\Users\\zachl\\OneDrive\\Documents\\GitHub\\StateSpaceDynamics.jl\\benchmarking\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W2sZmlsZQ==.jl:34"
     ]
    }
   ],
   "source": [
    "latent_dim, input_dim, obs_dim = 2, 3, 1\n",
    "seq_len = 100\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare Data\n",
    "\"\"\"\n",
    "# Create true model\n",
    "true_model, dynamax_model, params, props = generate_random_hmm(latent_dim, input_dim, obs_dim)\n",
    "                    \n",
    "# Generate test data\n",
    "model, labels, Φ, data = generate_test_data(true_model, seq_len)\n",
    "\n",
    "# convert to vector\n",
    "obs_seq = vec(data)\n",
    "# convert Φ (featuresxseq_len) to a vector of 100 column vectors\n",
    "control_seq = [Φ[:, t] for t in 1:size(Φ, 2)]\n",
    "seq_ends = [length(obs_seq)]\n",
    "\n",
    "\"\"\"\n",
    "Initialize Model\n",
    "\"\"\"\n",
    "init = model.πₖ\n",
    "trans = model.A\n",
    "dist_coeffs = [-ones(input_dim), ones(input_dim)]  # This is where I define the betas\n",
    "hmm = ControlledBernoulliHMM(init, trans, dist_coeffs)\n",
    "\n",
    "\"\"\"\n",
    "Fit Model\n",
    "\"\"\"\n",
    "# hmm_est, loglikelihood_evolution = baum_welch(hmm, obs_seq, control_seq; seq_ends)\n",
    "# first(loglikelihood_evolution), last(loglikelihood_evolution)\n",
    "\n",
    "fit_bern!(hmm, Φ, data, (input_dim, obs_dim), dist_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: method definition for objective_bern at c:\\Users\\zachl\\OneDrive\\Documents\\GitHub\\StateSpaceDynamics.jl\\benchmarking\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:73 declares type variable T but does not use it.\n",
      "WARNING: method definition for objective_gradient_bern! at c:\\Users\\zachl\\OneDrive\\Documents\\GitHub\\StateSpaceDynamics.jl\\benchmarking\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W3sZmlsZQ==.jl:87 declares type variable T but does not use it.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Functions for fitting SwitchingBernoulliRegression using HMM.jl\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "logistic(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "struct ControlledBernoulliHMM{T} <: HMMs.AbstractHMM\n",
    "    init::Vector{T}\n",
    "    trans::Matrix{T}\n",
    "    dist_coeffs::Vector{Vector{T}}  # One vector for each state\n",
    "end\n",
    "\n",
    "function HMMs.initialization(hmm::ControlledBernoulliHMM)\n",
    "    return hmm.init\n",
    "end\n",
    "\n",
    "function HMMs.transition_matrix(hmm::ControlledBernoulliHMM, control::AbstractVector)\n",
    "    return hmm.trans\n",
    "end\n",
    "\n",
    "# Modified to use Bernoulli with probabilities from logistic regression\n",
    "function HMMs.obs_distributions(hmm::ControlledBernoulliHMM, control::AbstractVector)\n",
    "    return [\n",
    "        Bernoulli(logistic(dot(hmm.dist_coeffs[i], control))) for i in 1:length(hmm)\n",
    "    ]\n",
    "end\n",
    "\n",
    "struct RegressionOptimization{}\n",
    "    model::ControlledBernoulliHMM\n",
    "    X::Matrix{<:Real}\n",
    "    y::Matrix{<:Real}\n",
    "    w::Vector{Float64}\n",
    "    β_shape::Tuple{Int,Int}  # Added to track original shape\n",
    "end\n",
    "\n",
    "function fit_bern!(model::ControlledBernoulliHMM, \n",
    "    X::Matrix{<:Real},\n",
    "    y::Matrix{<:Real},\n",
    "    β_shape::Tuple{Int, Int},\n",
    "    β::Vector{Vector{Float64}},\n",
    "    w::Vector{Float64}=ones(size(y,1))\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Edited up to here\n",
    "    opt_problem = RegressionOptimization(model, X, y, w, β_shape)\n",
    "\n",
    "    # # Create closure functions for Optim.jl\n",
    "    f(β) = objective_bern(opt_problem, β)\n",
    "    g!(G, β) = objective_gradient_bern!(G, opt_problem, β)\n",
    "\n",
    "    opts = Optim.Options(;\n",
    "        x_abstol=1e-8,\n",
    "        x_reltol=1e-8,\n",
    "        f_abstol=1e-8,\n",
    "        f_reltol=1e-8,\n",
    "        g_abstol=1e-8,\n",
    "        g_reltol=1e-8,\n",
    "    )\n",
    "\n",
    "    # # Run optimization\n",
    "    result = optimize(f, g!, β, LBFGS(), opts)\n",
    "\n",
    "    # Update model parameters\n",
    "    # model.β = vec_to_matrix(result.minimizer, opt_problem.β_shape)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "# Bernoulli Regression Implementation\n",
    "function objective_bern(\n",
    "    opt, β_vec::Vector{}\n",
    ") where {T<:Real}\n",
    "    β_mat = vec_to_matrix(β_vec, opt.β_shape)\n",
    "    p = logistic.(opt.X * β_mat)\n",
    "\n",
    "    # calculate regularization\n",
    "    regularization = calc_regularization(β_mat, opt.model.λ, opt.model.include_intercept)\n",
    "\n",
    "    val = -sum(opt.w .* (opt.y .* log.(p) .+ (1 .- opt.y) .* log.(1 .- p))) + regularization\n",
    "\n",
    "    return val\n",
    "end\n",
    "\n",
    "function objective_gradient_bern!(\n",
    "    G::Vector{Float64},\n",
    "    opt,\n",
    "    β_vec::Vector{},\n",
    ") where {T<:Real}\n",
    "    β_mat = vec_to_matrix(β_vec, opt.β_shape)\n",
    "    p = logistic.(opt.X * β_mat)\n",
    "\n",
    "    # calc gradient of penalty\n",
    "    regularization = calc_regularization_gradient(\n",
    "        β_mat, opt.model.λ, opt.model.include_intercept\n",
    "    )\n",
    "\n",
    "    grad_mat = -(opt.X' * (opt.w .* (opt.y .- p))) + regularization\n",
    "    return G .= vec(grad_mat)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function StatsAPI.fit!(\n",
    "    hmm::ControlledBernoulliHMM{T},\n",
    "    fb_storage::HMMs.ForwardBackwardStorage,\n",
    "    obs_seq::AbstractVector,\n",
    "    control_seq::AbstractVector;\n",
    "    seq_ends,\n",
    ") where {T}\n",
    "    (; γ, ξ) = fb_storage\n",
    "    N = length(hmm)\n",
    "\n",
    "    # Update initial probabilities and transition matrix\n",
    "    hmm.init .= 0\n",
    "    hmm.trans .= 0\n",
    "    for k in eachindex(seq_ends)\n",
    "        t1, t2 = HMMs.seq_limits(seq_ends, k)\n",
    "        hmm.init .+= γ[:, t1]\n",
    "        hmm.trans .+= sum(ξ[t1:t2])\n",
    "    end\n",
    "    hmm.init ./= sum(hmm.init)\n",
    "    for row in eachrow(hmm.trans)\n",
    "        row ./= sum(row)\n",
    "    end\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    UPDATE THIS PART FOR BERNOULLI REGRESSION\n",
    "    \"\"\"\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Python: <module 'jax.numpy' from 'c:\\\\Users\\\\zachl\\\\OneDrive\\\\Documents\\\\GitHub\\\\StateSpaceDynamics.jl\\\\benchmarking\\\\.CondaPkg\\\\env\\\\Lib\\\\site-packages\\\\jax\\\\numpy\\\\__init__.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const np = pyimport(\"numpy\")\n",
    "const dynamax = pyimport(\"dynamax.hidden_markov_model\")\n",
    "const jr = pyimport(\"jax.random\")\n",
    "const jnp = pyimport(\"jax.numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benchmark_fitting (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "struct BenchConfig\n",
    "    latent_dims::Vector{Int}\n",
    "    input_dims::Vector{Int}\n",
    "    obs_dims::Vector{Int} \n",
    "    seq_lengths::Vector{Int}\n",
    "    n_iters::Int\n",
    "    n_repeats::Int\n",
    "end\n",
    "\n",
    "default_config = BenchConfig(\n",
    "    [2,4,8,16],       # latent dimensions\n",
    "    [2,4,8,16],        # input dimension\n",
    "    [1],      # observation dimensions \n",
    "    [100, 500, 1000],    # sequence lengths\n",
    "    200,                 # EM iterations\n",
    "    5                    # benchmark repeats\n",
    ")\n",
    "\n",
    "function initialize_transition_matrix(K::Int)\n",
    "    # Initialize a transition matrix with zeros\n",
    "    A = zeros(Float64, K, K)\n",
    "    \n",
    "    for i in 1:K\n",
    "        # Sample from a Dirichlet distribution\n",
    "        A[i, :] = rand(Dirichlet(ones(K)))\n",
    "    end\n",
    "\n",
    "    A .+= 0.5.*I(K)\n",
    "    A .= A ./ sum(A, dims=2)\n",
    "    return A\n",
    "end\n",
    "\n",
    "function initialize_state_distribution(K::Int)\n",
    "    # initialize a state distribution\n",
    "    return rand(Dirichlet(ones(K)))\n",
    "end\n",
    "\n",
    "\n",
    "function generate_random_hmm(latent_dim::Int, input_dim::Int, obs_dim::Int)\n",
    "    \"\"\"\n",
    "    Create the StateSpaceDynamics.jl Model\n",
    "    \"\"\"\n",
    "    # Create Gaussian Emission Models with random means and covariances\n",
    "    emissions = Vector{BernoulliRegressionEmission}(undef, latent_dim)\n",
    "    true_model = StateSpaceDynamics.SwitchingBernoulliRegression(K=latent_dim, input_dim=input_dim, output_dim=obs_dim, include_intercept=false)\n",
    "\n",
    "    # Make the dynamax emission weights\n",
    "    key=jr.PRNGKey(1)\n",
    "    emission_dyna = jr.uniform(key, shape=(latent_dim, input_dim))\n",
    "    emission_biases=jnp.zeros(input_dim)\n",
    "    emission_ssd = pyconvert(Matrix, emission_dyna)\n",
    "    emission_ssd = convert(Matrix{Float64}, emission_ssd)\n",
    "\n",
    "    # Loop through each row of the emission_ssd matrix\n",
    "    for (state, row) in enumerate(eachrow(emission_ssd))\n",
    "        β = Matrix(reshape(row, :, 1))\n",
    "        true_model.B[state] = BernoulliRegressionEmission(input_dim=input_dim, output_dim=obs_dim, β=β, include_intercept=false)\n",
    "    end\n",
    "\n",
    "    true_model.A = initialize_transition_matrix(latent_dim)\n",
    "    true_model.πₖ = initialize_state_distribution(latent_dim)\n",
    "\n",
    "    \"\"\"\n",
    "    Create the Dynamax Model\n",
    "    \"\"\"\n",
    "    # Convert Julia parameters to NumPy arrays\n",
    "    initial_probs = jnp.array(true_model.πₖ)  # Convert initial state probabilities\n",
    "    transition_matrix = jnp.array(true_model.A)  # Convert transition matrix\n",
    "\n",
    "    dynamax_model = dynamax.LogisticRegressionHMM(\n",
    "    num_states=latent_dim,\n",
    "    input_dim=input_dim\n",
    "    )\n",
    "\n",
    "    params, props = dynamax_model.initialize(\n",
    "        method=\"prior\",\n",
    "        initial_probs=initial_probs,\n",
    "        transition_matrix=transition_matrix,\n",
    "        emission_weights=emission_dyna,\n",
    "        emission_biases=emission_biases\n",
    "    )\n",
    "\n",
    "    return true_model, dynamax_model, params, props\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function generate_test_data(model, seq_len::Int)\n",
    "    # Generate random input data\n",
    "    Φ = randn(model.B[1].input_dim, seq_len)\n",
    "\n",
    "    # Sample from the model\n",
    "    labels, data = StateSpaceDynamics.sample(model, Φ, n=seq_len)\n",
    "\n",
    "    return model, labels, Φ, data\n",
    "end\n",
    "\n",
    "\n",
    "function run_single_benchmark(model_type::Symbol, hmm_ssd, y, inputs, params=nothing, props=nothing; config=default_config)\n",
    "    if model_type == :julia\n",
    "        bench = @benchmark begin\n",
    "            model = deepcopy($hmm_ssd)  # Create a fresh copy for each iteration\n",
    "            StateSpaceDynamics.fit!(model, $y, $inputs, max_iters=200, tol=1e-15)\n",
    "        end samples=config.n_repeats\n",
    "        return (time=median(bench).time, memory=bench.memory, allocs=bench.allocs, success=true)\n",
    "    else\n",
    "        bench = @benchmark begin\n",
    "            dynamax_model=deepcopy($hmm_ssd)\n",
    "            p = deepcopy($params)\n",
    "            pr = deepcopy($props)\n",
    "            dynamax_model.fit_em(p, pr, $y, $inputs, num_iters=200,verbose=false)\n",
    "        end samples=config.n_repeats\n",
    "        return (time=median(bench).time, memory=bench.memory, allocs=bench.allocs, success=true)        \n",
    "    end\n",
    "end\n",
    "\n",
    "function benchmark_fitting(config::BenchConfig = default_config)\n",
    "    results = []\n",
    "\n",
    "    for latent_dim in config.latent_dims\n",
    "        for input_dim in config.input_dims\n",
    "            for obs_dim in config.obs_dims\n",
    "                for seq_len in config.seq_lengths\n",
    "                    println(\"\\nTesting configuration: latent_dim=$latent_dim, input_dim=$input_dim, obs_dim=$obs_dim, seq_len=$seq_len\")\n",
    "\n",
    "                    # Create true model\n",
    "                    true_model, dynamax_model, params, props = generate_random_hmm(latent_dim, input_dim, obs_dim)\n",
    "                    \n",
    "                    # Generate test data\n",
    "                    model, labels, Φ, data = generate_test_data(true_model, seq_len)\n",
    "                    vectorized_data = [data[:, i] for i in 1:size(data, 2)]  # Vectorize for HMMjl\n",
    "\n",
    "                    # Convert inputs to NumPy format (inputs are seq_len x input_dim in dynamax)\n",
    "                    inputs_np = np.array(Φ')\n",
    "                    data_np = np.array(data)[0]\n",
    "                    labels_np = np.array(labels .- 1)  # Dynamax expects labels indexed from 0\n",
    "\n",
    "                    # Generate random HMMs for fitting\n",
    "                    test_model, dynamax_model, params, props = generate_random_hmm(latent_dim, input_dim, obs_dim)\n",
    "\n",
    "                    # Run benchmarks separately with error handling\n",
    "                    julia_result = try\n",
    "                        run_single_benchmark(:julia, test_model, data, Φ)\n",
    "                    catch err\n",
    "                        println(\"Error in SSD.jl benchmarking: \", err)\n",
    "                        (time=\"FAIL\", memory=\"FAIL\", allocs=\"FAIL\", success=false)\n",
    "                    end\n",
    "\n",
    "                    dynamax_result = try\n",
    "                        run_single_benchmark(:dynamax, dynamax_model, data_np, inputs_np, params, props)\n",
    "                    catch err\n",
    "                        println(\"Error in dynamax benchmarking: \", err)\n",
    "                        (time=\"FAIL\", memory=\"FAIL\", allocs=\"FAIL\", success=false)\n",
    "                    end\n",
    "\n",
    "                    # Save results\n",
    "                    push!(results, Dict(\n",
    "                        \"config\" => (latent_dim=latent_dim, input_dim=input_dim, obs_dim=obs_dim, seq_len=seq_len),\n",
    "                        \"SSD.jl\" => julia_result,\n",
    "                        \"Dynamax\" => dynamax_result\n",
    "                    ))\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmark_fitting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function prepare_results_for_csv(results)\n",
    "    rows = []\n",
    "    for result in results\n",
    "        config = result[\"config\"]\n",
    "        ssd = result[\"SSD.jl\"]\n",
    "        Dynamax = result[\"Dynamax\"]\n",
    "\n",
    "        # Add a row for SSD.jl\n",
    "        push!(rows, (\n",
    "            latent_dim=config.latent_dim,\n",
    "            obs_dim=config.obs_dim,\n",
    "            seq_len=config.seq_len,\n",
    "            library=\"SSD.jl\",\n",
    "            time=ssd.time,\n",
    "            memory=ssd.memory,\n",
    "            allocs=ssd.allocs,\n",
    "            success=ssd.success,\n",
    "        ))\n",
    "\n",
    "        # Add a row for HMM.jl\n",
    "        push!(rows, (\n",
    "            latent_dim=config.latent_dim,\n",
    "            obs_dim=config.obs_dim,\n",
    "            seq_len=config.seq_len,\n",
    "            library=\"Dynamax\",\n",
    "            time=Dynamax.time,\n",
    "            memory=Dynamax.memory,\n",
    "            allocs=Dynamax.allocs,\n",
    "            success=Dynamax.success,\n",
    "        ))\n",
    "    end\n",
    "    return DataFrame(rows)\n",
    "end\n",
    "\n",
    "results_df = prepare_results_for_csv(results)\n",
    "CSV.write(\"benchmark_results.csv\", results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function transform_to_df(data_vector::Vector)\n",
    "    # Initialize vectors for all our columns\n",
    "    packages = String[]\n",
    "    times = Float64[]\n",
    "    memories = Int[]\n",
    "    allocs = Int[]\n",
    "    successes = Bool[]\n",
    "    latent_dims = Int[]\n",
    "    obs_dims = Int[]\n",
    "    seq_lens = Int[]\n",
    "    \n",
    "    # Process each dictionary in the vector\n",
    "    for dict in data_vector\n",
    "        # Get configuration values for this batch\n",
    "        config = dict[\"config\"]\n",
    "        latent_dim = config.latent_dim\n",
    "        obs_dim = config.obs_dim\n",
    "        seq_len = config.seq_len\n",
    "        \n",
    "        # Process each package's results\n",
    "        for (pkg_name, results) in dict\n",
    "            if pkg_name != \"config\"\n",
    "                push!(packages, pkg_name)\n",
    "                push!(times, results.time)\n",
    "                push!(memories, results.memory)\n",
    "                push!(allocs, results.allocs)\n",
    "                push!(successes, results.success)\n",
    "                push!(latent_dims, latent_dim)\n",
    "                push!(obs_dims, obs_dim)\n",
    "                push!(seq_lens, seq_len)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    DataFrame(\n",
    "        package = packages,\n",
    "        time = times,\n",
    "        memory = memories,\n",
    "        allocs = allocs,\n",
    "        success = successes,\n",
    "        latent_dim = latent_dims,\n",
    "        obs_dim = obs_dims,\n",
    "        seq_length = seq_lens\n",
    "    )\n",
    "end\n",
    "\n",
    "function plot_benchmarks(df::DataFrame)\n",
    "    # Create a unique identifier for each obs_dim/latent_dim combination\n",
    "    df.dim_combo = string.(df.obs_dim, \"x\", df.latent_dim)\n",
    "    \n",
    "    # Define line styles that will cycle if we have more combinations than styles\n",
    "    base_styles = [:solid, :dash, :dot, :dashdot, :dashdotdot]\n",
    "    dim_combos = unique(df.dim_combo)\n",
    "    \n",
    "    # Create style dictionary by cycling through available styles\n",
    "    style_dict = Dict(\n",
    "        combo => base_styles[mod1(i, length(base_styles))] \n",
    "        for (i, combo) in enumerate(dim_combos)\n",
    "    )\n",
    "    \n",
    "    # Create the plot\n",
    "    p = plot(\n",
    "        xlabel=\"Sequence Length\",\n",
    "        ylabel=\"Time (seconds)\",\n",
    "        title=\"Package Performance Across Sequence Lengths\",\n",
    "        legend=:outertopright,\n",
    "        xscale=:log10,\n",
    "        yscale=:log10\n",
    "    )\n",
    "    \n",
    "    # Plot each package with a different color\n",
    "    packages = unique(df.package)\n",
    "    for (i, pkg) in enumerate(packages)\n",
    "        pkg_data = df[df.package .== pkg, :]\n",
    "        \n",
    "        # Plot each dimension combination for this package\n",
    "        for dim_combo in dim_combos\n",
    "            combo_data = pkg_data[pkg_data.dim_combo .== dim_combo, :]\n",
    "            if !isempty(combo_data)\n",
    "                plot!(\n",
    "                    p,\n",
    "                    combo_data.seq_length,\n",
    "                    combo_data.time ./ 1e9,  # Convert to seconds\n",
    "                    label=\"$(pkg) ($(dim_combo))\",\n",
    "                    color=i,\n",
    "                    linestyle=style_dict[dim_combo],\n",
    "                    marker=:circle,\n",
    "                    markersize=4\n",
    "                )\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Add gridlines and adjust layout\n",
    "    plot!(\n",
    "        p,\n",
    "        grid=true,\n",
    "        minorgrid=true,\n",
    "        size=(900, 600),\n",
    "        margin=10Plots.mm\n",
    "    )\n",
    "    \n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_to_df(results)\n",
    "df.time = df.time / 1e9;\n",
    "\n",
    "CSV.write(\"benchmark_results_bernoulli.csv\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "benchmark_plot = plot_benchmarks(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
