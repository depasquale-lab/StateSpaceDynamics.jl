<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Hidden Markov Models · StateSpaceDynamics.jl</title><meta name="title" content="Hidden Markov Models · StateSpaceDynamics.jl"/><meta property="og:title" content="Hidden Markov Models · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Hidden Markov Models · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li class="is-active"><a class="tocitem" href>Hidden Markov Models</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#What-is-a-Generalized-Linear-Model-Hidden-Markov-Model"><span>What is a Generalized Linear Model - Hidden Markov Model</span></a></li><li class="toplevel"><a class="tocitem" href="#Sampling-from-an-HMM"><span>Sampling from an HMM</span></a></li><li class="toplevel"><a class="tocitem" href="#Learning-in-an-HMM"><span>Learning in an HMM</span></a></li><li class="toplevel"><a class="tocitem" href="#Inference-in-an-HMM"><span>Inference in an HMM</span></a></li><li class="toplevel"><a class="tocitem" href="#Reference"><span>Reference</span></a></li></ul></li><li><a class="tocitem" href="../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../EmissionModels/">Emission Models</a></li><li><a class="tocitem" href="../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../tutorials/poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../tutorials/lds_model_selection_example/">LDS Model Selection Example</a></li><li><a class="tocitem" href="../tutorials/lds_identifiability_example/">Non-Identifiability in LDS Models</a></li><li><a class="tocitem" href="../tutorials/hidden_markov_model_example/">Hidden Markov Model Example</a></li><li><a class="tocitem" href="../tutorials/hmm_model_selection_example/">HMM Model Selection</a></li><li><a class="tocitem" href="../tutorials/gaussian_glm_hmm_example/">Gaussian GLM-HMM Example</a></li><li><a class="tocitem" href="../tutorials/hmm_identifiability_example/">HMM Identifiability</a></li><li><a class="tocitem" href="../tutorials/gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li><a class="tocitem" href="../tutorials/poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li><a class="tocitem" href="../tutorials/Probabilistic_PCA_example/">Probabilistic PCA Example</a></li><li><a class="tocitem" href="../tutorials/switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Models</a></li><li class="is-active"><a href>Hidden Markov Models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Hidden Markov Models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/src/HiddenMarkovModels.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="What-is-a-Hidden-Markov-Model?"><a class="docs-heading-anchor" href="#What-is-a-Hidden-Markov-Model?">What is a Hidden Markov Model?</a><a id="What-is-a-Hidden-Markov-Model?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-Hidden-Markov-Model?" title="Permalink"></a></h1><p>A <strong>Hidden Markov Model (HMM)</strong> is a graphical model that describes how systems change over time. When modeling a time series with <span>$T$</span> observations using an HMM, we assume that the observed data <span>$y_{1:T}$</span> depends on hidden states <span>$x_{1:T}$</span> that are not observed. Specifically, an HMM is a type of <strong>state-space model</strong> in which the hidden states are discrete.</p><p>The three components of an HMM are as follows:</p><ul><li><strong>An initial state distribution (<span>$\pi$</span>):</strong> which hidden states we are likely to start in.</li><li><strong>A transition matrix (<span>$A$</span>):</strong> how the hidden states evolve over time.</li><li><strong>An emission model:</strong> how the hidden states generate the observed data.</li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.HiddenMarkovModel" href="#StateSpaceDynamics.HiddenMarkovModel"><code>StateSpaceDynamics.HiddenMarkovModel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HiddenMarkovModel</code></pre><p>Store a Hidden Markov Model (HMM) with custom emissions.</p><p><strong>Fields</strong></p><ul><li><code>A::AbstractMatrix{&lt;:Real}</code>: Transition matrix.</li><li><code>B::AbstractVector{&lt;:EmissionModel}</code>: State-dependent emission models.</li><li><code>πₖ::AbstractVector{&lt;:Real}</code>: Initial state distribution.</li><li><code>K::Int</code>: Number of states.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/1aa8d470fdf5a774918941c6e57503047da9b8f8/src/HiddenMarkovModels.jl#L5-L15">source</a></section></article><p>The generative model is given by:</p><p class="math-container">\[\begin{align*}
    x_1 &amp;\sim \text{Cat}(\pi) \\
    x_t &amp;\mid x_{t-1} \sim \text{Cat}(A_{x_{t-1}, :}) \\
    y_t &amp;\mid x_t \sim p(y_t \mid \theta_{x_t})
\end{align*}\]</p><p>Where:</p><ul><li><span>$x_t$</span> is the hidden (discrete) state at time <span>$t$</span></li><li><span>$y_t$</span> is the observed data at time <span>$t$</span></li><li><span>$\pi$</span> is the initial state distribution</li><li><span>$\mathbf{A}$</span> is the state transition matrix</li><li><span>$\theta_{x_t}$</span> are the parameters of the emission distribution for state <span>$x_t$</span></li></ul><p>The emission model can take many forms: Gaussian, Poisson, Bernoulli, categorical, etc... In the case of a Gaussian emission distribution, this becomes:</p><p class="math-container">\[y_t \mid (x_t = k) \sim \mathcal{N}(\mu_k, \Sigma_k)\]</p><p>Where:</p><ul><li><span>$\mu_k$</span> is the mean of the emission distribution for state <span>$k$</span></li><li><span>$\Sigma_k$</span> is the covariance of the emission distribution for state <span>$k$</span></li></ul><h1 id="What-is-a-Generalized-Linear-Model-Hidden-Markov-Model"><a class="docs-heading-anchor" href="#What-is-a-Generalized-Linear-Model-Hidden-Markov-Model">What is a Generalized Linear Model - Hidden Markov Model</a><a id="What-is-a-Generalized-Linear-Model-Hidden-Markov-Model-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-Generalized-Linear-Model-Hidden-Markov-Model" title="Permalink"></a></h1><p>A <strong>Hidden Markov Model - Generalized Linear Model (GLM-HMM)</strong> - also known as <strong>Switching Regression Model</strong> - is an extension to classic HMMs where the emission models are state-dependent GLMs that link an observed input to an observed output. This formulation allows each hidden state to define its own regression relationship between inputs and outputs, enabling the model to capture complex, state-dependent dynamics in the data. Currently, StateSpaceDynamics.jl support Gaussian, Bernoulli, Poisson, and Autoregressive GLMs as emission models.</p><p>The generative model is as follows:</p><p class="math-container">\[\begin{align*}
    x_1 &amp;\sim \text{Cat}(\pi) \\
    x_t &amp;\mid x_{t-1} \sim \text{Cat}(A_{x_{t-1}, :}) \\
    y_t &amp;\mid x_t, u_t \sim p(y_t \mid \theta_{x_t}, u_t)
\end{align*}\]</p><p>Where:</p><ul><li><span>$x_t$</span> is the hidden (discrete) state at time <span>$t$</span></li><li><span>$y_t$</span> is the observed output at time <span>$t$</span></li><li><span>$u_t$</span> is the observed input (covariate) at time <span>$t$</span></li><li><span>$\theta_{x_t}$</span> are the parameters of the GLM emission model for state <span>$x_t$</span></li></ul><h3 id="Example-Emission-Models"><a class="docs-heading-anchor" href="#Example-Emission-Models">Example Emission Models</a><a id="Example-Emission-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Example-Emission-Models" title="Permalink"></a></h3><p>For example, if the emission is a Gaussian GLM:</p><p class="math-container">\[y_t \mid (x_t = k), u_t \sim \mathcal{N}(\mu_k + \beta_k^\top u_t, \sigma_k^2)\]</p><p>Where:</p><ul><li><span>$\beta_k$</span> are the regression weights for state <span>$k$</span></li><li><span>$\sigma_k^2$</span> is the state-dependent variance</li><li><span>$\mu_k$</span> is the state-dependent bias</li></ul><p>If the emission is Bernoulli (for binary outputs):</p><p class="math-container">\[y_t \mid (x_t = k), u_t \sim \text{Bernoulli} \left( \sigma \left( \mu_k + \beta_k^\top u_t \right) \right)\]</p><p>Where:</p><ul><li><span>$\beta_k$</span> are the regression weights for state <span>$k$</span></li><li><span>$\sigma(\cdot)$</span> is the logistic sigmoid function for binary outputs</li><li><span>$\mu_k$</span> is the state-dependent bias</li></ul><h1 id="Sampling-from-an-HMM"><a class="docs-heading-anchor" href="#Sampling-from-an-HMM">Sampling from an HMM</a><a id="Sampling-from-an-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-from-an-HMM" title="Permalink"></a></h1><p>You can generate synthetic data from an HMM:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.rand" href="#Base.rand"><code>Base.rand</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Random.rand(
    rng::AbstractRNG,
    model::HiddenMarkovModel,
    X::Union{Matrix{&lt;:Real}, Nothing}=nothing;
    n::Int,
    autoregressive::Bool=false
)</code></pre><p>Generate <code>n</code> samples from a Hidden Markov Model. Returns a tuple of the state sequence and the observation sequence.</p><p><strong>Arguments</strong></p><ul><li><code>rng::AbstractRNG</code>: The seed.</li><li><code>model::HiddenMarkovModel</code>: The Hidden Markov Model to sample from.</li><li><code>X</code>: The input data for switching regression models.</li><li><code>n::Int</code>: The number of samples to generate.</li></ul><p><strong>Returns</strong></p><ul><li><code>state_sequence::Vector{Int}</code>: The state sequence, where each element is an integer 1:K.</li><li><code>observation_sequence::Matrix{Float64}</code>: The observation sequence. This takes the form of   the emission model&#39;s output.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/1aa8d470fdf5a774918941c6e57503047da9b8f8/src/HiddenMarkovModels.jl#L203-L225">source</a></section></article><h1 id="Learning-in-an-HMM"><a class="docs-heading-anchor" href="#Learning-in-an-HMM">Learning in an HMM</a><a id="Learning-in-an-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-in-an-HMM" title="Permalink"></a></h1><p><code>StateSpaceDynamics.jl</code> implements Expectation-Maximization (EM) for parameter learning in both HMMs and GLM-HMMs. EM is an iterative method for finding maximum likelihood estimates of the parameters in graphical models with hidden variables. </p><div class="admonition is-warning" id="Identifiability-caveats-in-HMMs-(and-GLM-HMMs)-749a0d28b697de66"><header class="admonition-header">Identifiability caveats in HMMs (and GLM-HMMs)<a class="admonition-anchor" href="#Identifiability-caveats-in-HMMs-(and-GLM-HMMs)-749a0d28b697de66" title="Permalink"></a></header><div class="admonition-body"><p>HMM parameters are <strong>not uniquely identifiable</strong>. For any permutation matrix <span>$P$</span> that relabels the <span>$K$</span> hidden states, the reparameterization</p><p class="math-container">\[\begin{aligned}
\pi&#39; &amp;= P\,\pi,\\
A&#39; &amp;= P\,A\,P^\top,\\
\theta&#39;_k &amp;= \theta_{P^\top k}
\end{aligned}\]</p><p>yields the <strong>same likelihood</strong>. Consequences:</p><ul><li><strong>Label switching:</strong> state indices are arbitrary; EM runs can return permuted states.</li><li><strong>Degenerate solutions:</strong> with Gaussian emissions, likelihood can blow up by shrinking a component’s variance onto a few points; with GLM emissions, <strong>separation</strong> or collinearity can make parameters diverge.</li><li><strong>Non-unique GLM parametrizations:</strong> standard GLM identifiability issues apply (e.g., intercept vs. redundant dummy variables, collinear covariates).</li></ul><p><strong>Practical remedies</strong></p><ul><li><strong>Canonicalize state order</strong> after each fit (or each EM iteration) using a criterion such as descending stationary probability, mean emission value, or a chosen scalar summary.</li><li><strong>Post-hoc alignment</strong> across runs: match states with a <strong>Hungarian/Procrustes</strong> step using emission statistics or posterior state occupancies.</li><li><strong>Regularize emissions:</strong>   Gaussian: add priors/penalties, variance floors, tied/diagonal <span>$\Sigma_k$</span>;   GLM: <span>$L_2/L_1$</span> penalties, remove/orthogonalize collinear features, use reference coding.</li><li><strong>Stabilize transitions:</strong> Dirichlet priors or pseudocounts on <span>$\pi$</span> and <span>$A$</span>; avoid zero rows/columns.</li><li><strong>Report with uncertainty:</strong> prefer state-invariant summaries (likelihood, predictive metrics). When interpreting parameters, note that labels are arbitrary.</li></ul></div></div><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{&lt;:Real}}}} where T&lt;:Real" href="#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{&lt;:Real}}}} where T&lt;:Real"><code>StateSpaceDynamics.fit!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fit!(
    model::HiddenMarkovModel,
    Y::Matrix{&lt;:Real},
    X::Union{Matrix{&lt;:Real}, Nothing}=nothing
    ;
    max_iters::Int=100,
    tol::Float64=1e-6
)</code></pre><p>Fit the Hidden Markov Model using the EM algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>model::HiddenMarkovModel</code>: The Hidden Markov Model to fit.</li><li><code>Y::Matrix{&lt;:Real}</code>: The emission data.</li><li><code>X::Union{Matrix{&lt;:Real}, Nothing}=nothing</code>: Optional input data for fitting Switching   Regression Models</li><li><code>max_iters::Int=100</code>: The maximum number of iterations to run the EM algorithm.</li><li><code>tol::Float64=1e-6</code>: When the log likelihood is improving by less than this value, the   algorithm will stop.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/1aa8d470fdf5a774918941c6e57503047da9b8f8/src/HiddenMarkovModels.jl#L677-L697">source</a></section></article><h3 id="Expectation-Step-(E-step)"><a class="docs-heading-anchor" href="#Expectation-Step-(E-step)">Expectation Step (E-step)</a><a id="Expectation-Step-(E-step)-1"></a><a class="docs-heading-anchor-permalink" href="#Expectation-Step-(E-step)" title="Permalink"></a></h3><p>In the <strong>expectation step (E-step)</strong>, we calculate the posterior distribution of the latent states given the current parameters of the model:</p><p class="math-container">\[p(X \mid Y, \theta_{\text{old}})\]</p><p>We use dynamic programming to efficiently calculate this posterior using the <strong>forward</strong> and <strong>backward</strong> recursions for HMMs. This posterior is then used to construct the expectation of the complete data log-likelihood, also known as the <strong>Q-function</strong>:</p><p class="math-container">\[Q(\theta, \theta_{\text{old}}) = \sum_X p(X \mid Y, \theta_{\text{old}}) \ln p(Y, X \mid \theta)\]</p><h3 id="Maximization-Step-(M-step)"><a class="docs-heading-anchor" href="#Maximization-Step-(M-step)">Maximization Step (M-step)</a><a id="Maximization-Step-(M-step)-1"></a><a class="docs-heading-anchor-permalink" href="#Maximization-Step-(M-step)" title="Permalink"></a></h3><p>In the <strong>maximization step (M-step)</strong>, we maximize this expectation with respect to the parameters <span>$\theta$</span>. Specifically:</p><ul><li>For the initial state distribution and the transition matrix, we use <strong>analytical updates</strong> for the parameters, derived using Lagrange multipliers.</li><li>For emission models in the case of HMMs, we also implement <strong>analytical updates</strong>.</li><li>If the emission model is a GLM, we use <code>Optim.jl</code> to <strong>numerically optimize</strong> the objective function.</li></ul><h1 id="Inference-in-an-HMM"><a class="docs-heading-anchor" href="#Inference-in-an-HMM">Inference in an HMM</a><a id="Inference-in-an-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-in-an-HMM" title="Permalink"></a></h1><p>For state inference in Hidden Markov Models (HMMs), we implement two common algorithms:</p><h3 id="Forward-Backward-Algorithm"><a class="docs-heading-anchor" href="#Forward-Backward-Algorithm">Forward-Backward Algorithm</a><a id="Forward-Backward-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-Backward-Algorithm" title="Permalink"></a></h3><p>The <strong>Forward-Backward</strong> algorithm is used to compute the <strong>posterior state probabilities</strong> at each time step. Given the observed data, it calculates the probability of being in each possible hidden state at each time step, marginalizing over all possible state sequences.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.class_probabilities-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{&lt;:Real}}}} where T&lt;:Real" href="#StateSpaceDynamics.class_probabilities-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{&lt;:Real}}}} where T&lt;:Real"><code>StateSpaceDynamics.class_probabilities</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">class_probabilities(
    model::HiddenMarkovModel,
    Y::Matrix{&lt;:Real},
    X::Union{Matrix{&lt;:Real},Nothing}=nothing;
)</code></pre><p>Calculate the class probabilities at each time point using forward backward algorithm</p><p><strong>Arguments</strong></p><ul><li><code>model::HiddenMarkovModel</code>: The Hidden Markov Model to fit.</li><li><code>Y::Matrix{&lt;:Real}</code>: The emission data</li><li><code>X::Union{Matrix{&lt;:Real},Nothing}=nothing</code>: Optional input data for fitting Switching   Regression Models</li></ul><p><strong>Returns</strong></p><ul><li><code>class_probabilities::Matrix{Float64}</code>: The class probabilities at each timepoint</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/1aa8d470fdf5a774918941c6e57503047da9b8f8/src/HiddenMarkovModels.jl#L819-L836">source</a></section></article><h3 id="Viterbi-Algorithm"><a class="docs-heading-anchor" href="#Viterbi-Algorithm">Viterbi Algorithm</a><a id="Viterbi-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Viterbi-Algorithm" title="Permalink"></a></h3><p>The <strong>Viterbi</strong> algorithm is used for <strong>best state sequence labeling</strong>. It finds the most likely sequence of hidden states given the observed data. This is done by dynamically computing the highest probability path through the state space, which maximizes the likelihood of the observed sequence.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.viterbi-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{&lt;:Real}}}} where T&lt;:Real" href="#StateSpaceDynamics.viterbi-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{&lt;:Real}}}} where T&lt;:Real"><code>StateSpaceDynamics.viterbi</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">viterbi(
    model::HiddenMarkovModel,
    Y::Matrix{&lt;:Real},
    X::Union{Matrix{&lt;:Real},Nothing}=nothing
)</code></pre><p>Get most likely class labels using the Viterbi algorithm</p><p><strong>Arguments</strong></p><ul><li><code>model::HiddenMarkovModel</code>: The Hidden Markov Model to fit.</li><li><code>Y::Matrix{&lt;:Real}</code>: The emission data</li><li><code>X::Union{Matrix{&lt;:Real},Nothing}=nothing</code>: Optional input data for fitting Switching   Regression Models</li></ul><p><strong>Returns</strong></p><ul><li><code>best_path::Vector{Float64}</code>: The most likely state label at each timepoint</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/1aa8d470fdf5a774918941c6e57503047da9b8f8/src/HiddenMarkovModels.jl#L895-L912">source</a></section></article><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><p>For a complete mathematical formulation of the relevant HMM and HMM-GLM learning and inference algorithms, we recommend <strong>Pattern Recognition and Machine Learning, Chapter 13</strong> by <strong>Christopher Bishop</strong>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../LinearDynamicalSystems/">« Linear Dynamical Systems</a><a class="docs-footer-nextpage" href="../SLDS/">Switching Linear Dynamical Systems »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 21 October 2025 17:55">Tuesday 21 October 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
