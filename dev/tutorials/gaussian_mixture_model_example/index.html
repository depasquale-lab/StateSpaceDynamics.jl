<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gaussian Mixture Model Example · StateSpaceDynamics.jl</title><meta name="title" content="Gaussian Mixture Model Example · StateSpaceDynamics.jl"/><meta property="og:title" content="Gaussian Mixture Model Example · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Gaussian Mixture Model Example · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../EmissionModels/">Emission Models</a></li><li><a class="tocitem" href="../../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../lds_model_selection_example/">LDS Model Selection Example</a></li><li><a class="tocitem" href="../lds_identifiability_example/">Non-Identifiability in LDS Models</a></li><li><a class="tocitem" href="../hidden_markov_model_example/">Hidden Markov Model Example</a></li><li><a class="tocitem" href="../hmm_model_selection_example/">HMM Model Selection</a></li><li><a class="tocitem" href="../gaussian_glm_hmm_example/">Gaussian GLM-HMM Example</a></li><li><a class="tocitem" href="../hmm_identifiability_example/">HMM Identifiability</a></li><li class="is-active"><a class="tocitem" href>Gaussian Mixture Model Example</a><ul class="internal"><li><a class="tocitem" href="#Load-Required-Packages"><span>Load Required Packages</span></a></li><li><a class="tocitem" href="#Create-a-True-Gaussian-Mixture-Model"><span>Create a True Gaussian Mixture Model</span></a></li><li><a class="tocitem" href="#Sample-Data-from-the-True-GMM"><span>Sample Data from the True GMM</span></a></li><li><a class="tocitem" href="#Fit-GMM-Using-EM-Algorithm"><span>Fit GMM Using EM Algorithm</span></a></li><li><a class="tocitem" href="#Visualize-Fitted-Model"><span>Visualize Fitted Model</span></a></li><li><a class="tocitem" href="#Component-Assignment-Analysis"><span>Component Assignment Analysis</span></a></li><li><a class="tocitem" href="#Final-Comparison-Visualization"><span>Final Comparison Visualization</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li><a class="tocitem" href="../Probabilistic_PCA_example/">Probabilistic PCA Example</a></li><li><a class="tocitem" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Gaussian Mixture Model Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Gaussian Mixture Model Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/examples/GaussianMixtureModel.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Simulating-and-Fitting-a-Gaussian-Mixture-Model"><a class="docs-heading-anchor" href="#Simulating-and-Fitting-a-Gaussian-Mixture-Model">Simulating and Fitting a Gaussian Mixture Model</a><a id="Simulating-and-Fitting-a-Gaussian-Mixture-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-and-Fitting-a-Gaussian-Mixture-Model" title="Permalink"></a></h1><p>This tutorial demonstrates how to use <code>StateSpaceDynamics.jl</code> to create a Gaussian Mixture Model (GMM) and fit it using the EM algorithm. Unlike Hidden Markov Models which model temporal sequences, GMMs are designed for clustering and density estimation of independent observations. Each data point is assumed to come from one of several Gaussian components, but there&#39;s no temporal dependence.</p><p>GMMs are fundamental in machine learning for unsupervised clustering, density estimation, anomaly detection, and as building blocks for more complex models. The key insight is that complex data distributions can often be well-approximated as mixtures of simpler Gaussian distributions, each representing a different &quot;mode&quot; or cluster in the data.</p><h2 id="Load-Required-Packages"><a class="docs-heading-anchor" href="#Load-Required-Packages">Load Required Packages</a><a id="Load-Required-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Required-Packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using StateSpaceDynamics
using LinearAlgebra
using Random
using Plots
using StableRNGs
using Distributions
using StatsPlots
using Combinatorics
using LaTeXStrings</code></pre><p>Set up reproducible random number generation</p><pre><code class="language-julia hljs">rng = StableRNG(1234);</code></pre><h2 id="Create-a-True-Gaussian-Mixture-Model"><a class="docs-heading-anchor" href="#Create-a-True-Gaussian-Mixture-Model">Create a True Gaussian Mixture Model</a><a id="Create-a-True-Gaussian-Mixture-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Create-a-True-Gaussian-Mixture-Model" title="Permalink"></a></h2><p>We&#39;ll create a &quot;ground truth&quot; GMM with known parameters, generate data from it, then see how well we can recover these parameters using only the observed data.</p><pre><code class="language-julia hljs">k = 3  # Number of mixture components (clusters)
D = 2  # Data dimensionality (2D for easy visualization)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2</code></pre><p>Define the true component means: <span>$\boldsymbol{\mu}_i \in \mathbb{R}^D$</span> for <span>$i = 1, \ldots, k$</span> Each column represents the mean vector <span>$\boldsymbol{\mu}_i$</span> for one component</p><pre><code class="language-julia hljs">true_μs = [
    -1.0  1.0  0.0;   # $x_1$ coordinates of the 3 component centers
    -1.0 -1.5  2.0    # $x_2$ coordinates of the 3 component centers
];  # Shape: $(D, k) = (2, 3)$</code></pre><p>Define covariance matrices <span>$\boldsymbol{\Sigma}_i$</span> for each component Using isotropic (spherical) covariances for simplicity</p><pre><code class="language-julia hljs">true_Σs = [Matrix{Float64}(0.3 * I(2)) for _ in 1:k];</code></pre><p>Define mixing weights <span>$\pi_i$</span> (must sum to 1) These represent <span>$P(\text{component} = i)$</span> for a random sample</p><pre><code class="language-julia hljs">true_πs = [0.5, 0.2, 0.3];  # Component 1 most likely, component 2 least likely</code></pre><p>Construct the complete GMM</p><pre><code class="language-julia hljs">true_gmm = GaussianMixtureModel(k, true_μs, true_Σs, true_πs);

print(&quot;Created GMM: $k components, $D dimensions\n&quot;)
for i in 1:k
    print(&quot;Component $i: μ = $(true_μs[:, i]), π = $(true_πs[i])\n&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Created GMM: 3 components, 2 dimensions
Component 1: μ = [-1.0, -1.0], π = 0.5
Component 2: μ = [1.0, -1.5], π = 0.2
Component 3: μ = [0.0, 2.0], π = 0.3</code></pre><h2 id="Sample-Data-from-the-True-GMM"><a class="docs-heading-anchor" href="#Sample-Data-from-the-True-GMM">Sample Data from the True GMM</a><a id="Sample-Data-from-the-True-GMM-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-Data-from-the-True-GMM" title="Permalink"></a></h2><p>Generate synthetic data from our true model. We&#39;ll sample both component assignments (for evaluation) and the actual observations.</p><pre><code class="language-julia hljs">n = 500  # Number of data points to generate</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">500</code></pre><p>Determine which component each sample comes from</p><pre><code class="language-julia hljs">labels = rand(rng, Categorical(true_πs), n);</code></pre><p>Count samples per component for verification</p><pre><code class="language-julia hljs">component_counts = [sum(labels .== i) for i in 1:k]
print(&quot;Samples per component: $(component_counts) (expected: $(round.(n .* true_πs)))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Samples per component: [252, 96, 152] (expected: [250.0, 100.0, 150.0])</code></pre><p>Generate the actual data points</p><pre><code class="language-julia hljs">X = Matrix{Float64}(undef, D, n)
for i in 1:n
    component = labels[i]
    X[:, i] = rand(rng, MvNormal(true_μs[:, component], true_Σs[component]))
end</code></pre><p>Visualize the generated data colored by true component membership</p><pre><code class="language-julia hljs">p1 = scatter(X[1, :], X[2, :];
    group=labels,
    title=&quot;True GMM Components&quot;,
    xlabel=L&quot;x_1&quot;, ylabel=L&quot;x_2&quot;,
    markersize=4,
    alpha=0.7,
    palette=:Set1_3,
    legend=:topright
)

for i in 1:k
    scatter!(p1, [true_μs[1, i]], [true_μs[2, i]];
        marker=:star, markersize=10, color=i,
        markerstrokewidth=2, markerstrokecolor=:black,
        label=&quot;&quot;)
end

p1</code></pre><img src="a9421305.svg" alt="Example block output"/><h2 id="Fit-GMM-Using-EM-Algorithm"><a class="docs-heading-anchor" href="#Fit-GMM-Using-EM-Algorithm">Fit GMM Using EM Algorithm</a><a id="Fit-GMM-Using-EM-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-GMM-Using-EM-Algorithm" title="Permalink"></a></h2><p>Now we simulate the realistic scenario: observe only data points <span>$\mathbf{X}$</span>, not the true component labels or parameters. Our goal is to recover the underlying mixture structure using EM.</p><p>Initialize a GMM with correct number of components but unknown parameters</p><pre><code class="language-julia hljs">fit_gmm = GaussianMixtureModel(k, D)

print(&quot;Running EM algorithm...&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running EM algorithm...</code></pre><p>Fit the model using EM algorithm</p><pre><code class="language-julia hljs">class_probabilities, lls = fit!(fit_gmm, X;
    maxiter=100,
    tol=1e-6,
    initialize_kmeans=true  # K-means initialization helps convergence
);

print(&quot;EM converged in $(length(lls)) iterations\n&quot;)
print(&quot;Log-likelihood improved by $(round(lls[end] - lls[1], digits=1))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Iteration 1: Log-likelihood = -1439.345491730744
Iteration 2: Log-likelihood = -1414.9776123839324
Iteration 3: Log-likelihood = -1396.1431130477552
Iteration 4: Log-likelihood = -1380.953360527758
Iteration 5: Log-likelihood = -1371.1369074917416
Iteration 6: Log-likelihood = -1365.3655466314203
Iteration 7: Log-likelihood = -1361.639335742245
Iteration 8: Log-likelihood = -1358.8041403262855
Iteration 9: Log-likelihood = -1356.3047654868813
Iteration 10: Log-likelihood = -1353.8769932554872
Iteration 11: Log-likelihood = -1351.4235080390201
Iteration 12: Log-likelihood = -1348.976151088614
Iteration 13: Log-likelihood = -1346.599148846448
Iteration 14: Log-likelihood = -1344.2564080625893
Iteration 15: Log-likelihood = -1341.8262949195014
Iteration 16: Log-likelihood = -1339.2492911606853
Iteration 17: Log-likelihood = -1336.6903122159245
Iteration 18: Log-likelihood = -1334.4697894159617
Iteration 19: Log-likelihood = -1332.73030509787
Iteration 20: Log-likelihood = -1331.4245542596734
Iteration 21: Log-likelihood = -1330.4735687802636
Iteration 22: Log-likelihood = -1329.8046568027362
Iteration 23: Log-likelihood = -1329.348592438472
Iteration 24: Log-likelihood = -1329.0429606780945
Iteration 25: Log-likelihood = -1328.837726502864
Iteration 26: Log-likelihood = -1328.696996981745
Iteration 27: Log-likelihood = -1328.5970565962157
Iteration 28: Log-likelihood = -1328.5230163270428
Iteration 29: Log-likelihood = -1328.4657476090567
Iteration 30: Log-likelihood = -1328.419662877381
Iteration 31: Log-likelihood = -1328.3812909243638
Iteration 32: Log-likelihood = -1328.3484185652742
Iteration 33: Log-likelihood = -1328.3195881575098
Iteration 34: Log-likelihood = -1328.2938051634671
Iteration 35: Log-likelihood = -1328.2703663154452
Iteration 36: Log-likelihood = -1328.2487566299244
Iteration 37: Log-likelihood = -1328.2285860589213
Iteration 38: Log-likelihood = -1328.2095493575957
Iteration 39: Log-likelihood = -1328.191399851236
Iteration 40: Log-likelihood = -1328.1739317181994
Iteration 41: Log-likelihood = -1328.156967602738
Iteration 42: Log-likelihood = -1328.140349619509
Iteration 43: Log-likelihood = -1328.123932534601
Iteration 44: Log-likelihood = -1328.1075783362664
Iteration 45: Log-likelihood = -1328.091151667906
Iteration 46: Log-likelihood = -1328.0745157564943
Iteration 47: Log-likelihood = -1328.0575285725283
Iteration 48: Log-likelihood = -1328.0400390275483
Iteration 49: Log-likelihood = -1328.021883069747
Iteration 50: Log-likelihood = -1328.002879590827
Iteration 51: Log-likelihood = -1327.9828261221464
Iteration 52: Log-likelihood = -1327.9614943908284
Iteration 53: Log-likelihood = -1327.9386259458831
Iteration 54: Log-likelihood = -1327.9139282714832
Iteration 55: Log-likelihood = -1327.8870720980165
Iteration 56: Log-likelihood = -1327.8576910057732
Iteration 57: Log-likelihood = -1327.8253848586146
Iteration 58: Log-likelihood = -1327.7897290019018
Iteration 59: Log-likelihood = -1327.7502912986652
Iteration 60: Log-likelihood = -1327.7066586332085
Iteration 61: Log-likelihood = -1327.6584731188013
Iteration 62: Log-likelihood = -1327.6054757314046
Iteration 63: Log-likelihood = -1327.5475518091023
Iteration 64: Log-likelihood = -1327.484769928267
Iteration 65: Log-likelihood = -1327.4174047260392
Iteration 66: Log-likelihood = -1327.3459365177187
Iteration 67: Log-likelihood = -1327.2710258006307
Iteration 68: Log-likelihood = -1327.1934669617544
Iteration 69: Log-likelihood = -1327.1141301119044
Iteration 70: Log-likelihood = -1327.0339013519742
Iteration 71: Log-likelihood = -1326.953630095879
Iteration 72: Log-likelihood = -1326.8740887185452
Iteration 73: Log-likelihood = -1326.7959463062
Iteration 74: Log-likelihood = -1326.7197556339759
Iteration 75: Log-likelihood = -1326.6459509758417
Iteration 76: Log-likelihood = -1326.5748538110622
Iteration 77: Log-likelihood = -1326.506683608289
Iteration 78: Log-likelihood = -1326.4415713301878
Iteration 79: Log-likelihood = -1326.3795738782235
Iteration 80: Log-likelihood = -1326.320688249782
Iteration 81: Log-likelihood = -1326.2648646414705
Iteration 82: Log-likelihood = -1326.2120180841805
Iteration 83: Log-likelihood = -1326.1620384439852
Iteration 84: Log-likelihood = -1326.114798786746
Iteration 85: Log-likelihood = -1326.0701622037966
Iteration 86: Log-likelihood = -1326.0279872500125
Iteration 87: Log-likelihood = -1325.9881321687058
Iteration 88: Log-likelihood = -1325.950458081262
Iteration 89: Log-likelihood = -1325.9148313109924
Iteration 90: Log-likelihood = -1325.8811249957312
Iteration 91: Log-likelihood = -1325.849220125583
Iteration 92: Log-likelihood = -1325.819006123434
Iteration 93: Log-likelihood = -1325.7903810675475
Iteration 94: Log-likelihood = -1325.7632516387855
Iteration 95: Log-likelihood = -1325.7375328599753
Iteration 96: Log-likelihood = -1325.7131476821146
Iteration 97: Log-likelihood = -1325.6900264612314
Iteration 98: Log-likelihood = -1325.668106360832
Iteration 99: Log-likelihood = -1325.6473307077626
Iteration 100: Log-likelihood = -1325.62764832378
EM converged in 100 iterations
Log-likelihood improved by 113.7</code></pre><p>Plot EM convergence</p><pre><code class="language-julia hljs">p2 = plot(lls, xlabel=&quot;EM Iteration&quot;, ylabel=&quot;Log-Likelihood&quot;,
          title=&quot;EM Algorithm Convergence&quot;, legend=false,
          marker=:circle, markersize=3, lw=2, color=:darkblue)

if length(lls) &lt; 100
    annotate!(p2, length(lls)*0.7, lls[end]*0.95,
        text(&quot;Converged in $(length(lls)) iterations&quot;, 10)) # Add convergence annotation
end</code></pre><h2 id="Visualize-Fitted-Model"><a class="docs-heading-anchor" href="#Visualize-Fitted-Model">Visualize Fitted Model</a><a id="Visualize-Fitted-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-Fitted-Model" title="Permalink"></a></h2><p>Create visualization showing both data and fitted GMM with probability contours. Create grid for plotting contours</p><pre><code class="language-julia hljs">x_range = range(extrema(X[1, :])..., length=100)
y_range = range(extrema(X[2, :])..., length=100)
xs = collect(x_range)
ys = collect(y_range)

p3 = scatter(X[1, :], X[2, :];
    markersize=3,
    alpha=0.5,
    color=:gray,
    xlabel=L&quot;x_1&quot;,
    ylabel=L&quot;x_2&quot;,
    title=&quot;Fitted GMM Components&quot;,
    legend=:topright,
    label=&quot;Data points&quot;
)

p3

colors = [:red, :green, :blue] # Plot probability density contours for each learned component
for i in 1:fit_gmm.k
    comp_dist = MvNormal(fit_gmm.μₖ[:, i], fit_gmm.Σₖ[i])
    Z_i = [fit_gmm.πₖ[i] * pdf(comp_dist, [x, y]) for y in ys, x in xs]

    contour!(p3, xs, ys, Z_i;
        levels=6,
        linewidth=2,
        c=colors[i],
        label=&quot;Component $i (π=$(round(fit_gmm.πₖ[i], digits=2)))&quot;
    )

    scatter!(p3, [fit_gmm.μₖ[1, i]], [fit_gmm.μₖ[2, i]];
        marker=:star, markersize=8, color=colors[i],
        markerstrokewidth=2, markerstrokecolor=:black,
        label=&quot;&quot;)
end</code></pre><h2 id="Component-Assignment-Analysis"><a class="docs-heading-anchor" href="#Component-Assignment-Analysis">Component Assignment Analysis</a><a id="Component-Assignment-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Component-Assignment-Analysis" title="Permalink"></a></h2><p>Use fitted model to assign each data point to its most likely component and compare with true assignments.</p><p>Get posterior probabilities: <span>$P(\text{component } i | \mathbf{x}_j)$</span></p><pre><code class="language-julia hljs">predicted_labels = [argmax(class_probabilities[:, j]) for j in 1:n];</code></pre><p>Calculate assignment accuracy (accounting for possible label permutation) Since EM can converge with components in different order</p><pre><code class="language-julia hljs">function best_permutation_accuracy(true_labels, pred_labels, k)
    best_acc = 0.0
    best_perm = collect(1:k)

    for perm in Combinatorics.permutations(1:k)
        mapped_pred = [perm[pred_labels[i]] for i in 1:length(pred_labels)]
        acc = mean(true_labels .== mapped_pred)
        if acc &gt; best_acc
            best_acc = acc
            best_perm = perm
        end
    end

    return best_acc, best_perm
end

accuracy, best_perm = best_permutation_accuracy(labels, predicted_labels, k)
print(&quot;Component assignment accuracy: $(round(accuracy*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Component assignment accuracy: 95.4%</code></pre><h2 id="Final-Comparison-Visualization"><a class="docs-heading-anchor" href="#Final-Comparison-Visualization">Final Comparison Visualization</a><a id="Final-Comparison-Visualization-1"></a><a class="docs-heading-anchor-permalink" href="#Final-Comparison-Visualization" title="Permalink"></a></h2><p>Side-by-side comparison of true vs learned component assignments</p><pre><code class="language-julia hljs">p_true = scatter(X[1, :], X[2, :]; group=labels, title=&quot;True Components&quot;,
                xlabel=L&quot;x_1&quot;, ylabel=L&quot;x_2&quot;, markersize=3, alpha=0.7,
                palette=:Set1_3, legend=false)

remapped_predicted = [best_perm[predicted_labels[i]] for i in 1:n] # Apply best permutation to predicted labels for fair comparison
p_learned = scatter(X[1, :], X[2, :]; group=remapped_predicted, title=&quot;Learned Components&quot;,
                   xlabel=L&quot;x_1&quot;, ylabel=L&quot;x_2&quot;, markersize=3, alpha=0.7,
                   palette=:Set1_3, legend=false)

p4 = plot(p_true, p_learned, layout=(1, 2), size=(800, 350))</code></pre><img src="4a9f00d4.svg" alt="Example block output"/><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This tutorial demonstrated the complete Gaussian Mixture Model workflow:</p><p><strong>Key Concepts:</strong></p><ul><li><strong>Mixture modeling</strong>: Complex distributions as weighted combinations of simpler Gaussians</li><li><strong>EM algorithm</strong>: Iterative parameter learning via expectation-maximization</li><li><strong>Soft clustering</strong>: Probabilistic component assignments rather than hard clusters</li><li><strong>Label permutation</strong>: Handling component identifiability issues</li></ul><p><strong>Applications:</strong></p><ul><li>Unsupervised clustering and density estimation</li><li>Anomaly detection via likelihood thresholding</li><li>Dimensionality reduction (when extended to factor analysis)</li><li>Building blocks for more complex probabilistic models</li></ul><p><strong>Technical Insights:</strong></p><ul><li>K-means initialization significantly improves EM convergence</li><li>Log-likelihood monitoring ensures proper algorithm behavior</li><li>Parameter recovery quality depends on component separation and sample size</li></ul><p>GMMs provide a flexible, interpretable framework for modeling heterogeneous data with multiple underlying modes or clusters, forming the foundation for many advanced machine learning techniques.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../hmm_identifiability_example/">« HMM Identifiability</a><a class="docs-footer-nextpage" href="../poisson_mixture_model_example/">Poisson Mixture Model Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Monday 22 September 2025 16:02">Monday 22 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
