<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Poisson Mixture Model Example · StateSpaceDynamics.jl</title><meta name="title" content="Poisson Mixture Model Example · StateSpaceDynamics.jl"/><meta property="og:title" content="Poisson Mixture Model Example · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Poisson Mixture Model Example · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../EmissionModels/">Emission Models</a></li><li><a class="tocitem" href="../../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../lds_model_selection_example/">LDS Model Selection Example</a></li><li><a class="tocitem" href="../lds_identifiability_example/">Non-Identifiability in LDS Models</a></li><li><a class="tocitem" href="../hidden_markov_model_example/">Hidden Markov Model Example</a></li><li><a class="tocitem" href="../hmm_model_selection_example/">HMM Model Selection</a></li><li><a class="tocitem" href="../gaussian_glm_hmm_example/">Gaussian GLM-HMM Example</a></li><li><a class="tocitem" href="../hmm_identifiability_example/">HMM Identifiability</a></li><li><a class="tocitem" href="../gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li class="is-active"><a class="tocitem" href>Poisson Mixture Model Example</a><ul class="internal"><li><a class="tocitem" href="#What-is-a-Poisson-Mixture-Model?"><span>What is a Poisson Mixture Model?</span></a></li><li><a class="tocitem" href="#EM-Algorithm-Overview"><span>EM Algorithm Overview</span></a></li><li><a class="tocitem" href="#Load-Required-Packages"><span>Load Required Packages</span></a></li><li><a class="tocitem" href="#Create-True-Poisson-Mixture-Model"><span>Create True Poisson Mixture Model</span></a></li><li><a class="tocitem" href="#Generate-Synthetic-Data"><span>Generate Synthetic Data</span></a></li><li><a class="tocitem" href="#Fit-Poisson-Mixture-Model-with-EM"><span>Fit Poisson Mixture Model with EM</span></a></li><li><a class="tocitem" href="#Monitor-EM-Convergence"><span>Monitor EM Convergence</span></a></li><li><a class="tocitem" href="#Visual-Model-Assessment"><span>Visual Model Assessment</span></a></li><li><a class="tocitem" href="#Posterior-Responsibilities-(Soft-Clustering)"><span>Posterior Responsibilities (Soft Clustering)</span></a></li><li><a class="tocitem" href="#Information-Criteria-for-Model-Selection"><span>Information Criteria for Model Selection</span></a></li><li><a class="tocitem" href="#Parameter-Recovery-Assessment"><span>Parameter Recovery Assessment</span></a></li><li><a class="tocitem" href="#Model-Selection-Example"><span>Model Selection Example</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../Probabilistic_PCA_example/">Probabilistic PCA Example</a></li><li><a class="tocitem" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Poisson Mixture Model Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Poisson Mixture Model Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/examples/PoissonMixtureModel.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Simulating-and-Fitting-a-Poisson-Mixture-Model"><a class="docs-heading-anchor" href="#Simulating-and-Fitting-a-Poisson-Mixture-Model">Simulating and Fitting a Poisson Mixture Model</a><a id="Simulating-and-Fitting-a-Poisson-Mixture-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-and-Fitting-a-Poisson-Mixture-Model" title="Permalink"></a></h1><p>This tutorial demonstrates how to build and fit a <strong>Poisson Mixture Model (PMM)</strong> with <code>StateSpaceDynamics.jl</code> using the Expectation-Maximization (EM) algorithm. We&#39;ll cover simulation, fitting, diagnostics, interpretation, and practical considerations.</p><h2 id="What-is-a-Poisson-Mixture-Model?"><a class="docs-heading-anchor" href="#What-is-a-Poisson-Mixture-Model?">What is a Poisson Mixture Model?</a><a id="What-is-a-Poisson-Mixture-Model?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-Poisson-Mixture-Model?" title="Permalink"></a></h2><p>A PMM assumes each observation <span>$x_i \in \{0,1,2,\ldots\}$</span> is drawn from one of <span>$k$</span> Poisson distributions with rates <span>$\lambda_1,\ldots,\lambda_k$</span>. The component assignment is a latent categorical variable <span>$z_i \in \{1,\ldots,k\}$</span> with mixing weights <span>$\pi_1,\ldots,\pi_k$</span> where <span>$\sum_j \pi_j = 1$</span>.</p><p><strong>Generative process:</strong></p><ol><li>Draw <span>$z_i \sim \text{Categorical}(\boldsymbol{\pi})$</span></li><li>Given <span>$z_i = j$</span>, draw <span>$x_i \sim \text{Poisson}(\lambda_j)$</span></li></ol><p>PMMs are useful for <strong>count data</strong> from heterogeneous sub-populations (e.g., spike counts from different neuron types, customer transaction counts from different segments, or event frequencies across different regimes).</p><h2 id="EM-Algorithm-Overview"><a class="docs-heading-anchor" href="#EM-Algorithm-Overview">EM Algorithm Overview</a><a id="EM-Algorithm-Overview-1"></a><a class="docs-heading-anchor-permalink" href="#EM-Algorithm-Overview" title="Permalink"></a></h2><p>EM maximizes the marginal log-likelihood <span>$\log p(\mathbf{x} | \boldsymbol{\pi}, \boldsymbol{\lambda})$</span> by iterating:</p><ul><li><strong>E-step:</strong> Compute responsibilities <span>$\gamma_{ij} = P(z_i = j | x_i, \boldsymbol{\theta})$</span></li><li><strong>M-step:</strong> Update parameters to maximize expected complete-data log-likelihood</li></ul><p>For Poisson mixtures, the M-step has closed-form updates: <span>$\pi_j \leftarrow \frac{1}{n} \sum_i \gamma_{ij}, \quad \lambda_j \leftarrow \frac{\sum_i \gamma_{ij} x_i}{\sum_i \gamma_{ij}}$</span></p><h2 id="Load-Required-Packages"><a class="docs-heading-anchor" href="#Load-Required-Packages">Load Required Packages</a><a id="Load-Required-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Required-Packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using StateSpaceDynamics
using LinearAlgebra
using Random
using Plots
using StableRNGs
using StatsPlots
using Distributions</code></pre><p>Fix RNG for reproducible simulation and k-means seeding</p><pre><code class="language-julia hljs">rng = StableRNG(1234);</code></pre><h2 id="Create-True-Poisson-Mixture-Model"><a class="docs-heading-anchor" href="#Create-True-Poisson-Mixture-Model">Create True Poisson Mixture Model</a><a id="Create-True-Poisson-Mixture-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Create-True-Poisson-Mixture-Model" title="Permalink"></a></h2><p>We&#39;ll simulate from a mixture of <span>$k=3$</span> Poisson components with distinct rates and mixing weights. These parameters create well-separated components that should be recoverable by EM.</p><pre><code class="language-julia hljs">k = 3
true_λs = [5.0, 10.0, 25.0]   # Poisson rates per component
true_πs = [0.25, 0.45, 0.30]  # Mixing weights (sum to 1)

true_pmm = PoissonMixtureModel(k, true_λs, true_πs);

print(&quot;True model: k=$k components\n&quot;)
for i in 1:k
    print(&quot;Component $i: λ=$(true_λs[i]), π=$(true_πs[i])\n&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">True model: k=3 components
Component 1: λ=5.0, π=0.25
Component 2: λ=10.0, π=0.45
Component 3: λ=25.0, π=0.3</code></pre><h2 id="Generate-Synthetic-Data"><a class="docs-heading-anchor" href="#Generate-Synthetic-Data">Generate Synthetic Data</a><a id="Generate-Synthetic-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Generate-Synthetic-Data" title="Permalink"></a></h2><p>Draw <span>$n$</span> independent samples. The <code>labels</code> indicate true component membership for each observation (unknown in practice and must be inferred).</p><pre><code class="language-julia hljs">n = 500
labels = rand(rng, Categorical(true_πs), n)
data = [rand(rng, Poisson(true_λs[labels[i]])) for i in 1:n];

print(&quot;Generated $n samples with count range [$(minimum(data)), $(maximum(data))]\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generated 500 samples with count range [1, 38]</code></pre><p>Visualize samples by true component membership Components with larger <span>$\lambda$</span> shift mass toward higher counts</p><pre><code class="language-julia hljs">p1 = histogram(data;
    group=labels,
    bins=0:1:maximum(data),
    bar_position=:dodge,
    xlabel=&quot;Count&quot;, ylabel=&quot;Frequency&quot;,
    title=&quot;Poisson Mixture Samples by True Component&quot;,
    alpha=0.7, legend=:topright
)</code></pre><img src="38be527a.svg" alt="Example block output"/><h2 id="Fit-Poisson-Mixture-Model-with-EM"><a class="docs-heading-anchor" href="#Fit-Poisson-Mixture-Model-with-EM">Fit Poisson Mixture Model with EM</a><a id="Fit-Poisson-Mixture-Model-with-EM-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-Poisson-Mixture-Model-with-EM" title="Permalink"></a></h2><p>Construct model with <span>$k$</span> components and fit using EM algorithm. Key options:</p><ul><li><code>maxiter</code>: Maximum EM iterations</li><li><code>tol</code>: Convergence tolerance (relative log-likelihood improvement)</li><li><code>initialize_kmeans=true</code>: Use k-means for stable initialization</li></ul><pre><code class="language-julia hljs">fit_pmm = PoissonMixtureModel(k)
_, lls = fit!(fit_pmm, data; maxiter=100, tol=1e-6, initialize_kmeans=true);

print(&quot;EM converged in $(length(lls)) iterations\n&quot;)
print(&quot;Log-likelihood improved by $(round(lls[end] - lls[1], digits=1))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Iteration 1: Log-likelihood = -1701.4170574741024
Iteration 2: Log-likelihood = -1700.4496058876384
Iteration 3: Log-likelihood = -1699.49680411903
Iteration 4: Log-likelihood = -1698.4566035471096
Iteration 5: Log-likelihood = -1697.276408451627
Iteration 6: Log-likelihood = -1695.9227062072143
Iteration 7: Log-likelihood = -1694.3831623449275
Iteration 8: Log-likelihood = -1692.6737183621606
Iteration 9: Log-likelihood = -1690.8429939428036
Iteration 10: Log-likelihood = -1688.966579692283
Iteration 11: Log-likelihood = -1687.128757483291
Iteration 12: Log-likelihood = -1685.3993693662787
Iteration 13: Log-likelihood = -1683.8191619716679
Iteration 14: Log-likelihood = -1682.4001505264225
Iteration 15: Log-likelihood = -1681.1358759227746
Iteration 16: Log-likelihood = -1680.01238260039
Iteration 17: Log-likelihood = -1679.0149328217365
Iteration 18: Log-likelihood = -1678.130488019635
Iteration 19: Log-likelihood = -1677.347891793613
Iteration 20: Log-likelihood = -1676.6573456835404
Iteration 21: Log-likelihood = -1676.049922983467
Iteration 22: Log-likelihood = -1675.5173022297793
Iteration 23: Log-likelihood = -1675.0516754016712
Iteration 24: Log-likelihood = -1674.6457453573184
Iteration 25: Log-likelihood = -1674.292750688176
Iteration 26: Log-likelihood = -1673.9864866127032
Iteration 27: Log-likelihood = -1673.7213109540196
Iteration 28: Log-likelihood = -1673.4921344806392
Iteration 29: Log-likelihood = -1673.2943987436545
Iteration 30: Log-likelihood = -1673.1240452844297
Iteration 31: Log-likelihood = -1672.977479627546
Iteration 32: Log-likelihood = -1672.8515327137145
Iteration 33: Log-likelihood = -1672.7434217108193
Iteration 34: Log-likelihood = -1672.6507115607624
Iteration 35: Log-likelihood = -1672.571278176374
Iteration 36: Log-likelihood = -1672.5032738710302
Iteration 37: Log-likelihood = -1672.445095358223
Iteration 38: Log-likelihood = -1672.3953544790697
Iteration 39: Log-likelihood = -1672.3528516856009
Iteration 40: Log-likelihood = -1672.3165522169095
Iteration 41: Log-likelihood = -1672.285564843269
Iteration 42: Log-likelihood = -1672.2591230142611
Iteration 43: Log-likelihood = -1672.2365682247155
Iteration 44: Log-likelihood = -1672.2173354027111
Iteration 45: Log-likelihood = -1672.2009401233727
Iteration 46: Log-likelihood = -1672.1869674578818
Iteration 47: Log-likelihood = -1672.175062277122
Iteration 48: Log-likelihood = -1672.1649208418928
Iteration 49: Log-likelihood = -1672.1562835253253
Iteration 50: Log-likelihood = -1672.1489285276039
Iteration 51: Log-likelihood = -1672.142666457042
Iteration 52: Log-likelihood = -1672.137335665309
Iteration 53: Log-likelihood = -1672.1327982371733
Iteration 54: Log-likelihood = -1672.1289365471662
Iteration 55: Log-likelihood = -1672.125650306002
Iteration 56: Log-likelihood = -1672.1228540295176
Iteration 57: Log-likelihood = -1672.120474871485
Iteration 58: Log-likelihood = -1672.1184507693192
Iteration 59: Log-likelihood = -1672.1167288585675
Iteration 60: Log-likelihood = -1672.1152641179724
Iteration 61: Log-likelihood = -1672.1140182121708
Iteration 62: Log-likelihood = -1672.1129585037656
Iteration 63: Log-likelihood = -1672.1120572101363
Iteration 64: Log-likelihood = -1672.1112906842113
Iteration 65: Log-likelihood = -1672.1106388011458
Iteration 66: Log-likelihood = -1672.110084435389
Iteration 67: Log-likelihood = -1672.1096130150436
Iteration 68: Log-likelihood = -1672.109212142096
Iteration 69: Log-likelihood = -1672.1088712689166
Iteration 70: Log-likelihood = -1672.1085814227408
Iteration 71: Log-likelihood = -1672.108334970937
Iteration 72: Log-likelihood = -1672.1081254213007
Iteration 73: Log-likelihood = -1672.1079472518984
Iteration 74: Log-likelihood = -1672.1077957662899
Iteration 75: Log-likelihood = -1672.1076669703252
Iteration 76: Log-likelihood = -1672.1075574672059
Iteration 77: Log-likelihood = -1672.1074643683105
Iteration 78: Log-likelihood = -1672.1073852172226
Iteration 79: Log-likelihood = -1672.1073179251232
Iteration 80: Log-likelihood = -1672.107260715853
Iteration 81: Log-likelihood = -1672.1072120791105
Iteration 82: Log-likelihood = -1672.1071707307347
Iteration 83: Log-likelihood = -1672.1071355788476
Iteration 84: Log-likelihood = -1672.10710569507
Iteration 85: Log-likelihood = -1672.1070802900729
Iteration 86: Log-likelihood = -1672.1070586927437
Iteration 87: Log-likelihood = -1672.1070403325155
Iteration 88: Log-likelihood = -1672.1070247242799
Iteration 89: Log-likelihood = -1672.1070114556112
Iteration 90: Log-likelihood = -1672.1070001758885
Iteration 91: Log-likelihood = -1672.1069905870036
Iteration 92: Log-likelihood = -1672.1069824355384
Iteration 93: Log-likelihood = -1672.1069755060353
Iteration 94: Log-likelihood = -1672.1069696153395
Iteration 95: Log-likelihood = -1672.1069646077372
Iteration 96: Log-likelihood = -1672.1069603508595
Iteration 97: Log-likelihood = -1672.106956732158
Iteration 98: Log-likelihood = -1672.106953655976
Iteration 99: Log-likelihood = -1672.1069510409816
Iteration 100: Log-likelihood = -1672.1069488180317
EM converged in 100 iterations
Log-likelihood improved by 29.3</code></pre><p>Display learned parameters</p><pre><code class="language-julia hljs">print(&quot;Learned parameters:\n&quot;)
for i in 1:k
    print(&quot;Component $i: λ=$(round(fit_pmm.λₖ[i], digits=2)), π=$(round(fit_pmm.πₖ[i], digits=3))\n&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Learned parameters:
Component 1: λ=5.22, π=0.319
Component 2: λ=24.88, π=0.295
Component 3: λ=10.58, π=0.386</code></pre><h2 id="Monitor-EM-Convergence"><a class="docs-heading-anchor" href="#Monitor-EM-Convergence">Monitor EM Convergence</a><a id="Monitor-EM-Convergence-1"></a><a class="docs-heading-anchor-permalink" href="#Monitor-EM-Convergence" title="Permalink"></a></h2><p>EM guarantees non-decreasing log-likelihood. Monotonic ascent indicates proper convergence.</p><pre><code class="language-julia hljs">p2 = plot(lls;
    xlabel=&quot;Iteration&quot;, ylabel=&quot;Log-Likelihood&quot;,
    title=&quot;EM Convergence&quot;,
    marker=:circle, markersize=3, lw=2,
    legend=false, color=:darkblue
)

annotate!(p2, length(lls)*0.7, lls[end]*0.98,
    text(&quot;Final LL: $(round(lls[end], digits=1))&quot;, 10))</code></pre><img src="19916d07.svg" alt="Example block output"/><h2 id="Visual-Model-Assessment"><a class="docs-heading-anchor" href="#Visual-Model-Assessment">Visual Model Assessment</a><a id="Visual-Model-Assessment-1"></a><a class="docs-heading-anchor-permalink" href="#Visual-Model-Assessment" title="Permalink"></a></h2><p>Overlay fitted component PMFs and overall mixture PMF on normalized histogram. Components should explain major modes and tail behavior in the data.</p><pre><code class="language-julia hljs">p3 = histogram(data;
    bins=0:1:maximum(data), normalize=true, alpha=0.3,
    xlabel=&quot;Count&quot;, ylabel=&quot;Probability Density&quot;,
    title=&quot;Data vs. Fitted Mixture Components&quot;,
    label=&quot;Data&quot;, color=:gray
)

x_range = collect(0:maximum(data))
colors = [:red, :green, :blue]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Symbol}:
 :red
 :green
 :blue</code></pre><p>Plot individual component PMFs</p><pre><code class="language-julia hljs">for i in 1:k
    λᵢ = fit_pmm.λₖ[i]
    πᵢ = fit_pmm.πₖ[i]
    pmf_i = πᵢ .* pdf.(Poisson(λᵢ), x_range)
    plot!(p3, x_range, pmf_i;
        lw=2, color=colors[i],
        label=&quot;Component $i (λ=$(round(λᵢ, digits=1)))&quot;
    )
end</code></pre><p>Plot overall mixture PMF</p><pre><code class="language-julia hljs">mixture_pmf = reduce(+, (πᵢ .* pdf.(Poisson(λᵢ), x_range)
                        for (λᵢ, πᵢ) in zip(fit_pmm.λₖ, fit_pmm.πₖ)))
plot!(p3, x_range, mixture_pmf;
    lw=3, linestyle=:dash, color=:black,
    label=&quot;Mixture PMF&quot;
)</code></pre><img src="70eaff0c.svg" alt="Example block output"/><h2 id="Posterior-Responsibilities-(Soft-Clustering)"><a class="docs-heading-anchor" href="#Posterior-Responsibilities-(Soft-Clustering)">Posterior Responsibilities (Soft Clustering)</a><a id="Posterior-Responsibilities-(Soft-Clustering)-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Responsibilities-(Soft-Clustering)" title="Permalink"></a></h2><p>Responsibilities <span>$\gamma_{ij} = P(z_i = j | x_i, \hat{\boldsymbol{\theta}})$</span> quantify how likely each observation belongs to each component. These provide soft assignments and uncertainty quantification.</p><pre><code class="language-julia hljs">function responsibilities_pmm(λs::AbstractVector, πs::AbstractVector, x::AbstractVector)
    k, n = length(λs), length(x)
    Γ = zeros(n, k)

    for i in 1:n
        for j in 1:k
            Γ[i, j] = πs[j] * pdf(Poisson(λs[j]), x[i])
        end

        row_sum = sum(Γ[i, :]) # Normalize to get probabilities
        if row_sum &gt; 0
            Γ[i, :] ./= row_sum
        end
    end
    return Γ
end

Γ = responsibilities_pmm(fit_pmm.λₖ, fit_pmm.πₖ, data);</code></pre><p>Hard assignments (if needed) are argmax over responsibilities</p><pre><code class="language-julia hljs">hard_labels = [argmax(Γ[i, :]) for i in 1:n];</code></pre><p>Calculate assignment accuracy compared to true labels</p><pre><code class="language-julia hljs">accuracy = mean(labels .== hard_labels)
print(&quot;Component assignment accuracy: $(round(accuracy*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Component assignment accuracy: 26.4%</code></pre><h2 id="Information-Criteria-for-Model-Selection"><a class="docs-heading-anchor" href="#Information-Criteria-for-Model-Selection">Information Criteria for Model Selection</a><a id="Information-Criteria-for-Model-Selection-1"></a><a class="docs-heading-anchor-permalink" href="#Information-Criteria-for-Model-Selection" title="Permalink"></a></h2><p>When <span>$k$</span> is unknown, compare models using AIC/BIC:</p><ul><li>AIC = <span>$2p - 2\text{LL}$</span></li><li>BIC = <span>$p \log(n) - 2\text{LL}$</span></li></ul><p>where parameter count <span>$p = (k-1) + k = 2k-1$</span> (mixing weights + rates)</p><pre><code class="language-julia hljs">function compute_ic(lls::AbstractVector, n::Int, k::Int)
    ll = last(lls)
    p = 2k - 1
    return (AIC = 2p - 2ll, BIC = p*log(n) - 2ll)
end

ic = compute_ic(lls, n, k)
print(&quot;Information criteria: AIC = $(round(ic.AIC, digits=1)), BIC = $(round(ic.BIC, digits=1))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Information criteria: AIC = 3354.2, BIC = 3375.3</code></pre><h2 id="Parameter-Recovery-Assessment"><a class="docs-heading-anchor" href="#Parameter-Recovery-Assessment">Parameter Recovery Assessment</a><a id="Parameter-Recovery-Assessment-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Recovery-Assessment" title="Permalink"></a></h2><pre><code class="language-julia hljs">print(&quot;\n=== Parameter Recovery Assessment ===\n&quot;)

λ_errors = [abs(true_λs[i] - fit_pmm.λₖ[i]) / true_λs[i] for i in 1:k]
π_errors = [abs(true_πs[i] - fit_pmm.πₖ[i]) for i in 1:k]

print(&quot;Rate recovery errors (%):\n&quot;)
for i in 1:k
    print(&quot;Component $i: $(round(λ_errors[i]*100, digits=1))%\n&quot;)
end

print(&quot;Mixing weight recovery errors:\n&quot;)
for i in 1:k
    print(&quot;Component $i: $(round(π_errors[i], digits=3))\n&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">
=== Parameter Recovery Assessment ===
Rate recovery errors (%):
Component 1: 4.4%
Component 2: 148.8%
Component 3: 57.7%
Mixing weight recovery errors:
Component 1: 0.069
Component 2: 0.155
Component 3: 0.086</code></pre><h2 id="Model-Selection-Example"><a class="docs-heading-anchor" href="#Model-Selection-Example">Model Selection Example</a><a id="Model-Selection-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Selection-Example" title="Permalink"></a></h2><p>Demonstrate fitting multiple values of k and comparing via BIC</p><pre><code class="language-julia hljs">print(&quot;\n=== Model Selection Demo ===\n&quot;)

k_range = 1:5
bic_scores = Float64[]
aic_scores = Float64[]

for k_test in k_range
    pmm_test = PoissonMixtureModel(k_test)
    _, lls_test = fit!(pmm_test, data; maxiter=50, tol=1e-6, initialize_kmeans=true)

    ic_test = compute_ic(lls_test, n, k_test)
    push!(aic_scores, ic_test.AIC)
    push!(bic_scores, ic_test.BIC)

    print(&quot;k=$k_test: AIC=$(round(ic_test.AIC, digits=1)), BIC=$(round(ic_test.BIC, digits=1))\n&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">
=== Model Selection Demo ===
Iteration 1: Log-likelihood = -2446.0120010620803
Iteration 2: Log-likelihood = -2446.0120010620803
Converged at iteration 2
k=1: AIC=4894.0, BIC=4898.2
Iteration 1: Log-likelihood = -1707.2052935545328
Iteration 2: Log-likelihood = -1707.0047864028513
Iteration 3: Log-likelihood = -1706.9642372392507
Iteration 4: Log-likelihood = -1706.9541047278067
Iteration 5: Log-likelihood = -1706.9515504467433
Iteration 6: Log-likelihood = -1706.9509037585117
Iteration 7: Log-likelihood = -1706.9507396761167
Iteration 8: Log-likelihood = -1706.950697998575
Iteration 9: Log-likelihood = -1706.9506874065096
Iteration 10: Log-likelihood = -1706.9506847138678
Iteration 11: Log-likelihood = -1706.9506840292643
Converged at iteration 11
k=2: AIC=3419.9, BIC=3432.5
Iteration 1: Log-likelihood = -1710.3027675982219
Iteration 2: Log-likelihood = -1703.9739685180687
Iteration 3: Log-likelihood = -1702.730146251201
Iteration 4: Log-likelihood = -1702.2092752041488
Iteration 5: Log-likelihood = -1701.7429440849837
Iteration 6: Log-likelihood = -1701.238749310594
Iteration 7: Log-likelihood = -1700.6653374228827
Iteration 8: Log-likelihood = -1699.9970343139016
Iteration 9: Log-likelihood = -1699.2075701828371
Iteration 10: Log-likelihood = -1698.2698383665877
Iteration 11: Log-likelihood = -1697.158270957976
Iteration 12: Log-likelihood = -1695.8541574410249
Iteration 13: Log-likelihood = -1694.3539221638723
Iteration 14: Log-likelihood = -1692.6783787190277
Iteration 15: Log-likelihood = -1690.877946092217
Iteration 16: Log-likelihood = -1689.027573507297
Iteration 17: Log-likelihood = -1687.209667589512
Iteration 18: Log-likelihood = -1685.4926053602662
Iteration 19: Log-likelihood = -1683.9171865521262
Iteration 20: Log-likelihood = -1682.4969165609336
Iteration 21: Log-likelihood = -1681.2274109873092
Iteration 22: Log-likelihood = -1680.0965813536088
Iteration 23: Log-likelihood = -1679.0910084512097
Iteration 24: Log-likelihood = -1678.1984403986019
Iteration 25: Log-likelihood = -1677.4081294234647
Iteration 26: Log-likelihood = -1676.7104593127453
Iteration 27: Log-likelihood = -1676.0965641565876
Iteration 28: Log-likelihood = -1675.5581227574512
Iteration 29: Log-likelihood = -1675.0872987153837
Iteration 30: Log-likelihood = -1674.6767536208192
Iteration 31: Log-likelihood = -1674.3196795820368
Iteration 32: Log-likelihood = -1674.0098239670326
Iteration 33: Log-likelihood = -1673.7414974401552
Iteration 34: Log-likelihood = -1673.5095654849388
Iteration 35: Log-likelihood = -1673.3094269465753
Iteration 36: Log-likelihood = -1673.1369836501676
Iteration 37: Log-likelihood = -1672.9886046023423
Iteration 38: Log-likelihood = -1672.8610874871963
Iteration 39: Log-likelihood = -1672.7516194343207
Iteration 40: Log-likelihood = -1672.6577384470716
Iteration 41: Log-likelihood = -1672.5772964295286
Iteration 42: Log-likelihood = -1672.5084244136372
Iteration 43: Log-likelihood = -1672.449500339146
Iteration 44: Log-likelihood = -1672.3991195550846
Iteration 45: Log-likelihood = -1672.3560680794124
Iteration 46: Log-likelihood = -1672.3192985595763
Iteration 47: Log-likelihood = -1672.2879088130744
Iteration 48: Log-likelihood = -1672.2611227864852
Iteration 49: Log-likelihood = -1672.2382737480177
Iteration 50: Log-likelihood = -1672.2187895182508
k=3: AIC=3354.4, BIC=3375.5
Iteration 1: Log-likelihood = -1689.2991357814126
Iteration 2: Log-likelihood = -1682.9720556885482
Iteration 3: Log-likelihood = -1679.2504718894197
Iteration 4: Log-likelihood = -1677.0803959773157
Iteration 5: Log-likelihood = -1675.7730167607053
Iteration 6: Log-likelihood = -1674.9177339737873
Iteration 7: Log-likelihood = -1674.3195610989221
Iteration 8: Log-likelihood = -1673.8827348969448
Iteration 9: Log-likelihood = -1673.5537470104498
Iteration 10: Log-likelihood = -1673.299715713307
Iteration 11: Log-likelihood = -1673.0993765520436
Iteration 12: Log-likelihood = -1672.9385489924798
Iteration 13: Log-likelihood = -1672.8075375858223
Iteration 14: Log-likelihood = -1672.6995494355824
Iteration 15: Log-likelihood = -1672.6097051865777
Iteration 16: Log-likelihood = -1672.5344117477991
Iteration 17: Log-likelihood = -1672.4709591995252
Iteration 18: Log-likelihood = -1672.4172579592603
Iteration 19: Log-likelihood = -1672.371664510723
Iteration 20: Log-likelihood = -1672.3328636746687
Iteration 21: Log-likelihood = -1672.2997874571959
Iteration 22: Log-likelihood = -1672.2715579193639
Iteration 23: Log-likelihood = -1672.2474460845074
Iteration 24: Log-likelihood = -1672.2268417429502
Iteration 25: Log-likelihood = -1672.2092307977516
Iteration 26: Log-likelihood = -1672.1941779278434
Iteration 27: Log-likelihood = -1672.1813130722155
Iteration 28: Log-likelihood = -1672.170320712682
Iteration 29: Log-likelihood = -1672.1609312452435
Iteration 30: Log-likelihood = -1672.152913939042
Iteration 31: Log-likelihood = -1672.1460711237316
Iteration 32: Log-likelihood = -1672.1402333435342
Iteration 33: Log-likelihood = -1672.1352552839458
Iteration 34: Log-likelihood = -1672.1310123250216
Iteration 35: Log-likelihood = -1672.1273976092298
Iteration 36: Log-likelihood = -1672.1243195367394
Iteration 37: Log-likelihood = -1672.1216996191047
Iteration 38: Log-likelihood = -1672.1194706359056
Iteration 39: Log-likelihood = -1672.1175750490363
Iteration 40: Log-likelihood = -1672.1159636374175
Iteration 41: Log-likelihood = -1672.114594320726
Iteration 42: Log-likelihood = -1672.1134311460844
Iteration 43: Log-likelihood = -1672.1124434149722
Iteration 44: Log-likelihood = -1672.1116049315867
Iteration 45: Log-likelihood = -1672.110893355778
Iteration 46: Log-likelihood = -1672.1102896466773
Iteration 47: Log-likelihood = -1672.1097775845192
Iteration 48: Log-likelihood = -1672.1093433599767
Iteration 49: Log-likelihood = -1672.1089752219434
Iteration 50: Log-likelihood = -1672.1086631755109
k=4: AIC=3358.2, BIC=3387.7
Iteration 1: Log-likelihood = -1682.012250750103
Iteration 2: Log-likelihood = -1677.066240986196
Iteration 3: Log-likelihood = -1675.5639009166698
Iteration 4: Log-likelihood = -1674.9235760598651
Iteration 5: Log-likelihood = -1674.5822341911748
Iteration 6: Log-likelihood = -1674.3682059616124
Iteration 7: Log-likelihood = -1674.217247742426
Iteration 8: Log-likelihood = -1674.10137976048
Iteration 9: Log-likelihood = -1674.006877178547
Iteration 10: Log-likelihood = -1673.9263628594736
Iteration 11: Log-likelihood = -1673.8555837708282
Iteration 12: Log-likelihood = -1673.7919462859163
Iteration 13: Log-likelihood = -1673.7337901703036
Iteration 14: Log-likelihood = -1673.6800043627275
Iteration 15: Log-likelihood = -1673.6298135405532
Iteration 16: Log-likelihood = -1673.5826550398906
Iteration 17: Log-likelihood = -1673.5381056252013
Iteration 18: Log-likelihood = -1673.495836656497
Iteration 19: Log-likelihood = -1673.455585865754
Iteration 20: Log-likelihood = -1673.4171390813683
Iteration 21: Log-likelihood = -1673.380318048662
Iteration 22: Log-likelihood = -1673.3449720740116
Iteration 23: Log-likelihood = -1673.3109721257083
Iteration 24: Log-likelihood = -1673.2782065534816
Iteration 25: Log-likelihood = -1673.2465779022518
Iteration 26: Log-likelihood = -1673.216000485237
Iteration 27: Log-likelihood = -1673.1863984974555
Iteration 28: Log-likelihood = -1673.1577045232755
Iteration 29: Log-likelihood = -1673.1298583377677
Iteration 30: Log-likelihood = -1673.1028059314003
Iteration 31: Log-likelihood = -1673.0764987077716
Iteration 32: Log-likelihood = -1673.0508928171057
Iteration 33: Log-likelihood = -1673.0259485979304
Iteration 34: Log-likelihood = -1673.0016301056696
Iteration 35: Log-likelihood = -1672.977904711875
Iteration 36: Log-likelihood = -1672.9547427610585
Iteration 37: Log-likelihood = -1672.932117275138
Iteration 38: Log-likelihood = -1672.910003697119
Iteration 39: Log-likelihood = -1672.88837966746
Iteration 40: Log-likelihood = -1672.867224827749
Iteration 41: Log-likelihood = -1672.846520647153
Iteration 42: Log-likelihood = -1672.8262502681496
Iteration 43: Log-likelihood = -1672.8063983683908
Iteration 44: Log-likelihood = -1672.7869510362705
Iteration 45: Log-likelihood = -1672.7678956580921
Iteration 46: Log-likelihood = -1672.7492208150773
Iteration 47: Log-likelihood = -1672.7309161888047
Iteration 48: Log-likelihood = -1672.7129724738445
Iteration 49: Log-likelihood = -1672.6953812965764
Iteration 50: Log-likelihood = -1672.6781351393295
k=5: AIC=3363.4, BIC=3401.3</code></pre><p>Plot information criteria vs number of components</p><pre><code class="language-julia hljs">p4 = plot(k_range, [aic_scores bic_scores];
    xlabel=&quot;Number of Components (k)&quot;, ylabel=&quot;Information Criterion&quot;,
    title=&quot;Model Selection via Information Criteria&quot;,
    label=[&quot;AIC&quot; &quot;BIC&quot;], marker=:circle, lw=2
)

optimal_k_aic = k_range[argmin(aic_scores)]
optimal_k_bic = k_range[argmin(bic_scores)]

print(&quot;Optimal k: AIC suggests k=$optimal_k_aic, BIC suggests k=$optimal_k_bic\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Optimal k: AIC suggests k=3, BIC suggests k=3</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This tutorial demonstrated the complete Poisson Mixture Model workflow:</p><p><strong>Key Concepts:</strong></p><ul><li><strong>Discrete mixtures</strong>: Model count data as mixture of Poisson distributions</li><li><strong>EM algorithm</strong>: Iterative optimization with closed-form M-step updates</li><li><strong>Soft clustering</strong>: Posterior responsibilities provide probabilistic assignments</li><li><strong>Model selection</strong>: Information criteria help choose appropriate number of components</li></ul><p><strong>Applications:</strong></p><ul><li>Spike count analysis in neuroscience</li><li>Customer transaction modeling in business analytics</li><li>Event frequency analysis in reliability engineering</li><li>Gene expression count clustering in bioinformatics</li></ul><p><strong>Technical Insights:</strong></p><ul><li>Initialization strategy significantly affects final solution quality</li><li>Label switching is a fundamental identifiability issue in mixture models</li><li>Information criteria provide principled approach to model complexity selection</li><li>Component separation quality affects parameter recovery accuracy</li></ul><p><strong>Extensions:</strong></p><ul><li>Zero-inflated Poisson mixtures for excess zero counts</li><li>Negative Binomial mixtures for overdispersed count data</li><li>Bayesian approaches for uncertainty quantification</li><li>Mixture regression models for count data with covariates</li></ul><p>Poisson mixture models provide a flexible framework for modeling heterogeneous count data, enabling both clustering and density estimation while maintaining interpretable probabilistic structure.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gaussian_mixture_model_example/">« Gaussian Mixture Model Example</a><a class="docs-footer-nextpage" href="../Probabilistic_PCA_example/">Probabilistic PCA Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 21 October 2025 17:53">Tuesday 21 October 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
