<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Hidden Markov Model Example · StateSpaceDynamics.jl</title><meta name="title" content="Hidden Markov Model Example · StateSpaceDynamics.jl"/><meta property="og:title" content="Hidden Markov Model Example · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Hidden Markov Model Example · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../EmissionModels/">EmissionModels</a></li><li><a class="tocitem" href="../../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../lds_model_selection_example/">LDS Model Selection Example</a></li><li><a class="tocitem" href="../lds_identifiability_example/">Non-Identifiability in LDS Models</a></li><li class="is-active"><a class="tocitem" href>Hidden Markov Model Example</a><ul class="internal"><li><a class="tocitem" href="#Load-Required-Packages"><span>Load Required Packages</span></a></li><li><a class="tocitem" href="#Create-a-Gaussian-Emission-HMM"><span>Create a Gaussian Emission HMM</span></a></li><li><a class="tocitem" href="#Sample-from-the-HMM"><span>Sample from the HMM</span></a></li><li><a class="tocitem" href="#Visualize-the-Sampled-Dataset"><span>Visualize the Sampled Dataset</span></a></li><li><a class="tocitem" href="#Initialize-and-Fit-HMM-with-EM"><span>Initialize and Fit HMM with EM</span></a></li><li><a class="tocitem" href="#Hidden-State-Decoding-with-Viterbi"><span>Hidden State Decoding with Viterbi</span></a></li><li><a class="tocitem" href="#Multiple-Independent-Trials"><span>Multiple Independent Trials</span></a></li><li><a class="tocitem" href="#Multi-Trial-HMM-Fitting"><span>Multi-Trial HMM Fitting</span></a></li><li><a class="tocitem" href="#Multi-Trial-State-Decoding"><span>Multi-Trial State Decoding</span></a></li><li><a class="tocitem" href="#Parameter-Recovery-Assessment"><span>Parameter Recovery Assessment</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../hmm_model_selection_example/">HMM Model Selection</a></li><li><a class="tocitem" href="../gaussian_glm_hmm_example/">Gaussian GLM-HMM Example</a></li><li><a class="tocitem" href="../hmm_identifiability_example/">HMM Identifiability</a></li><li><a class="tocitem" href="../gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li><a class="tocitem" href="../poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li><a class="tocitem" href="../Probabilistic_PCA_example/">Probabilistic PCA Example</a></li><li><a class="tocitem" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li><li><a class="tocitem" href="../../Misc/">Miscellaneous</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Hidden Markov Model Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Hidden Markov Model Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/examples/HMM.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Simulating-and-Fitting-a-Hidden-Markov-Model-with-Gaussian-Emissions"><a class="docs-heading-anchor" href="#Simulating-and-Fitting-a-Hidden-Markov-Model-with-Gaussian-Emissions">Simulating and Fitting a Hidden Markov Model with Gaussian Emissions</a><a id="Simulating-and-Fitting-a-Hidden-Markov-Model-with-Gaussian-Emissions-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-and-Fitting-a-Hidden-Markov-Model-with-Gaussian-Emissions" title="Permalink"></a></h1><p>This tutorial demonstrates how to use <code>StateSpaceDynamics.jl</code> to create, sample from, and fit Hidden Markov Models (HMMs) with Gaussian emission distributions. This is the classical HMM formulation where each hidden state generates observations from a different multivariate Gaussian distribution.</p><p>Unlike GLM-HMMs, this model doesn&#39;t use input features - each state simply emits observations from its own characteristic Gaussian distribution. This makes it ideal for clustering time series data, identifying behavioral regimes, or modeling switching dynamics where each state has a distinct statistical signature.</p><h2 id="Load-Required-Packages"><a class="docs-heading-anchor" href="#Load-Required-Packages">Load Required Packages</a><a id="Load-Required-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Required-Packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using LinearAlgebra
using Plots
using Random
using StateSpaceDynamics
using StableRNGs
using Statistics: mean, std
using LaTeXStrings</code></pre><p>Set up reproducible random number generation</p><pre><code class="language-julia hljs">rng = StableRNG(1234);</code></pre><h2 id="Create-a-Gaussian-Emission-HMM"><a class="docs-heading-anchor" href="#Create-a-Gaussian-Emission-HMM">Create a Gaussian Emission HMM</a><a id="Create-a-Gaussian-Emission-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Create-a-Gaussian-Emission-HMM" title="Permalink"></a></h2><p>We&#39;ll create an HMM with two hidden states, each emitting 2D Gaussian observations. This creates a simple but illustrative model where hidden states correspond to different regions in the observation space.</p><pre><code class="language-julia hljs">output_dim = 2;  # Each observation is a 2D vector</code></pre><p>Define state transition dynamics: <span>$A_{ij} = P(\text{state}_t = j \mid \text{state}_{t-1} = i)$</span> High diagonal values mean states are &quot;sticky&quot; (tend to persist)</p><pre><code class="language-julia hljs">A = [0.99 0.01;    # From state 1: 99% stay, 1% switch to state 2
     0.05 0.95];   # From state 2: 5% switch to state 1, 95% stay</code></pre><p>Initial state probabilities: <span>$\pi_k = P(\text{state}_1 = k)$</span></p><pre><code class="language-julia hljs">πₖ = [0.5; 0.5];</code></pre><p>Define emission distributions for each hidden state State 1: Centered at (-1, -1) with small variance (tight cluster)</p><pre><code class="language-julia hljs">μ_1 = [-1.0, -1.0]
Σ_1 = 0.1 * Matrix{Float64}(I, output_dim, output_dim)
emission_1 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1);</code></pre><p>State 2: Centered at (1, 1) with larger variance (more spread out)</p><pre><code class="language-julia hljs">μ_2 = [1.0, 1.0]
Σ_2 = 0.2 * Matrix{Float64}(I, output_dim, output_dim)
emission_2 = GaussianEmission(output_dim=output_dim, μ=μ_2, Σ=Σ_2);</code></pre><p>Construct the complete HMM</p><pre><code class="language-julia hljs">model = HiddenMarkovModel(
    K=2,                        # Number of hidden states
    B=[emission_1, emission_2], # Emission distributions
    A=A,                        # State transition matrix
    πₖ=πₖ                      # Initial state distribution
);

print(&quot;Created Gaussian HMM with 2 states:\n&quot;)
print(&quot;State 1: μ = $μ_1, σ² = $(Σ_1[1,1]) (tight cluster)\n&quot;)
print(&quot;State 2: μ = $μ_2, σ² = $(Σ_2[1,1]) (looser cluster)\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Created Gaussian HMM with 2 states:
State 1: μ = [-1.0, -1.0], σ² = 0.1 (tight cluster)
State 2: μ = [1.0, 1.0], σ² = 0.2 (looser cluster)</code></pre><h2 id="Sample-from-the-HMM"><a class="docs-heading-anchor" href="#Sample-from-the-HMM">Sample from the HMM</a><a id="Sample-from-the-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-from-the-HMM" title="Permalink"></a></h2><p>Generate synthetic data from our true model. Each state generates observations from its own Gaussian distribution without requiring input features. The rand function samples both the hidden state sequence and the corresponding observations.</p><pre><code class="language-julia hljs">num_samples = 10000;
true_labels, data = rand(rng, model, n=num_samples);</code></pre><h2 id="Visualize-the-Sampled-Dataset"><a class="docs-heading-anchor" href="#Visualize-the-Sampled-Dataset">Visualize the Sampled Dataset</a><a id="Visualize-the-Sampled-Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-the-Sampled-Dataset" title="Permalink"></a></h2><p>Create a 2D scatter plot showing observations colored by their true hidden state. This illustrates how each state generates observations from a distinct region of space. We will also plot a trajectory line to show the temporal evolution for the first 1000 timepoints.</p><pre><code class="language-julia hljs">x_vals = data[1, 1:num_samples]
y_vals = data[2, 1:num_samples]
labels_slice = true_labels[1:num_samples]

state_colors = [:dodgerblue, :crimson]

p1 = plot()

for state in 1:2
    idx = findall(labels_slice .== state)
    scatter!(x_vals[idx], y_vals[idx];
        color=state_colors[state],
        label=&quot;State $state&quot;,
        markersize=3,
        alpha=0.6)
end

plot!(x_vals[1:1000], y_vals[1:1000];
    color=:gray, lw=1, alpha=0.3, label=&quot;Trajectory&quot;)

scatter!([x_vals[1]], [y_vals[1]]; marker=:star5, markersize=8,
         color=:green, label=&quot;Start&quot;)
scatter!([x_vals[end]], [y_vals[end]]; marker=:diamond, markersize=6,
         color=:black, label=&quot;End&quot;)

plot!(xlabel=L&quot;x_1&quot;, ylabel=L&quot;x_2&quot;,
      title=&quot;HMM Emissions by True Hidden State&quot;,
      legend=:topleft)</code></pre><img src="2234269c.svg" alt="Example block output"/><h2 id="Initialize-and-Fit-HMM-with-EM"><a class="docs-heading-anchor" href="#Initialize-and-Fit-HMM-with-EM">Initialize and Fit HMM with EM</a><a id="Initialize-and-Fit-HMM-with-EM-1"></a><a class="docs-heading-anchor-permalink" href="#Initialize-and-Fit-HMM-with-EM" title="Permalink"></a></h2><p>In reality, we only observe the data, not the hidden states. The goal of fitting is to learn the latent state sequence and the model parameters that best explain the data. We will initialize a new HMM with incorrect parameters and use the Expectation-Maximization (EM) algorithm to iteratively refine the parameters and infer the hidden states.</p><pre><code class="language-julia hljs">μ_1_init = [-0.25, -0.25]  # Closer to center than true
Σ_1_init = 0.3 * Matrix{Float64}(I, output_dim, output_dim)  # Larger variance
emission_1_init = GaussianEmission(output_dim=output_dim, μ=μ_1_init, Σ=Σ_1_init);

μ_2_init = [0.25, 0.25]    # Closer to center than true
Σ_2_init = 0.5 * Matrix{Float64}(I, output_dim, output_dim)  # Much larger variance
emission_2_init = GaussianEmission(output_dim=output_dim, μ=μ_2_init, Σ=Σ_2_init);

A_init = [0.8 0.2; 0.05 0.95]  # Less persistent than true model
πₖ_init = [0.6, 0.4];           # Biased toward state 1

test_model = HiddenMarkovModel(K=2, B=[emission_1_init, emission_2_init],
                              A=A_init, πₖ=πₖ_init);</code></pre><p>Fit using Expectation-Maximization</p><pre><code class="language-julia hljs">lls = fit!(test_model, data);

print(&quot;EM converged in $(length(lls)) iterations\n&quot;)
print(&quot;Log-likelihood improved by $(round(lls[end] - lls[1], digits=1))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">EM converged in 5 iterations
Log-likelihood improved by 23097.5</code></pre><p>Plot EM convergence</p><pre><code class="language-julia hljs">p2 = plot(lls, xlabel=&quot;EM Iteration&quot;, ylabel=&quot;Log-Likelihood&quot;,
          title=&quot;EM Algorithm Convergence&quot;, legend=false,
          marker=:circle, markersize=3, lw=2, color=:darkblue)

p2</code></pre><img src="e3cf5cad.svg" alt="Example block output"/><h2 id="Hidden-State-Decoding-with-Viterbi"><a class="docs-heading-anchor" href="#Hidden-State-Decoding-with-Viterbi">Hidden State Decoding with Viterbi</a><a id="Hidden-State-Decoding-with-Viterbi-1"></a><a class="docs-heading-anchor-permalink" href="#Hidden-State-Decoding-with-Viterbi" title="Permalink"></a></h2><p>Now that we have learned the model parameters from the observed data, we can decode the most likely sequence of hidden states using the Viterbi algorithm. Then, in this toy example where we know the true latent state path, we can assess the accuracy of our state predictions.</p><pre><code class="language-julia hljs">pred_labels = viterbi(test_model, data);

accuracy = mean(true_labels .== pred_labels)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.0</code></pre><p>Calling a specific set of parameters &quot;state 1&quot; and &quot;state 2&quot; is arbitrary and does not affect the correctness of the model. The EM algorithm can converge with the states swapped from our original convention. We check for this and correct it if necessary.</p><pre><code class="language-julia hljs">swapped_pred = 3 .- pred_labels  # Convert 1→2, 2→1
swapped_accuracy = mean(true_labels .== swapped_pred)

if swapped_accuracy &gt; accuracy
    pred_labels = swapped_pred
    accuracy = swapped_accuracy
    print(&quot;Detected and corrected label switching\n&quot;)
end

print(&quot;State prediction accuracy: $(round(accuracy*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">State prediction accuracy: 100.0%</code></pre><p>Our model looks like it is doing pretty well! Let&#39;s visualize the predicted and true state sequences as heatmaps (first 1000 timepoints)</p><pre><code class="language-julia hljs">n_display = 1000
true_seq = reshape(true_labels[1:n_display], 1, :)
pred_seq = reshape(pred_labels[1:n_display], 1, :)

p3 = plot(
    heatmap(true_seq, colormap=:roma, title=&quot;True State Sequence&quot;,
           xticks=false, yticks=false, colorbar=false),
    heatmap(pred_seq, colormap=:roma, title=&quot;Predicted State Sequence (Viterbi)&quot;,
           xlabel=&quot;Time Steps (1-$n_display)&quot;, xticks=0:200:n_display,
           yticks=false, colorbar=false),
    layout=(2, 1), size=(800, 300)
)</code></pre><img src="e22c2266.svg" alt="Example block output"/><h2 id="Multiple-Independent-Trials"><a class="docs-heading-anchor" href="#Multiple-Independent-Trials">Multiple Independent Trials</a><a id="Multiple-Independent-Trials-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Independent-Trials" title="Permalink"></a></h2><p>Many real applications involve multiple independent sequences (e.g., multiple subjects, sessions, or trials). In <code>StateSpaceDynamics.jl</code>, it is easy to incorporate data from multiple trials in parameters learning. Once again, we will generate a synthetic dataset from our ground truth model to illustrate this process.</p><pre><code class="language-julia hljs">n_trials = 100    # Number of independent sequences
n_samples = 1000  # Length of each sequence

all_true_labels = Vector{Vector{Int}}(undef, n_trials);
all_data = Vector{Matrix{Float64}}(undef, n_trials);

for i in 1:n_trials  # Sample each trial independently
    labels_trial, data_trial = rand(rng, model, n=n_samples)
    all_true_labels[i] = labels_trial
    all_data[i] = data_trial
end

total_state1_prop = mean([mean(labels .== 1) for labels in all_true_labels])
print(&quot;Average State 1 proportion: $(round(total_state1_prop, digits=3))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Average State 1 proportion: 0.84</code></pre><h2 id="Multi-Trial-HMM-Fitting"><a class="docs-heading-anchor" href="#Multi-Trial-HMM-Fitting">Multi-Trial HMM Fitting</a><a id="Multi-Trial-HMM-Fitting-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Trial-HMM-Fitting" title="Permalink"></a></h2><p>When fitting to multiple independent sequences, EM accounts for each sequence starting independently from the initial state distribution. Here, we initialize a new model and fit it to all trials simultaneously.</p><pre><code class="language-julia hljs">test_model_multi = HiddenMarkovModel(
    K=2,
    B=[deepcopy(emission_1_init), deepcopy(emission_2_init)],
    A=A_init, πₖ=πₖ_init
)

lls_multi = fit!(test_model_multi, all_data);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running EM algorithm...   2%|█                                                 |  ETA: 0:00:25 ( 0.25  s/it)Running EM algorithm...   4%|██                                                |  ETA: 0:00:16 ( 0.17  s/it)Running EM algorithm...   6%|███                                               |  ETA: 0:00:13 ( 0.14  s/it)Running EM algorithm...   8%|████                                              |  ETA: 0:00:11 ( 0.12  s/it)Running EM algorithm... 100%|██████████████████████████████████████████████████| Time: 0:00:01 (10.63 ms/it)</code></pre><p>Let&#39;s check on how our training went and what parameters we learned.</p><pre><code class="language-julia hljs">print(&quot;Multi-trial EM converged in $(length(lls_multi)) iterations\n&quot;)
print(&quot;Log-likelihood improved by $(round(lls_multi[end] - lls_multi[1], digits=1))\n&quot;);
print(&quot;Multi-trial learned parameters:\n&quot;)
print(&quot;State 1: μ = $(round.(test_model_multi.B[1].μ, digits=3)), σ² = $(round(test_model_multi.B[1].Σ[1,1], digits=3))\n&quot;)
print(&quot;State 2: μ = $(round.(test_model_multi.B[2].μ, digits=3)), σ² = $(round(test_model_multi.B[2].Σ[1,1], digits=3))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Multi-trial EM converged in 9 iterations
Log-likelihood improved by 1371.2
Multi-trial learned parameters:
State 1: μ = [-0.997, -1.0], σ² = 0.1
State 2: μ = [1.006, 1.004], σ² = 0.2</code></pre><p>Visualize multi-trial EM convergence</p><pre><code class="language-julia hljs">p4 = plot(lls_multi, xlabel=&quot;EM Iteration&quot;, ylabel=&quot;Log-Likelihood&quot;,
          title=&quot;Multi-Trial EM Convergence&quot;, legend=false,
          marker=:circle, markersize=3, lw=2, color=:darkgreen)</code></pre><img src="f49fbb48.svg" alt="Example block output"/><h2 id="Multi-Trial-State-Decoding"><a class="docs-heading-anchor" href="#Multi-Trial-State-Decoding">Multi-Trial State Decoding</a><a id="Multi-Trial-State-Decoding-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Trial-State-Decoding" title="Permalink"></a></h2><p>Now that we have done parameter learning, we can use Viterbi to find the most likely hidden state sequence for each trial with a single function call.</p><pre><code class="language-julia hljs">all_pred_labels_vec = viterbi(test_model_multi, all_data);

all_pred_labels = hcat(all_pred_labels_vec...)&#39;;      # trials × time
all_true_labels_matrix = hcat(all_true_labels...)&#39;;   # trials × time</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">100×1000 adjoint(::Matrix{Int64}) with eltype Int64:
 2  2  2  2  1  1  1  1  1  1  1  1  1  …  2  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 2  2  2  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  1  1  1  1  1  1  1  1
 2  2  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 2  2  2  2  1  1  1  1  1  1  1  1  1     1  1  1  1  2  2  2  2  2  2  2  2
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 2  2  2  2  2  2  2  2  2  2  2  2  1     1  1  1  1  1  1  1  1  1  1  1  1
 ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           
 2  2  2  2  2  2  2  2  2  2  2  2  2     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  2  2  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  2  2  2  2  2  2
 1  2  2  2  2  2  2  2  2  2  2  2  2     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 2  2  2  1  1  1  1  1  1  2  2  2  2     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1</code></pre><p>Calculate overall accuracy across all trials accounting for label switching</p><pre><code class="language-julia hljs">overall_accuracy = mean(all_true_labels_matrix .== all_pred_labels);

swapped_pred_all = 3 .- all_pred_labels;
swapped_accuracy_all = mean(all_true_labels_matrix .== swapped_pred_all);

if swapped_accuracy_all &gt; overall_accuracy
    all_pred_labels = swapped_pred_all
    overall_accuracy = swapped_accuracy_all
    print(&quot;Corrected label switching in multi-trial analysis\n&quot;)
end

print(&quot;Overall state prediction accuracy: $(round(overall_accuracy*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Overall state prediction accuracy: 100.0%</code></pre><p>We can also look at per-trial accuracies to see how consistent the model is across trials.</p><pre><code class="language-julia hljs">trial_accuracies = [mean(all_true_labels_matrix[i, :] .== all_pred_labels[i, :]) for i in 1:n_trials]
print(&quot;Per-trial accuracy: $(round(mean(trial_accuracies)*100, digits=1))% ± $(round(std(trial_accuracies)*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Per-trial accuracy: 100.0% ± 0.0%</code></pre><p>Visualize subset of trials (first 10 trials, first 500 timepoints)</p><pre><code class="language-julia hljs">n_trials_display = 10
n_time_display = 500

true_subset = all_true_labels_matrix[1:n_trials_display, 1:n_time_display]
pred_subset = all_pred_labels[1:n_trials_display, 1:n_time_display]

p5 = plot(
    heatmap(true_subset, colormap=:roma, title=&quot;True States ($n_trials_display trials)&quot;,
           xticks=false, ylabel=&quot;Trial&quot;, colorbar=false),
    heatmap(pred_subset, colormap=:roma, title=&quot;Predicted States (Viterbi)&quot;,
           xlabel=&quot;Time Steps&quot;, ylabel=&quot;Trial&quot;, colorbar=false),
    layout=(2, 1), size=(900, 400)
)</code></pre><img src="751a7165.svg" alt="Example block output"/><h2 id="Parameter-Recovery-Assessment"><a class="docs-heading-anchor" href="#Parameter-Recovery-Assessment">Parameter Recovery Assessment</a><a id="Parameter-Recovery-Assessment-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Recovery-Assessment" title="Permalink"></a></h2><p>Since we have access to the true model parameters, we can quantitatively assess how well the multi-trial fitting procedure recovered them.</p><pre><code class="language-julia hljs">true_μ1_orig, true_μ2_orig = [-1.0, -1.0], [1.0, 1.0]
learned_μ1 = test_model_multi.B[1].μ
learned_μ2 = test_model_multi.B[2].μ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 1.0064258376923567
 1.004188904637427</code></pre><p>Compare emission model mean vectors</p><pre><code class="language-julia hljs">μ1_error = norm(true_μ1_orig - learned_μ1) / norm(true_μ1_orig)
μ2_error = norm(true_μ2_orig - learned_μ2) / norm(true_μ2_orig)

print(&quot;Mean vector recovery errors:\n&quot;)
print(&quot;State 1: $(round(μ1_error*100, digits=1))%, State 2: $(round(μ2_error*100, digits=1))%\n&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Mean vector recovery errors:
State 1: 0.2%, State 2: 0.5%</code></pre><p>Compare covariance matrices</p><pre><code class="language-julia hljs">true_Σ1_orig, true_Σ2_orig = 0.1, 0.2
learned_Σ1 = test_model_multi.B[1].Σ[1,1]
learned_Σ2 = test_model_multi.B[2].Σ[1,1]

Σ1_error = abs(true_Σ1_orig - learned_Σ1) / true_Σ1_orig
Σ2_error = abs(true_Σ2_orig - learned_Σ2) / true_Σ2_orig

print(&quot;Variance recovery errors:\n&quot;)
print(&quot;State 1: $(round(Σ1_error*100, digits=1))%, State 2: $(round(Σ2_error*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Variance recovery errors:
State 1: 0.4%, State 2: 0.2%</code></pre><p>Compare transition matrices</p><pre><code class="language-julia hljs">true_A_orig = [0.99 0.01; 0.05 0.95]
A_error = norm(true_A_orig - test_model_multi.A) / norm(true_A_orig)
print(&quot;Transition matrix error: $(round(A_error*100, digits=1))%\n&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Transition matrix error: 0.3%</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This tutorial demonstrated the complete workflow for Gaussian emission Hidden Markov Models. We covered how to create, sample from, fit, and perform state inference with HMMs using <code>StateSpaceDynamics.jl</code>.</p><p><strong>Key Concepts:</strong></p><ul><li><strong>Discrete hidden states</strong> with Gaussian emission distributions</li><li><strong>Temporal dependencies</strong> through Markovian state transitions</li><li><strong>EM algorithm</strong> for joint parameter learning and state inference</li><li><strong>Viterbi decoding</strong> for finding most likely state sequences</li></ul><p><strong>Technical Insights:</strong></p><ul><li><strong>Label switching</strong> is a common identifiability issue requiring detection and correction</li><li><strong>Multi-trial analysis</strong> provides more robust parameter estimates than single sequences</li><li><strong>Parameter recovery</strong> quality depends on state separation and sequence length</li><li><strong>Convergence monitoring</strong> through log-likelihood plots ensures proper algorithm behavior</li></ul><p><strong>Applications:</strong></p><ul><li>Time series clustering and regime detection</li><li>Behavioral state analysis in sequential data</li><li>Exploratory analysis of temporal datasets with latent structure</li><li>Foundation for more complex state-space models</li></ul><p>Gaussian HMMs provide a fundamental framework for modeling sequential data with discrete latent structure, serving as both standalone models and building blocks for more sophisticated probabilistic time series methods.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../lds_identifiability_example/">« Non-Identifiability in LDS Models</a><a class="docs-footer-nextpage" href="../hmm_model_selection_example/">HMM Model Selection »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Friday 19 September 2025 15:10">Friday 19 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
