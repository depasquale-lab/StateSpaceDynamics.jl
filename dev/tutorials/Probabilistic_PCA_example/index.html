<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Probabilistic PCA Example · StateSpaceDynamics.jl</title><meta name="title" content="Probabilistic PCA Example · StateSpaceDynamics.jl"/><meta property="og:title" content="Probabilistic PCA Example · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Probabilistic PCA Example · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../EmissionModels/">EmissionModels</a></li><li><a class="tocitem" href="../../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../hidden_markov_model_example/">Hidden Markov Model Example</a></li><li><a class="tocitem" href="../hmm_model_selection_example/">HMM Model Selection</a></li><li><a class="tocitem" href="../gaussian_glm_hmm_example/">Gaussian GLM-HMM Example</a></li><li><a class="tocitem" href="../gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li><a class="tocitem" href="../poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li class="is-active"><a class="tocitem" href>Probabilistic PCA Example</a><ul class="internal"><li><a class="tocitem" href="#The-PPCA-Model"><span>The PPCA Model</span></a></li><li><a class="tocitem" href="#Load-Required-Packages"><span>Load Required Packages</span></a></li><li><a class="tocitem" href="#Create-and-Simulate-PPCA-Model"><span>Create and Simulate PPCA Model</span></a></li><li><a class="tocitem" href="#Visualize-Simulated-Data"><span>Visualize Simulated Data</span></a></li><li><a class="tocitem" href="#Fit-PPCA-Using-EM-Algorithm"><span>Fit PPCA Using EM Algorithm</span></a></li><li><a class="tocitem" href="#Visualize-Learned-Loading-Directions"><span>Visualize Learned Loading Directions</span></a></li><li><a class="tocitem" href="#Posterior-Inference-and-Reconstruction"><span>Posterior Inference and Reconstruction</span></a></li><li><a class="tocitem" href="#Variance-Explained-Analysis"><span>Variance Explained Analysis</span></a></li><li><a class="tocitem" href="#Parameter-Recovery-Assessment"><span>Parameter Recovery Assessment</span></a></li><li><a class="tocitem" href="#Model-Selection-Example"><span>Model Selection Example</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li><li><a class="tocitem" href="../../Misc/">Miscellaneous</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Probabilistic PCA Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Probabilistic PCA Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/examples/ProbabilisticPCA.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model"><a class="docs-heading-anchor" href="#Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model">Simulating and Fitting a Probabilistic PCA (PPCA) Model</a><a id="Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model" title="Permalink"></a></h1><p>This tutorial demonstrates <strong>Probabilistic PCA (PPCA)</strong> in <code>StateSpaceDynamics.jl</code>: simulating data, fitting with EM, and interpreting results. PPCA is a maximum-likelihood, probabilistic version of PCA with an explicit latent-variable generative model and noise model.</p><h2 id="The-PPCA-Model"><a class="docs-heading-anchor" href="#The-PPCA-Model">The PPCA Model</a><a id="The-PPCA-Model-1"></a><a class="docs-heading-anchor-permalink" href="#The-PPCA-Model" title="Permalink"></a></h2><p>The generative model for observations <span>$\mathbf{x} \in \mathbb{R}^D$</span> with <span>$k$</span> latent factors: <span>$\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_k), \quad \mathbf{x} | \mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu} + \mathbf{W}\mathbf{z}, \sigma^2 \mathbf{I}_D)$</span></p><p>where <span>$\mathbf{W} \in \mathbb{R}^{D \times k}$</span> (factor loadings), <span>$\boldsymbol{\mu} \in \mathbb{R}^D$</span> (mean), and <span>$\sigma^2 &gt; 0$</span> (isotropic noise variance).</p><p><strong>Key properties:</strong></p><ul><li>Marginal covariance: <span>$\text{Cov}(\mathbf{x}) = \mathbf{W}\mathbf{W}^T + \sigma^2 \mathbf{I}$</span></li><li>As <span>$\sigma^2 \to 0$</span>, PPCA approaches standard PCA</li><li>Rotational non-identifiability: <span>$\mathbf{W}\mathbf{R}$</span> for orthogonal <span>$\mathbf{R}$</span> spans same subspace</li></ul><p><strong>Posterior over latents:</strong> Given observation <span>$\mathbf{x}$</span>, the posterior is Gaussian with: <span>$\mathbf{M} = \mathbf{I}_k + \frac{1}{\sigma^2}\mathbf{W}^T\mathbf{W}, \quad \mathbb{E}[\mathbf{z}|\mathbf{x}] = \mathbf{M}^{-1}\mathbf{W}^T(\mathbf{x}-\boldsymbol{\mu})/\sigma^2$</span></p><h2 id="Load-Required-Packages"><a class="docs-heading-anchor" href="#Load-Required-Packages">Load Required Packages</a><a id="Load-Required-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Required-Packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using StateSpaceDynamics
using LinearAlgebra
using Random
using Plots
using StatsPlots
using StableRNGs
using Distributions
using LaTeXStrings</code></pre><p>Set reproducible randomness for simulation and initialization</p><pre><code class="language-julia hljs">rng = StableRNG(1234);</code></pre><h2 id="Create-and-Simulate-PPCA-Model"><a class="docs-heading-anchor" href="#Create-and-Simulate-PPCA-Model">Create and Simulate PPCA Model</a><a id="Create-and-Simulate-PPCA-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Create-and-Simulate-PPCA-Model" title="Permalink"></a></h2><p>We&#39;ll work in 2D with two latent factors for easy visualization and interpretation.</p><pre><code class="language-julia hljs">D = 2  # Observation dimensionality
k = 2  # Number of latent factors</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2</code></pre><p>True parameters for data generation</p><pre><code class="language-julia hljs">W_true = [-1.64  0.2;   # Factor loading matrix
           0.9  -2.8]
σ²_true = 0.5           # Noise variance
μ_true = [1.65, -1.3];  # Mean vector

ppca = ProbabilisticPCA(W_true, σ²_true, μ_true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ProbabilisticPCA{Float64, Matrix{Float64}, Vector{Float64}}([-1.64 0.2; 0.9 -2.8], 0.5, [1.65, -1.3], 2, 2, Matrix{Float64}(undef, 2, 0))</code></pre><p>Generate synthetic data</p><pre><code class="language-julia hljs">num_obs = 500
X, z = rand(rng, ppca, num_obs);

print(&quot;Generated $num_obs observations in $D dimensions with $k latent factors\n&quot;)
print(&quot;Data range: X₁ ∈ [$(round(minimum(X[1,:]), digits=2)), $(round(maximum(X[1,:]), digits=2))], &quot;)
print(&quot;X₂ ∈ [$(round(minimum(X[2,:]), digits=2)), $(round(maximum(X[2,:]), digits=2))]\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generated 500 observations in 2 dimensions with 2 latent factors
Data range: X₁ ∈ [-5.97, 6.68], X₂ ∈ [-10.54, 7.94]</code></pre><h2 id="Visualize-Simulated-Data"><a class="docs-heading-anchor" href="#Visualize-Simulated-Data">Visualize Simulated Data</a><a id="Visualize-Simulated-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-Simulated-Data" title="Permalink"></a></h2><p>Color points by dominant latent dimension for intuition (latent variables are unobserved in practice)</p><pre><code class="language-julia hljs">x1, x2 = X[1, :], X[2, :]
labels = [abs(z[1,i]) &gt; abs(z[2,i]) ? 1 : 2 for i in 1:size(z,2)]

p1 = scatter(x1, x2;
    group=labels, xlabel=L&quot;X_1&quot;, ylabel=L&quot;X_2&quot;,
    title=&quot;Simulated Data (colored by dominant latent factor)&quot;,
    markersize=4, alpha=0.7,
    palette=[:dodgerblue, :crimson],
    legend=:topright
)</code></pre><img src="4f3c6780.svg" alt="Example block output"/><h2 id="Fit-PPCA-Using-EM-Algorithm"><a class="docs-heading-anchor" href="#Fit-PPCA-Using-EM-Algorithm">Fit PPCA Using EM Algorithm</a><a id="Fit-PPCA-Using-EM-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-PPCA-Using-EM-Algorithm" title="Permalink"></a></h2><p>Start from random initialization and use EM to learn parameters. The algorithm maximizes the marginal log-likelihood of the observed data.</p><p>Initialize with random parameters</p><pre><code class="language-julia hljs">W_init = randn(rng, D, k)
σ²_init = 0.5
μ_init = randn(rng, D)

fit_ppca = ProbabilisticPCA(W_init, σ²_init, μ_init)

print(&quot;Running EM algorithm...&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running EM algorithm...</code></pre><p>Fit with EM - returns log-likelihood trace for convergence monitoring</p><pre><code class="language-julia hljs">lls = fit!(fit_ppca, X);

print(&quot;EM converged in $(length(lls)) iterations\n&quot;)
print(&quot;Log-likelihood improved by $(round(lls[end] - lls[1], digits=1))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">EM converged in 26 iterations
Log-likelihood improved by 376.6</code></pre><p>Monitor EM convergence - should show monotonic increase</p><pre><code class="language-julia hljs">p2 = plot(lls;
    xlabel=&quot;Iteration&quot;, ylabel=&quot;Log-Likelihood&quot;,
    title=&quot;EM Convergence&quot;, marker=:circle, markersize=3,
    lw=2, legend=false, color=:darkblue
)</code></pre><img src="e719d5cb.svg" alt="Example block output"/><h2 id="Visualize-Learned-Loading-Directions"><a class="docs-heading-anchor" href="#Visualize-Learned-Loading-Directions">Visualize Learned Loading Directions</a><a id="Visualize-Learned-Loading-Directions-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-Learned-Loading-Directions" title="Permalink"></a></h2><p>The columns of <span>$\mathbf{W}$</span> span the principal subspace (up to rotation). We&#39;ll plot these loading vectors from the learned mean.</p><pre><code class="language-julia hljs">μ_fit = fit_ppca.μ
W_fit = fit_ppca.W
w1, w2 = W_fit[:, 1], W_fit[:, 2]

p3 = scatter(x1, x2;
    xlabel=L&quot;X_1&quot;, ylabel=L&quot;X_2&quot;,
    title=&quot;Data with Learned PPCA Loading Directions&quot;,
    label=&quot;Data&quot;, alpha=0.5, markersize=3, color=:gray
)</code></pre><img src="86014eb1.svg" alt="Example block output"/><p>Draw loading vectors in both directions for visibility</p><pre><code class="language-julia hljs">scale = 2.0  # Scale for better visualization
quiver!(p3, [μ_fit[1]], [μ_fit[2]];
    quiver=([scale*w1[1]], [scale*w1[2]]),
    arrow=:arrow, lw=3, color=:red, label=&quot;W₁&quot;)
quiver!(p3, [μ_fit[1]], [μ_fit[2]];
    quiver=([-scale*w1[1]], [-scale*w1[2]]),
    arrow=:arrow, lw=3, color=:red, label=&quot;&quot;)
quiver!(p3, [μ_fit[1]], [μ_fit[2]];
    quiver=([scale*w2[1]], [scale*w2[2]]),
    arrow=:arrow, lw=3, color=:green, label=&quot;W₂&quot;)
quiver!(p3, [μ_fit[1]], [μ_fit[2]];
    quiver=([-scale*w2[1]], [-scale*w2[2]]),
    arrow=:arrow, lw=3, color=:green, label=&quot;&quot;)</code></pre><img src="aecf327d.svg" alt="Example block output"/><h2 id="Posterior-Inference-and-Reconstruction"><a class="docs-heading-anchor" href="#Posterior-Inference-and-Reconstruction">Posterior Inference and Reconstruction</a><a id="Posterior-Inference-and-Reconstruction-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Inference-and-Reconstruction" title="Permalink"></a></h2><p>Compute posterior means <span>$\mathbb{E}[\mathbf{z}|\mathbf{x}]$</span> and reconstructions <span>$\hat{\mathbf{x}} = \boldsymbol{\mu} + \mathbf{W}\mathbb{E}[\mathbf{z}|\mathbf{x}]$</span></p><pre><code class="language-julia hljs">function ppca_posterior_means(W::AbstractMatrix, σ²::Real, μ::AbstractVector, X::AbstractMatrix)
    k = size(W, 2)
    M = I(k) + (W&#39; * W) / σ²            # Posterior precision matrix
    B = M \ (W&#39; / σ²)                   # Efficient computation of M⁻¹W^T/σ²
    Z_mean = B * (X .- μ)               # Posterior means
    return Z_mean
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ppca_posterior_means (generic function with 1 method)</code></pre><p>Compute posterior latent means and reconstructions</p><pre><code class="language-julia hljs">Ẑ = ppca_posterior_means(W_fit, fit_ppca.σ², μ_fit, X)
X̂ = μ_fit .+ W_fit * Ẑ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×500 Matrix{Float64}:
  0.467832   0.800462   0.116062   0.898647  …   2.09684  -2.34089  -0.414906
 -1.28645   -1.476     -0.786339  -1.6469       -2.96826   1.82418  -0.192704</code></pre><p>Calculate reconstruction error</p><pre><code class="language-julia hljs">recon_mse = mean(sum((X - X̂).^2, dims=1)) / D
print(&quot;Reconstruction MSE: $(round(recon_mse, digits=4))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Reconstruction MSE: 2.6575</code></pre><h2 id="Variance-Explained-Analysis"><a class="docs-heading-anchor" href="#Variance-Explained-Analysis">Variance Explained Analysis</a><a id="Variance-Explained-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Variance-Explained-Analysis" title="Permalink"></a></h2><p>Compare sample covariance eigenvalues to PPCA model structure. PPCA should capture top-k eigenvalues via <span>$\mathbf{W}\mathbf{W}^T$</span> and approximate remainder with isotropic noise <span>$\sigma^2$</span>.</p><pre><code class="language-julia hljs">Σ_sample = cov(X, dims=2)  # Sample covariance matrix
λs = sort(eigvals(Σ_sample), rev=true)  # Eigenvalues in descending order</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 10.050826930825101
  2.737541654309271</code></pre><p>Proportion of variance explained by top-k eigenvalues</p><pre><code class="language-julia hljs">pve_sample = sum(λs[1:k]) / sum(λs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.0</code></pre><p>PPCA-implied variance components</p><pre><code class="language-julia hljs">total_var_ppca = tr(W_fit * W_fit&#39;) + D * fit_ppca.σ²
explained_var_ppca = tr(W_fit * W_fit&#39;)
pve_ppca = explained_var_ppca / total_var_ppca

print(&quot;Variance Analysis:\n&quot;)
print(&quot;Sample eigenvalues: $(round.(λs, digits=3))\n&quot;)
print(&quot;PVE (sample top-$k): $(round(pve_sample*100, digits=1))%\n&quot;)
print(&quot;PVE (PPCA model): $(round(pve_ppca*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Variance Analysis:
Sample eigenvalues: [10.051, 2.738]
PVE (sample top-2): 100.0%
PVE (PPCA model): 65.8%</code></pre><h2 id="Parameter-Recovery-Assessment"><a class="docs-heading-anchor" href="#Parameter-Recovery-Assessment">Parameter Recovery Assessment</a><a id="Parameter-Recovery-Assessment-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Recovery-Assessment" title="Permalink"></a></h2><pre><code class="language-julia hljs">print(&quot;\n=== Parameter Recovery Assessment ===\n&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">
=== Parameter Recovery Assessment ===</code></pre><p>Compare true vs learned parameters</p><pre><code class="language-julia hljs">W_error = norm(W_true - W_fit) / norm(W_true)
μ_error = norm(μ_true - μ_fit) / norm(μ_true)
σ²_error = abs(σ²_true - fit_ppca.σ²) / σ²_true

print(&quot;Parameter recovery errors:\n&quot;)
print(&quot;Loading matrix W: $(round(W_error*100, digits=1))%\n&quot;)
print(&quot;Mean vector μ: $(round(μ_error*100, digits=1))%\n&quot;)
print(&quot;Noise variance σ²: $(round(σ²_error*100, digits=1))%\n&quot;)

print(&quot;True parameters:\n&quot;)
print(&quot;W = $(round.(W_true, digits=2))\n&quot;)
print(&quot;μ = $(round.(μ_true, digits=2)), σ² = $(σ²_true)\n&quot;)

print(&quot;Learned parameters:\n&quot;)
print(&quot;W = $(round.(W_fit, digits=2))\n&quot;)
print(&quot;μ = $(round.(μ_fit, digits=2)), σ² = $(round(fit_ppca.σ², digits=2))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Parameter recovery errors:
Loading matrix W: 193.6%
Mean vector μ: 167.7%
Noise variance σ²: 794.9%
True parameters:
W = [-1.64 0.2; 0.9 -2.8]
μ = [1.65, -1.3], σ² = 0.5
Learned parameters:
W = [2.78 0.46; -3.04 -0.06]
μ = [-1.29, 0.65], σ² = 4.47</code></pre><h2 id="Model-Selection-Example"><a class="docs-heading-anchor" href="#Model-Selection-Example">Model Selection Example</a><a id="Model-Selection-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Selection-Example" title="Permalink"></a></h2><p>Demonstrate fitting models with different numbers of latent factors and comparing via information criteria.</p><pre><code class="language-julia hljs">function compute_aic_bic(ll::Real, n_params::Int, n_obs::Int)
    aic = 2*n_params - 2*ll
    bic = n_params*log(n_obs) - 2*ll
    return (aic, bic)
end

print(&quot;\n=== Model Selection Demo ===\n&quot;)

k_range = 1:min(D, 4)
aic_scores = Float64[]
bic_scores = Float64[]
lls_final = Float64[]

for k_test in k_range
    W_test = randn(rng, D, k_test) # Initialize and fit model with k_test factors
    ppca_test = ProbabilisticPCA(W_test, 0.5, zeros(D))
    lls_test = fit!(ppca_test, X)

    n_params = D * k_test + D + 1 # Parameter count: D*k_test (W) + D (μ) + 1 (σ²)
    ll_final = lls_test[end]
    aic, bic = compute_aic_bic(ll_final, n_params, num_obs)  # Calculate information criteria

    push!(aic_scores, aic)
    push!(bic_scores, bic)
    push!(lls_final, ll_final)

    print(&quot;k=$k_test: LL=$(round(ll_final, digits=1)), AIC=$(round(aic, digits=1)), BIC=$(round(bic, digits=1))\n&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">
=== Model Selection Demo ===
k=1: LL=-2246.6, AIC=4503.2, BIC=4524.3
k=2: LL=-2246.6, AIC=4507.2, BIC=4536.7</code></pre><p>Plot information criteria</p><pre><code class="language-julia hljs">p4 = plot(k_range, [aic_scores bic_scores];
    xlabel=&quot;Number of Latent Factors (k)&quot;, ylabel=&quot;Information Criterion&quot;,
    title=&quot;Model Selection via Information Criteria&quot;,
    label=[&quot;AIC&quot; &quot;BIC&quot;], marker=:circle, lw=2
)

optimal_k = k_range[argmin(bic_scores)]
print(&quot;BIC suggests optimal k = $optimal_k\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BIC suggests optimal k = 1</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This tutorial demonstrated the complete Probabilistic PCA workflow:</p><p><strong>Key Concepts:</strong></p><ul><li><strong>Probabilistic framework</strong>: Explicit generative model with latent factors and noise</li><li><strong>EM algorithm</strong>: Iterative maximum-likelihood parameter estimation</li><li><strong>Posterior inference</strong>: Probabilistic latent variable estimates and reconstructions</li><li><strong>Model selection</strong>: Information criteria for choosing appropriate number of factors</li></ul><p><strong>Advantages over Standard PCA:</strong></p><ul><li>Principled handling of missing data and noise</li><li>Probabilistic interpretation enables uncertainty quantification</li><li>Natural framework for model selection and comparison</li><li>Seamless extension to more complex latent variable models</li></ul><p><strong>Applications:</strong></p><ul><li>Dimensionality reduction for high-dimensional data</li><li>Exploratory data analysis and visualization</li><li>Feature extraction for machine learning pipelines</li><li>Foundation for more complex factor models and state-space models</li></ul><p><strong>Technical Insights:</strong></p><ul><li>Loading matrix <span>$\mathbf{W}$</span> captures principal directions of variation</li><li>Noise parameter <span>$\sigma^2$</span> quantifies unexplained variance</li><li>Rotational non-identifiability requires care in interpretation</li><li>EM convergence monitoring ensures reliable parameter estimates</li></ul><p>PPCA provides a flexible, probabilistic approach to factor analysis that bridges classical multivariate statistics with modern latent variable modeling, serving as both a standalone technique and building block for more sophisticated models.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../poisson_mixture_model_example/">« Poisson Mixture Model Example</a><a class="docs-footer-nextpage" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Wednesday 10 September 2025 18:38">Wednesday 10 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
