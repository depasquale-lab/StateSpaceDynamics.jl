<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Probabilistic PCA Example · StateSpaceDynamics.jl</title><meta name="title" content="Probabilistic PCA Example · StateSpaceDynamics.jl"/><meta property="og:title" content="Probabilistic PCA Example · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Probabilistic PCA Example · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../EmissionModels/">EmissionModels</a></li><li><a class="tocitem" href="../../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../hidden_markov_model_example/">Hidden Markov Model Example</a></li><li><a class="tocitem" href="../gaussian_glm_hmm_example/">Gaussian GLM-GMM Example</a></li><li><a class="tocitem" href="../gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li><a class="tocitem" href="../poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li class="is-active"><a class="tocitem" href>Probabilistic PCA Example</a><ul class="internal"><li><a class="tocitem" href="#The-PPCA-model-at-a-glance"><span>The PPCA model at a glance</span></a></li><li><a class="tocitem" href="#Load-Packages"><span>Load Packages</span></a></li><li><a class="tocitem" href="#Create-a-PPCA-model-and-simulate"><span>Create a PPCA model and simulate</span></a></li><li><a class="tocitem" href="#Visualize-the-simulated-data"><span>Visualize the simulated data</span></a></li><li><a class="tocitem" href="#Parameter-recovery:-fit-PPCA-with-EM"><span>Parameter recovery: fit PPCA with EM</span></a></li><li><a class="tocitem" href="#Interpreting-the-learned-parameters"><span>Interpreting the learned parameters</span></a></li><li><a class="tocitem" href="#Posterior-latents-and-reconstructions"><span>Posterior latents and reconstructions</span></a></li><li><a class="tocitem" href="#Variance-explained-and-choosing-k"><span>Variance explained and choosing k</span></a></li><li><a class="tocitem" href="#Practical-tips-and-pitfalls"><span>Practical tips &amp; pitfalls</span></a></li><li><a class="tocitem" href="#Where-this-fits-in-StateSpaceDynamics.jl"><span>Where this fits in <code>StateSpaceDynamics.jl</code></span></a></li><li><a class="tocitem" href="#Exercises"><span>Exercises</span></a></li></ul></li><li><a class="tocitem" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li><li><a class="tocitem" href="../../Misc/">Miscellaneous</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Probabilistic PCA Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Probabilistic PCA Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/examples/ProbabilisticPCA.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model"><a class="docs-heading-anchor" href="#Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model">Simulating and Fitting a Probabilistic PCA (PPCA) Model</a><a id="Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model" title="Permalink"></a></h1><p>This tutorial walks through <strong>Probabilistic PCA (PPCA)</strong> in <code>StateSpaceDynamics.jl</code>: simulating data, fitting with EM, and interpreting the results. PPCA is a maximum-likelihood, probabilistic version of PCA with a simple latent-variable generative model and an explicit noise model.</p><h2 id="The-PPCA-model-at-a-glance"><a class="docs-heading-anchor" href="#The-PPCA-model-at-a-glance">The PPCA model at a glance</a><a id="The-PPCA-model-at-a-glance-1"></a><a class="docs-heading-anchor-permalink" href="#The-PPCA-model-at-a-glance" title="Permalink"></a></h2><p>The generative story for (x\in\mathbb R^D) with (k) latent factors: [ z \sim \mathcal N(0, I<em>k),\qquad x \mid z \sim \mathcal N(\mu + W z,\ \sigma^2 I</em>D), ] where (W\in\mathbb R^{D\times k}) (factor loadings), (\mu\in\mathbb R^D), and (\sigma^2&gt;0) (isotropic noise). The marginal covariance is (\operatorname{Cov}(x)=W W^\top + \sigma^2 I). When (\sigma^2\to 0), PPCA approaches standard PCA. Rotations of (W) ((W R) for orthogonal (R)) span the same principal subspace—this is the usual rotational non-identifiability.</p><p><strong>Posterior over latents.</strong> Given an observed (x), the posterior is Gaussian with [ M = I_k + \tfrac{1}{\sigma^2} W^\top W,\qquad \mathbb E[z\mid x] = M^{-1} W^\top (x-\mu)/\sigma^2,\qquad \operatorname{Cov}(z\mid x) = M^{-1}. ] <code>fit!</code> in <code>StateSpaceDynamics.jl</code> performs EM to maximize the likelihood.</p><hr/><h2 id="Load-Packages"><a class="docs-heading-anchor" href="#Load-Packages">Load Packages</a><a id="Load-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using StateSpaceDynamics
using LinearAlgebra
using Random
using Plots
using StatsPlots
using StableRNGs
using Distributions</code></pre><p>Reproducible randomness for simulation and initialization.</p><pre><code class="language-julia hljs">rng = StableRNG(1234);</code></pre><h2 id="Create-a-PPCA-model-and-simulate"><a class="docs-heading-anchor" href="#Create-a-PPCA-model-and-simulate">Create a PPCA model and simulate</a><a id="Create-a-PPCA-model-and-simulate-1"></a><a class="docs-heading-anchor-permalink" href="#Create-a-PPCA-model-and-simulate" title="Permalink"></a></h2><p>We&#39;ll work in 2D with two latent factors for easy visualization.</p><pre><code class="language-julia hljs">D = 2
k = 2;</code></pre><p>True parameters used to generate synthetic data</p><pre><code class="language-julia hljs">W_true = [
   -1.64   0.2;
    0.9   -2.8
]

σ²_true = 0.5
μ_true  = [1.65, -1.3];

ppca = ProbabilisticPCA(W_true, σ²_true, μ_true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ProbabilisticPCA{Float64, Matrix{Float64}, Vector{Float64}}([-1.64 0.2; 0.9 -2.8], 0.5, [1.65, -1.3], 2, 2, Matrix{Float64}(undef, 2, 0))</code></pre><p>Draw IID samples from the model</p><pre><code class="language-julia hljs">num_obs = 500
X, z = rand(rng, ppca, num_obs);</code></pre><h2 id="Visualize-the-simulated-data"><a class="docs-heading-anchor" href="#Visualize-the-simulated-data">Visualize the simulated data</a><a id="Visualize-the-simulated-data-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-the-simulated-data" title="Permalink"></a></h2><p>We&#39;ll color points by the dominant latent dimension (for intuition only—latent variables are unobserved in real data).</p><pre><code class="language-julia hljs">x1 = X[1, :]
x2 = X[2, :]
labels = map(i -&gt; (abs(z[1,i]) &gt; abs(z[2,i]) ? 1 : 2), 1:size(z,2))

p = plot()
scatter!(
    p, x1, x2;
    group      = labels,
    xlabel     = &quot;X₁&quot;,
    ylabel     = &quot;X₂&quot;,
    title      = &quot;Samples grouped by dominant latent factor&quot;,
    label      = [&quot;Latent 1&quot; &quot;Latent 2&quot;],
    legend     = :topright,
    markersize = 5,
)

p</code></pre><img src="33a1b05b.svg" alt="Example block output"/><h2 id="Parameter-recovery:-fit-PPCA-with-EM"><a class="docs-heading-anchor" href="#Parameter-recovery:-fit-PPCA-with-EM">Parameter recovery: fit PPCA with EM</a><a id="Parameter-recovery:-fit-PPCA-with-EM-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-recovery:-fit-PPCA-with-EM" title="Permalink"></a></h2><p>We&#39;ll start from random loadings/mean and a reasonable noise variance, then call <code>fit!</code> to run EM until convergence.</p><p>(Re)define defaults to emphasize the fitting path</p><pre><code class="language-julia hljs">D = 2
k = 2
W = randn(rng, D, k)
σ² = 0.5
μ_vector = randn(rng, 2)

fit_ppca = ProbabilisticPCA(W, σ², μ_vector);</code></pre><h3 id="Fit-with-EM"><a class="docs-heading-anchor" href="#Fit-with-EM">Fit with EM</a><a id="Fit-with-EM-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-with-EM" title="Permalink"></a></h3><p><code>fit!</code> returns the log-likelihood trace. Monotone ascent is a good sanity check.</p><pre><code class="language-julia hljs">lls = fit!(fit_ppca, X)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">26-element Vector{Float64}:
 -2942.9846914566488
 -2649.021975079838
 -2596.3727771298704
 -2578.2703501279584
 -2571.2629060042264
 -2568.437824572584
 -2567.265115905659
 -2566.7600331093895
 -2566.5343236584004
 -2566.430444632034
     ⋮
 -2566.337136047855
 -2566.3369917179375
 -2566.3369215564903
 -2566.336887441408
 -2566.336870850639
 -2566.3368627813325
 -2566.3368588563276
 -2566.3368569470554
 -2566.336856018277</code></pre><h3 id="Log-likelihood-diagnostic"><a class="docs-heading-anchor" href="#Log-likelihood-diagnostic">Log-likelihood diagnostic</a><a id="Log-likelihood-diagnostic-1"></a><a class="docs-heading-anchor-permalink" href="#Log-likelihood-diagnostic" title="Permalink"></a></h3><pre><code class="language-julia hljs">ll_plot = plot(
    lls;
    xlabel=&quot;Iteration&quot;,
    ylabel=&quot;Log-Likelihood&quot;,
    title=&quot;EM Convergence (PPCA)&quot;,
    marker=:circle,
    label=&quot;log_likelihood&quot;,
    reuse=false,
)

ll_plot</code></pre><img src="e3305840.svg" alt="Example block output"/><h2 id="Interpreting-the-learned-parameters"><a class="docs-heading-anchor" href="#Interpreting-the-learned-parameters">Interpreting the learned parameters</a><a id="Interpreting-the-learned-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Interpreting-the-learned-parameters" title="Permalink"></a></h2><ul><li><code>fit_ppca.W</code> are the learned loading directions. Columns span the principal subspace (up to rotation). For k=2 in 2D, they form a basis centered at μ.</li><li><code>fit_ppca.μ</code> is the learned mean of the data.</li><li><code>fit_ppca.σ²</code> is the isotropic residual variance.</li></ul><pre><code class="language-julia hljs">x1, x2 = X[1, :], X[2, :]
μ1, μ2  = fit_ppca.μ
W_fit   = fit_ppca.W
w1      = W_fit[:, 1]
w2      = W_fit[:, 2]

P = plot()
scatter!(
    P, x1, x2;
    xlabel     = &quot;X₁&quot;,
    ylabel     = &quot;X₂&quot;,
    title      = &quot;Data with PPCA loading directions&quot;,
    label      = &quot;Data&quot;,
    alpha      = 0.5,
    markersize = 4,
);</code></pre><p>Draw loading vectors from the mean in both directions for visibility</p><pre><code class="language-julia hljs">quiver!(P, [μ1], [μ2]; quiver=([ w1[1]], [ w1[2]]), arrow=:arrow, lw=3, color=:red,   label=&quot;W₁&quot;)
quiver!(P, [μ1], [μ2]; quiver=([-w1[1]], [-w1[2]]), arrow=:arrow, lw=3, color=:red,   label=&quot;&quot;)
quiver!(P, [μ1], [μ2]; quiver=([ w2[1]], [ w2[2]]), arrow=:arrow, lw=3, color=:green, label=&quot;W₂&quot;)
quiver!(P, [μ1], [μ2]; quiver=([-w2[1]], [-w2[2]]), arrow=:arrow, lw=3, color=:green, label=&quot;&quot;)

P</code></pre><img src="900de4f3.svg" alt="Example block output"/><h2 id="Posterior-latents-and-reconstructions"><a class="docs-heading-anchor" href="#Posterior-latents-and-reconstructions">Posterior latents and reconstructions</a><a id="Posterior-latents-and-reconstructions-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-latents-and-reconstructions" title="Permalink"></a></h2><p>Compute \nE[z|x]\n and optional reconstructions (\hat x = \mu + W\,\mathbb E[z|x]).</p><pre><code class="language-julia hljs">function ppca_posterior_means(W::AbstractMatrix, σ²::Real, μ::AbstractVector, X::AbstractMatrix)
    D, N = size(X)
    k    = size(W, 2)
    M = I(k) + (W&#39; * W) / σ²            # k×k
    B = M \ (W&#39; / σ²)                  # k×D, equals M^{-1} W^T / σ²
    Zmean = B * (X .- μ)               # k×N
    return Zmean
end

Ẑ = ppca_posterior_means(W_fit, fit_ppca.σ², fit_ppca.μ, X)
X̂ = fit_ppca.μ .+ W_fit * Ẑ;</code></pre><p>Example: reconstruction error (per-dimension MSE)</p><pre><code class="language-julia hljs">recon_mse = mean(norm.(eachcol(X - X̂)).^2) / size(X,1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.657489894980282</code></pre><h2 id="Variance-explained-and-choosing-k"><a class="docs-heading-anchor" href="#Variance-explained-and-choosing-k">Variance explained and choosing k</a><a id="Variance-explained-and-choosing-k-1"></a><a class="docs-heading-anchor-permalink" href="#Variance-explained-and-choosing-k" title="Permalink"></a></h2><p>A quick check is to compare the sample covariance eigenvalues to the PPCA model. For PPCA with k factors, the top-k eigenvalues should be captured by W W^T and the remainder approximated by σ².</p><pre><code class="language-julia hljs">Σ̂ = cov(permutedims(X))            # D×D sample covariance
λs = sort(eigvals(Symmetric(Σ̂)); rev=true);</code></pre><p>Proportion of variance explained by top-k sample eigenvalues</p><pre><code class="language-julia hljs">pve_sample = sum(λs[1:k]) / sum(λs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.0</code></pre><p>PPCA-implied total variance: tr(WW^T) + D*σ²</p><pre><code class="language-julia hljs">pve_ppca = (tr(W_fit * W_fit&#39;) ) / (tr(W_fit * W_fit&#39;) + size(X,1) * 0 + length(μ1:μ2) * fit_ppca.σ²)  # placeholder formula to keep inline; see note below</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.6576121171586113</code></pre><p>NOTE: For PPCA in D dims, total variance is tr(WW^T) + D<em>σ². If you prefer, compute: pve<em>ppca = tr(W</em>fit</em>W<em>fit&#39;) / (tr(W</em>fit<em>W_fit&#39;) + D</em>fit_ppca.σ²)</p><h2 id="Practical-tips-and-pitfalls"><a class="docs-heading-anchor" href="#Practical-tips-and-pitfalls">Practical tips &amp; pitfalls</a><a id="Practical-tips-and-pitfalls-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-tips-and-pitfalls" title="Permalink"></a></h2><ul><li><strong>Scaling matters.</strong> Standardize your features if units differ.</li><li><strong>Initialization:</strong> Multiple random starts can help avoid poor local optima.</li><li><strong>Rotational ambiguity:</strong> For presentation, you can orthonormalize W or align it with PCA loadings via Procrustes.</li><li><strong>Choosing k:</strong> Use scree plots, PVE, or information criteria (AIC/BIC) on the marginal likelihood returned by <code>fit!</code>.</li><li><strong>Outliers/heavy tails:</strong> Consider robust variants (e.g., t-PCA) if needed.</li></ul><h2 id="Where-this-fits-in-StateSpaceDynamics.jl"><a class="docs-heading-anchor" href="#Where-this-fits-in-StateSpaceDynamics.jl">Where this fits in <code>StateSpaceDynamics.jl</code></a><a id="Where-this-fits-in-StateSpaceDynamics.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Where-this-fits-in-StateSpaceDynamics.jl" title="Permalink"></a></h2><p>PPCA is an IID latent-factor model. In the ecosystem, it bridges to time-series models with latent structure, such as LDS/PLDS where factors evolve over time. The workflow mirrors those models: specify (W, σ², μ), simulate, fit with EM, and validate with likelihood curves and posterior diagnostics.</p><h2 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h2><ol><li><strong>Compare to PCA:</strong> Compute the top-2 eigenvectors of the sample covariance and align <code>fit_ppca.W</code> to them with an orthogonal Procrustes transform.</li><li><strong>Vary noise:</strong> Increase <code>σ²_true</code> and see how loading directions and convergence behave.</li><li><strong>Model selection:</strong> Fit k=1..D and report AIC/BIC (parameter count is p = D*k + 1 (σ²) + D (μ)).</li><li><strong>Held-out likelihood:</strong> Split X into train/validation and compare models.</li></ol><hr/><p>End of tutorial.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../poisson_mixture_model_example/">« Poisson Mixture Model Example</a><a class="docs-footer-nextpage" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 9 September 2025 14:53">Tuesday 9 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
