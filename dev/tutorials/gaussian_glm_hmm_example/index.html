<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gaussian GLM-HMM Example · StateSpaceDynamics.jl</title><meta name="title" content="Gaussian GLM-HMM Example · StateSpaceDynamics.jl"/><meta property="og:title" content="Gaussian GLM-HMM Example · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Gaussian GLM-HMM Example · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../EmissionModels/">EmissionModels</a></li><li><a class="tocitem" href="../../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../lds_model_selection_example/">LDS Model Selection Example</a></li><li><a class="tocitem" href="../lds_identifiability_example/">Non-Identifiability in LDS Models</a></li><li><a class="tocitem" href="../hidden_markov_model_example/">Hidden Markov Model Example</a></li><li><a class="tocitem" href="../hmm_model_selection_example/">HMM Model Selection</a></li><li class="is-active"><a class="tocitem" href>Gaussian GLM-HMM Example</a><ul class="internal"><li><a class="tocitem" href="#Load-Required-Packages"><span>Load Required Packages</span></a></li><li><a class="tocitem" href="#Create-a-Gaussian-GLM-HMM"><span>Create a Gaussian GLM-HMM</span></a></li><li><a class="tocitem" href="#Sample-from-the-GLM-HMM"><span>Sample from the GLM-HMM</span></a></li><li><a class="tocitem" href="#Visualize-the-Sampled-Dataset"><span>Visualize the Sampled Dataset</span></a></li><li><a class="tocitem" href="#Initialize-and-Fit-HMM-with-EM"><span>Initialize and Fit HMM with EM</span></a></li><li><a class="tocitem" href="#Visualize-Learned-vs-True-Regression-Models"><span>Visualize Learned vs True Regression Models</span></a></li><li><a class="tocitem" href="#Hidden-State-Decoding-with-Viterbi-Algorithm"><span>Hidden State Decoding with Viterbi Algorithm</span></a></li><li><a class="tocitem" href="#Multiple-Independent-Trials"><span>Multiple Independent Trials</span></a></li><li><a class="tocitem" href="#Fitting-HMM-to-Multiple-Trials"><span>Fitting HMM to Multiple Trials</span></a></li><li><a class="tocitem" href="#Multi-Trial-State-Decoding"><span>Multi-Trial State Decoding</span></a></li><li><a class="tocitem" href="#Model-Assessment-Summary"><span>Model Assessment Summary</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../hmm_identifiability_example/">HMM Identifiability</a></li><li><a class="tocitem" href="../gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li><a class="tocitem" href="../poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li><a class="tocitem" href="../Probabilistic_PCA_example/">Probabilistic PCA Example</a></li><li><a class="tocitem" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li><li><a class="tocitem" href="../../Misc/">Miscellaneous</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Gaussian GLM-HMM Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Gaussian GLM-HMM Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/examples/Gaussian_GLM_HMM.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Simulating-and-Fitting-a-Hidden-Markov-Model"><a class="docs-heading-anchor" href="#Simulating-and-Fitting-a-Hidden-Markov-Model">Simulating and Fitting a Hidden Markov Model</a><a id="Simulating-and-Fitting-a-Hidden-Markov-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-and-Fitting-a-Hidden-Markov-Model" title="Permalink"></a></h1><p>This tutorial demonstrates how to use <code>StateSpaceDynamics.jl</code> to create, sample from, and fit Hidden Markov Models (HMMs). Unlike Linear Dynamical Systems which have continuous latent states, HMMs have discrete latent states that switch between a finite number of modes. This makes them ideal for modeling data with distinct behavioral regimes, switching dynamics, or categorical latent structure.</p><p>We&#39;ll focus on a Gaussian generalized linear model HMM (GLM-HMM), where each hidden state corresponds to a different regression relationship between inputs and outputs. This is particularly useful for modeling data where the input-output relationship changes over time in discrete jumps.</p><h2 id="Load-Required-Packages"><a class="docs-heading-anchor" href="#Load-Required-Packages">Load Required Packages</a><a id="Load-Required-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Required-Packages" title="Permalink"></a></h2><pre><code class="language-julia hljs">using LinearAlgebra
using Plots
using Random
using StateSpaceDynamics
using StableRNGs
using Statistics: mean</code></pre><p>Set up reproducible random number generation</p><pre><code class="language-julia hljs">rng = StableRNG(1234);</code></pre><h2 id="Create-a-Gaussian-GLM-HMM"><a class="docs-heading-anchor" href="#Create-a-Gaussian-GLM-HMM">Create a Gaussian GLM-HMM</a><a id="Create-a-Gaussian-GLM-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Create-a-Gaussian-GLM-HMM" title="Permalink"></a></h2><p>In a GLM-HMM, each hidden state defines a different regression model. The system switches between these regression models according to Markovian dynamics. This is useful for modeling scenarios where the relationship between predictors and outcomes changes over time. In this example, we will demonstrate how to use <code>StateSpaceDynamics.jl</code> to create a GLM-HMM, generate synthetic data, fit the model using the EM algorithm, and perform state inference with the Viterbi algorithm.</p><p>We will start by defining a simple GLM-HMM with two hidden states. State 1: Positive relationship between input and output</p><pre><code class="language-julia hljs">emission_1 = GaussianRegressionEmission(
    input_dim=3,                                    # Number of input features
    output_dim=1,                                   # Number of output dimensions
    include_intercept=true,                         # Include intercept term
    β=reshape([3.0, 2.0, 2.0, 3.0], :, 1),        # Regression coefficients [intercept, β₁, β₂, β₃]
    Σ=[1.0;;],                                     # Observation noise variance
    λ=0.0                                          # Regularization parameter
);</code></pre><p>State 2: Different relationship (negative intercept, different slopes)</p><pre><code class="language-julia hljs">emission_2 = GaussianRegressionEmission(
    input_dim=3,
    output_dim=1,
    include_intercept=true,
    β=reshape([-4.0, -2.0, 3.0, 2.0], :, 1),      # Different regression coefficients
    Σ=[1.0;;],                                     # Same noise level
    λ=0.0
);</code></pre><p>Define the state transition matrix <span>$\mathbf{A}$</span>: <span>$A_{ij} = P(\text{state}_t = j \mid \text{state}_{t-1} = i)$</span></p><p>Diagonal elements are high (states are persistent)</p><pre><code class="language-julia hljs">A = [0.99 0.01;    # From state 1: 99% stay, 1% switch to state 2
     0.05 0.95];    # From state 2: 5% switch to state 1, 95% stay</code></pre><p>Initial state distribution: <span>$\pi_k = P(\text{state}_1 = k)$</span></p><pre><code class="language-julia hljs">πₖ = [0.8; 0.2];    # 80% chance of starting in state 1, 20% in state 2</code></pre><p>Now that we have defined our emission models, we can construct the complete GLM-HMM.</p><pre><code class="language-julia hljs">true_model = HiddenMarkovModel(
    K=2,                        # Number of hidden states
    A=A,                        # Transition matrix
    πₖ=πₖ,                     # Initial state distribution
    B=[emission_1, emission_2]  # Emission models for each state
);

print(&quot;Created GLM-HMM with regression models:\n&quot;)
print(&quot;State 1: y = 3.0 + 2.0x₁ + 2.0x₂ + 3.0x₃ + ε\n&quot;)
print(&quot;State 2: y = -4.0 - 2.0x₁ + 3.0x₂ + 2.0x₃ + ε\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Created GLM-HMM with regression models:
State 1: y = 3.0 + 2.0x₁ + 2.0x₂ + 3.0x₃ + ε
State 2: y = -4.0 - 2.0x₁ + 3.0x₂ + 2.0x₃ + ε</code></pre><h2 id="Sample-from-the-GLM-HMM"><a class="docs-heading-anchor" href="#Sample-from-the-GLM-HMM">Sample from the GLM-HMM</a><a id="Sample-from-the-GLM-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-from-the-GLM-HMM" title="Permalink"></a></h2><p>Generate synthetic data from our true model. To do this, we must define the inputs to the model. Then, sampling will yield the outputs nad the true hidden state sequence.</p><pre><code class="language-julia hljs">n = 20000  # Number of time points

Φ = randn(rng, 3, n);  # Generate random input features (predictors)

true_labels, data = rand(rng, true_model, Φ, n=n);  # Sample from the HMM: returns both hidden states and observations

print(&quot;Generated $(n) samples: State 1 ($(round(mean(true_labels .== 1)*100, digits=1))%), State 2 ($(round(mean(true_labels .== 2)*100, digits=1))%)\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generated 20000 samples: State 1 (83.4%), State 2 (16.6%)</code></pre><h2 id="Visualize-the-Sampled-Dataset"><a class="docs-heading-anchor" href="#Visualize-the-Sampled-Dataset">Visualize the Sampled Dataset</a><a id="Visualize-the-Sampled-Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-the-Sampled-Dataset" title="Permalink"></a></h2><p>Here, we create a scatter plot to show how the input-output relationship differs between the two hidden states. We plot feature 1 vs output, with points colored by true state. State 1 (blue) has a positive slope, while State 2 (red) has a negative slopes. In addition to the scatter plot, we overlay the true regression lines for each state.</p><pre><code class="language-julia hljs">colors = [:dodgerblue, :crimson]  # Blue for state 1, red for state 2

p1 = scatter(Φ[1, :], vec(data);
    color = colors[true_labels],
    ms = 2,
    alpha = 0.4,
    label = &quot;&quot;,
    xlabel = &quot;Input Feature 1&quot;,
    ylabel = &quot;Output&quot;,
    title = &quot;GLM-HMM Data (colored by true state)&quot;
)

xvals = range(extrema(Φ[1, :])..., length=100) # Overlay true regression lines (holding other features at 0)

β1 = emission_1.β[:, 1]
y_pred_1 = β1[1] .+ β1[2] .* xvals  # intercept + slope*x₁
plot!(xvals, y_pred_1;
    color = :dodgerblue,
    lw = 3,
    label = &quot;State 1 regression&quot;
)

β2 = emission_2.β[:, 1]
y_pred_2 = β2[1] .+ β2[2] .* xvals  # intercept + slope*x₁
plot!(xvals, y_pred_2;
    color = :crimson,
    lw = 3,
    label = &quot;State 2 regression&quot;,
    legend = :topright
)</code></pre><img src="099f0a7d.svg" alt="Example block output"/><h2 id="Initialize-and-Fit-HMM-with-EM"><a class="docs-heading-anchor" href="#Initialize-and-Fit-HMM-with-EM">Initialize and Fit HMM with EM</a><a id="Initialize-and-Fit-HMM-with-EM-1"></a><a class="docs-heading-anchor-permalink" href="#Initialize-and-Fit-HMM-with-EM" title="Permalink"></a></h2><p>In a realistic scenario, we would not have access to the latent states; we would only observe the inputs and outputs. We can use the Expectation-Maximization (EM) algorithm to learn the model parameters and infer the hidden states from the observed data alone.</p><p>To demonstrate this process, we start with a randomly initialized GLM-HMM with different parameters than the true model.</p><pre><code class="language-julia hljs">A_init = [0.8 0.2; 0.1 0.9]     # Different transition probabilities
πₖ_init = [0.6; 0.4]            # Different initial distribution

emission_1_init = GaussianRegressionEmission(
    input_dim=3, output_dim=1, include_intercept=true,
    β=reshape([2.0, -1.0, 1.0, 2.0], :, 1),    # Random coefficients
    Σ=[2.0;;], λ=0.0
);

emission_2_init = GaussianRegressionEmission(
    input_dim=3, output_dim=1, include_intercept=true,
    β=reshape([-2.5, -1.0, 3.5, 3.0], :, 1),   # Random coefficients
    Σ=[0.5;;], λ=0.0
);


test_model = HiddenMarkovModel(K=2, A=A_init, πₖ=πₖ_init, B=[emission_1_init, emission_2_init])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">HiddenMarkovModel{Float64, Vector{Float64}, Matrix{Float64}, Vector{GaussianRegressionEmission{Float64, Matrix{Float64}}}}([0.8 0.2; 0.1 0.9], GaussianRegressionEmission{Float64, Matrix{Float64}}[GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [2.0; -1.0; 1.0; 2.0;;], [2.0;;], true, 0.0), GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [-2.5; -1.0; 3.5; 3.0;;], [0.5;;], true, 0.0)], [0.6, 0.4], 2)</code></pre><p>Now that we have definedour test model, we can fit it to the data using the EM algorithm.</p><pre><code class="language-julia hljs">lls = fit!(test_model, data, Φ);

print(&quot;EM converged after $(length(lls)) iterations\n&quot;)
print(&quot;Log-likelihood improved by $(round(lls[end] - lls[1], digits=1))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">EM converged after 7 iterations
Log-likelihood improved by 95384.7</code></pre><p>Plot EM convergence</p><pre><code class="language-julia hljs">p2 = plot(lls, xlabel=&quot;EM Iteration&quot;, ylabel=&quot;Log-Likelihood&quot;,
          title=&quot;Model Convergence&quot;, legend=false, lw=2, color=:darkblue)</code></pre><img src="cf782876.svg" alt="Example block output"/><h2 id="Visualize-Learned-vs-True-Regression-Models"><a class="docs-heading-anchor" href="#Visualize-Learned-vs-True-Regression-Models">Visualize Learned vs True Regression Models</a><a id="Visualize-Learned-vs-True-Regression-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-Learned-vs-True-Regression-Models" title="Permalink"></a></h2><p>Now that we have done parameter learning, we can visualize how well the learned regression models match the true models. We plot the data again, colored by true state, and overlay both the true and learned regression lines. As you can see, the learned models (dashed lines) closely match the true models (solid lines).</p><pre><code class="language-julia hljs">p3 = scatter(Φ[1, :], vec(data);
    color = colors[true_labels],
    ms = 2,
    alpha = 0.3,
    label = &quot;&quot;,
    xlabel = &quot;Input Feature 1&quot;,
    ylabel = &quot;Output&quot;,
    title = &quot;True vs. Learned Regression Models&quot;
)

xvals = range(extrema(Φ[1, :])..., length=100)

plot!(xvals, β1[1] .+ β1[2] .* xvals;
    color = :green, lw = 3, linestyle = :solid, label = &quot;State 1 (true)&quot;
)
plot!(xvals, β2[1] .+ β2[2] .* xvals;
    color = :orange, lw = 3, linestyle = :solid, label = &quot;State 2 (true)&quot;
)

β1_learned = test_model.B[1].β[:, 1]
β2_learned = test_model.B[2].β[:, 1]
plot!(xvals, β1_learned[1] .+ β1_learned[2] .* xvals;
    color = :yellow, lw = 3, linestyle = :dash, label = &quot;State 1 (learned)&quot;
)
plot!(xvals, β2_learned[1] .+ β2_learned[2] .* xvals;
    color = :purple, lw = 3, linestyle = :dash, label = &quot;State 2 (learned)&quot;,
    legend = :topright
)</code></pre><img src="f9fe278c.svg" alt="Example block output"/><h2 id="Hidden-State-Decoding-with-Viterbi-Algorithm"><a class="docs-heading-anchor" href="#Hidden-State-Decoding-with-Viterbi-Algorithm">Hidden State Decoding with Viterbi Algorithm</a><a id="Hidden-State-Decoding-with-Viterbi-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Hidden-State-Decoding-with-Viterbi-Algorithm" title="Permalink"></a></h2><p>Now we use the Viterbi algorithm to find the most likely sequence of hidden states given the observed data. We&#39;ll compare true vs predicted state sequences.</p><pre><code class="language-julia hljs">pred_labels = viterbi(test_model, data, Φ);

accuracy = mean(true_labels .== pred_labels)
print(&quot;Hidden state prediction accuracy: $(round(accuracy*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Hidden state prediction accuracy: 99.9%</code></pre><p>We can also visualize the true and predicted state sequences as heatmaps (first 1000 time points).</p><pre><code class="language-julia hljs">n_display = 1000
true_seq = reshape(true_labels[1:n_display], 1, :)
pred_seq = reshape(pred_labels[1:n_display], 1, :)

p4 = plot(
    heatmap(true_seq, colormap=:roma, title=&quot;True State Sequence&quot;,
           xticks=false, yticks=false, colorbar=false),
    heatmap(pred_seq, colormap=:roma, title=&quot;Predicted State Sequence (Viterbi)&quot;,
           xlabel=&quot;Time Steps (1-$n_display)&quot;, xticks=0:200:n_display,
           yticks=false, colorbar=false),
    layout=(2, 1), size=(800, 300)
)</code></pre><img src="d28c248e.svg" alt="Example block output"/><h2 id="Multiple-Independent-Trials"><a class="docs-heading-anchor" href="#Multiple-Independent-Trials">Multiple Independent Trials</a><a id="Multiple-Independent-Trials-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Independent-Trials" title="Permalink"></a></h2><p>Real-world scenarios often involve multiple independent sequences. Here, we generate multiple trials of synthetic data and show how to fit GLM-HMMs to this data structure using <code>StateSpaceDynamics.jl</code>.</p><pre><code class="language-julia hljs">num_trials = 100   # Number of independent sequences
n_trial = 1000;    # Length of each sequence

print(&quot;Generating $num_trials independent trials of length $n_trial...\n&quot;)

all_data = Vector{Matrix{Float64}}()
Φ_total = Vector{Matrix{Float64}}()
all_true_labels = Vector{Vector{Int64}}()

for i in 1:num_trials  # Generate multiple trials from our ground truth model
    Φ_trial = randn(rng, 3, n_trial)
    true_labels_trial, data_trial = rand(rng, true_model, Φ_trial, n=n_trial)
    push!(all_true_labels, true_labels_trial)
    push!(all_data, data_trial)
    push!(Φ_total, Φ_trial)
end

print(&quot;Total data points: $(num_trials * n_trial)\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generating 100 independent trials of length 1000...
Total data points: 100000</code></pre><h2 id="Fitting-HMM-to-Multiple-Trials"><a class="docs-heading-anchor" href="#Fitting-HMM-to-Multiple-Trials">Fitting HMM to Multiple Trials</a><a id="Fitting-HMM-to-Multiple-Trials-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-HMM-to-Multiple-Trials" title="Permalink"></a></h2><p>When we have multiple independent sequences, EM accounts for each sequence starting fresh from the initial distribution, providing more robust estimates. To fit models of this kind, simply create a test model as before and call <code>fit!</code> with the data and inputs as vectors of the independent sequences! We will use the same random initialization as before.</p><pre><code class="language-julia hljs">test_model_multi = HiddenMarkovModel(
    K=2, A=A_init, πₖ=πₖ_init,
    B=[deepcopy(emission_1_init), deepcopy(emission_2_init)]
)  # Initialize fresh model for multi-trial fitting

lls_multi = fit!(test_model_multi, all_data, Φ_total);

print(&quot;Multi-trial EM converged after $(length(lls_multi)) iterations\n&quot;)
print(&quot;Log-likelihood improved by $(round(lls_multi[end] - lls_multi[1], digits=1))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running EM algorithm...   2%|█                                                 |  ETA: 0:00:51 ( 0.52  s/it)Running EM algorithm... 100%|██████████████████████████████████████████████████| Time: 0:00:01 (17.52 ms/it)
Multi-trial EM converged after 5 iterations
Log-likelihood improved by 637.5</code></pre><p>Plot multi-trial convergence</p><pre><code class="language-julia hljs">p5 = plot(lls_multi, xlabel=&quot;EM Iteration&quot;, ylabel=&quot;Log-Likelihood&quot;,
          title=&quot;Multi-Trial Model Convergence&quot;, legend=false, lw=2, color=:darkgreen)</code></pre><img src="6d1946c7.svg" alt="Example block output"/><h2 id="Multi-Trial-State-Decoding"><a class="docs-heading-anchor" href="#Multi-Trial-State-Decoding">Multi-Trial State Decoding</a><a id="Multi-Trial-State-Decoding-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Trial-State-Decoding" title="Permalink"></a></h2><p>Decode hidden states for all trials and visualize as a multi-trial heatmap.</p><pre><code class="language-julia hljs">all_pred_labels = viterbi(test_model_multi, all_data, Φ_total);</code></pre><p>Calculate overall accuracy across all trials</p><pre><code class="language-julia hljs">all_true_matrix = hcat(all_true_labels...);
all_pred_matrix = hcat(all_pred_labels...);
total_accuracy = mean(all_true_matrix .== all_pred_matrix);

print(&quot;Overall state prediction accuracy: $(round(total_accuracy*100, digits=1))%\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Overall state prediction accuracy: 99.8%</code></pre><p>Visualize subset of trials (first 10 trials, first 500 time points)</p><pre><code class="language-julia hljs">n_trials_display = 10
n_time_display = 500

true_subset = hcat(all_true_labels[1:n_trials_display]...)&#39;[:, 1:n_time_display]
pred_subset = hcat(all_pred_labels[1:n_trials_display]...)&#39;[:, 1:n_time_display]

p6 = plot(
    heatmap(true_subset, colormap=:roma, title=&quot;True States ($n_trials_display trials)&quot;,
           xticks=false, ylabel=&quot;Trial&quot;, colorbar=false),
    heatmap(pred_subset, colormap=:roma, title=&quot;Predicted States (Viterbi)&quot;,
           xlabel=&quot;Time Steps&quot;, ylabel=&quot;Trial&quot;, colorbar=false),
    layout=(2, 1), size=(900, 400)
)</code></pre><img src="697a50d0.svg" alt="Example block output"/><h2 id="Model-Assessment-Summary"><a class="docs-heading-anchor" href="#Model-Assessment-Summary">Model Assessment Summary</a><a id="Model-Assessment-Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Assessment-Summary" title="Permalink"></a></h2><p>Compare learned parameters with true parameters</p><pre><code class="language-julia hljs">true_A_orig = [0.99 0.01; 0.05 0.95]
A_error = norm(true_A_orig - test_model_multi.A) / norm(true_A_orig)
print(&quot;Transition matrix relative error: $(round(A_error, digits=4))\n&quot;)

true_π_orig = [0.8; 0.2]
π_error = norm(true_π_orig - test_model_multi.πₖ) / norm(true_π_orig)
print(&quot;Initial distribution relative error: $(round(π_error, digits=4))\n&quot;)

print(&quot;\nRegression Coefficient Recovery:\n&quot;)
print(&quot;State 1 - True β:    [3.0, 2.0, 2.0, 3.0]\n&quot;)
print(&quot;State 1 - Learned β: $(round.(test_model_multi.B[1].β[:, 1], digits=2))\n&quot;)
print(&quot;State 2 - True β:    [-4.0, -2.0, 3.0, 2.0]\n&quot;)
print(&quot;State 2 - Learned β: $(round.(test_model_multi.B[2].β[:, 1], digits=2))\n&quot;);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Transition matrix relative error: 0.0018
Initial distribution relative error: 0.0171

Regression Coefficient Recovery:
State 1 - True β:    [3.0, 2.0, 2.0, 3.0]
State 1 - Learned β: [3.0, 2.0, 2.0, 3.0]
State 2 - True β:    [-4.0, -2.0, 3.0, 2.0]
State 2 - Learned β: [-4.01, -2.0, 3.0, 2.0]</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This tutorial demonstrated the complete workflow for Hidden Markov Models with regression emissions:</p><p><strong>Key Concepts:</strong></p><ul><li><strong>Discrete latent states</strong> with different regression relationships in each state</li><li><strong>Markovian dynamics</strong> governing state transitions over time</li><li><strong>EM algorithm</strong> for joint parameter learning and state inference</li><li><strong>Viterbi decoding</strong> for finding most likely state sequences</li></ul><p><strong>Applications:</strong></p><ul><li>Modeling switching dynamics and regime changes</li><li>Context-dependent input-output relationships</li><li>Multiple independent trial analysis</li><li>Robust parameter estimation across sequences</li></ul><p>GLM-HMMs provide a powerful framework for modeling data with discrete latent structure, making them valuable for neuroscience, economics, and other domains with switching behaviors.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../hmm_model_selection_example/">« HMM Model Selection</a><a class="docs-footer-nextpage" href="../hmm_identifiability_example/">HMM Identifiability »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Friday 19 September 2025 15:09">Friday 19 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
