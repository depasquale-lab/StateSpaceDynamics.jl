<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gaussian GLM-GMM Example · StateSpaceDynamics.jl</title><meta name="title" content="Gaussian GLM-GMM Example · StateSpaceDynamics.jl"/><meta property="og:title" content="Gaussian GLM-GMM Example · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Gaussian GLM-GMM Example · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../../EmissionModels/">EmissionModels</a></li><li><a class="tocitem" href="../../MixtureModels/">Mixture Models</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../hidden_markov_model_example/">Hidden Markov Model Example</a></li><li class="is-active"><a class="tocitem" href>Gaussian GLM-GMM Example</a><ul class="internal"><li><a class="tocitem" href="#Simulating-and-Fitting-a-Hidden-Markov-Model"><span>Simulating and Fitting a Hidden Markov Model</span></a></li><li><a class="tocitem" href="#Load-Required-Packages"><span>Load Required Packages</span></a></li><li><a class="tocitem" href="#Create-a-Gaussian-Generalized-Linear-Model-Hidden-Markov-Model-(GLM-HMM)"><span>Create a Gaussian Generalized Linear Model-Hidden Markov Model (GLM-HMM)</span></a></li><li><a class="tocitem" href="#Sample-from-the-GLM-HMM"><span>Sample from the GLM-HMM</span></a></li><li><a class="tocitem" href="#Visualize-the-Sampled-Dataset"><span>Visualize the Sampled Dataset</span></a></li><li><a class="tocitem" href="#Initialize-and-Fit-a-New-HMM-to-the-Sampled-Data"><span>Initialize and Fit a New HMM to the Sampled Data</span></a></li><li><a class="tocitem" href="#Visualize-the-Emission-Model-Predictions"><span>Visualize the Emission Model Predictions</span></a></li><li><a class="tocitem" href="#Visualize-the-Latent-State-Predictions-using-Viterbi-Algorithm"><span>Visualize the Latent State Predictions using Viterbi Algorithm</span></a></li><li><a class="tocitem" href="#Sampling-Multiple,-Independent-Trials-of-Data-from-an-HMM"><span>Sampling Multiple, Independent Trials of Data from an HMM</span></a></li><li><a class="tocitem" href="#Fitting-an-HMM-to-Multiple,-Independent-Trials-of-Data"><span>Fitting an HMM to Multiple, Independent Trials of Data</span></a></li><li><a class="tocitem" href="#Visualize-Latent-State-Predictions-for-Multiple-Trials-using-Viterbi"><span>Visualize Latent State Predictions for Multiple Trials using Viterbi</span></a></li><li><a class="tocitem" href="#Summary-and-Model-Assessment"><span>Summary and Model Assessment</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li><a class="tocitem" href="../poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li><a class="tocitem" href="../Probabilistic_PCA_example/">Probabilistic PCA Example</a></li><li><a class="tocitem" href="../switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li><li><a class="tocitem" href="../../Misc/">Miscellaneous</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Gaussian GLM-GMM Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Gaussian GLM-GMM Example</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/examples/Gaussian_GLM_HMM.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h2 id="Simulating-and-Fitting-a-Hidden-Markov-Model"><a class="docs-heading-anchor" href="#Simulating-and-Fitting-a-Hidden-Markov-Model">Simulating and Fitting a Hidden Markov Model</a><a id="Simulating-and-Fitting-a-Hidden-Markov-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-and-Fitting-a-Hidden-Markov-Model" title="Permalink"></a></h2><p>This tutorial demonstrates how to use <code>StateSpaceDynamics.jl</code> to create, sample from, and fit Hidden Markov Models (HMMs). Unlike Linear Dynamical Systems which have continuous latent states, HMMs have discrete latent states that switch between a finite number of modes. This makes them ideal for modeling data with distinct behavioral regimes, switching dynamics, or categorical latent structure.</p><p>We&#39;ll focus on a Gaussian generalized linear model HMM (GLM-HMM), where each hidden state corresponds to a different regression relationship between inputs and outputs. This is particularly useful for modeling data where the input-output relationship changes over time in discrete jumps.</p><h2 id="Load-Required-Packages"><a class="docs-heading-anchor" href="#Load-Required-Packages">Load Required Packages</a><a id="Load-Required-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Required-Packages" title="Permalink"></a></h2><p>We load the essential packages for HMM modeling, visualization, and reproducible random sampling.</p><pre><code class="language-julia hljs">using LinearAlgebra
using Plots
using Random
using StateSpaceDynamics
using StableRNGs
using Statistics: mean</code></pre><p>Set up reproducible random number generation</p><pre><code class="language-julia hljs">rng = StableRNG(1234);</code></pre><h2 id="Create-a-Gaussian-Generalized-Linear-Model-Hidden-Markov-Model-(GLM-HMM)"><a class="docs-heading-anchor" href="#Create-a-Gaussian-Generalized-Linear-Model-Hidden-Markov-Model-(GLM-HMM)">Create a Gaussian Generalized Linear Model-Hidden Markov Model (GLM-HMM)</a><a id="Create-a-Gaussian-Generalized-Linear-Model-Hidden-Markov-Model-(GLM-HMM)-1"></a><a class="docs-heading-anchor-permalink" href="#Create-a-Gaussian-Generalized-Linear-Model-Hidden-Markov-Model-(GLM-HMM)" title="Permalink"></a></h2><p>In a GLM-HMM, each hidden state defines a different regression model. The system switches between these regression models according to Markovian dynamics. This is useful for modeling scenarios where the relationship between predictors and outcomes changes over time.</p><p>Define emission models for each hidden state State 1: Positive relationship between input and output</p><pre><code class="language-julia hljs">emission_1 = GaussianRegressionEmission(
    input_dim=3,                                    # Number of input features
    output_dim=1,                                   # Number of output dimensions
    include_intercept=true,                         # Include intercept term
    β=reshape([3.0, 2.0, 2.0, 3.0], :, 1),        # Regression coefficients [intercept, β1, β2, β3]
    Σ=[1.0;;],                                     # Observation noise variance
    λ=0.0                                          # Regularization parameter
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [3.0; 2.0; 2.0; 3.0;;], [1.0;;], true, 0.0)</code></pre><p>State 2: Different relationship (negative intercept, different slopes)</p><pre><code class="language-julia hljs">emission_2 = GaussianRegressionEmission(
    input_dim=3,
    output_dim=1,
    include_intercept=true,
    β=reshape([-4.0, -2.0, 3.0, 2.0], :, 1),      # Different regression coefficients
    Σ=[1.0;;],                                     # Same noise level
    λ=0.0
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [-4.0; -2.0; 3.0; 2.0;;], [1.0;;], true, 0.0)</code></pre><p>Define the state transition matrix A A[i,j] = probability of transitioning from state i to state j Diagonal elements are high (states are persistent), off-diagonal elements are low</p><pre><code class="language-julia hljs">A = [0.99 0.01;    # From state 1: 99% stay, 1% switch to state 2
     0.05 0.95]    # From state 2: 5% switch to state 1, 95% stay</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Float64}:
 0.99  0.01
 0.05  0.95</code></pre><p>Initial state distribution: probability of starting in each state</p><pre><code class="language-julia hljs">πₖ = [0.8; 0.2]    # 80% chance of starting in state 1, 20% in state 2</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.8
 0.2</code></pre><p>Construct the complete HMM</p><pre><code class="language-julia hljs">true_model = HiddenMarkovModel(
    K=2,                        # Number of hidden states
    A=A,                        # Transition matrix
    πₖ=πₖ,                     # Initial state distribution
    B=[emission_1, emission_2]  # Emission models for each state
)

println(&quot;Created GLM-HMM with 2 states and 3 input features&quot;)
println(&quot;State 1 regression: y = 3.0 + 2.0*x₁ + 2.0*x₂ + 3.0*x₃ + ε&quot;)
println(&quot;State 2 regression: y = -4.0 - 2.0*x₁ + 3.0*x₂ + 2.0*x₃ + ε&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Created GLM-HMM with 2 states and 3 input features
State 1 regression: y = 3.0 + 2.0*x₁ + 2.0*x₂ + 3.0*x₃ + ε
State 2 regression: y = -4.0 - 2.0*x₁ + 3.0*x₂ + 2.0*x₃ + ε</code></pre><h2 id="Sample-from-the-GLM-HMM"><a class="docs-heading-anchor" href="#Sample-from-the-GLM-HMM">Sample from the GLM-HMM</a><a id="Sample-from-the-GLM-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-from-the-GLM-HMM" title="Permalink"></a></h2><p>Generate synthetic data from our true model. This will give us both the observed data (inputs and outputs) and the true hidden state sequence, which we&#39;ll use to evaluate our parameter recovery.</p><pre><code class="language-julia hljs">n = 20000  # Number of time points
println(&quot;Generating $n samples from the GLM-HMM...&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generating 20000 samples from the GLM-HMM...</code></pre><p>Generate random input features (predictors)</p><pre><code class="language-julia hljs">Φ = randn(rng, 3, n)  # 3 features × n time points</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3×20000 Matrix{Float64}:
  0.519495  -1.29473    0.0922364  …  -0.575481  -0.912891  0.0479683
  0.90514   -0.684365  -0.0896371      0.947068  -0.763815  1.6591
 -1.67566   -0.686151  -1.4521         0.66005    1.08251   0.996231</code></pre><p>Sample from the HMM: returns both hidden states and observations</p><pre><code class="language-julia hljs">true_labels, data = rand(rng, true_model, Φ, n=n)

println(&quot;Generated data summary:&quot;)
println(&quot;  - Input features shape: $(size(Φ))&quot;)
println(&quot;  - Output data shape: $(size(data))&quot;)
println(&quot;  - True labels shape: $(size(true_labels))&quot;)
println(&quot;  - State 1 proportion: $(round(mean(true_labels .== 1), digits=3))&quot;)
println(&quot;  - State 2 proportion: $(round(mean(true_labels .== 2), digits=3))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generated data summary:
  - Input features shape: (3, 20000)
  - Output data shape: (1, 20000)
  - True labels shape: (20000,)
  - State 1 proportion: 0.834
  - State 2 proportion: 0.166</code></pre><h2 id="Visualize-the-Sampled-Dataset"><a class="docs-heading-anchor" href="#Visualize-the-Sampled-Dataset">Visualize the Sampled Dataset</a><a id="Visualize-the-Sampled-Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-the-Sampled-Dataset" title="Permalink"></a></h2><p>Create a scatter plot showing how the input-output relationship differs between the two hidden states. Points are colored by their true hidden state.</p><pre><code class="language-julia hljs">colors = [:dodgerblue, :crimson]  # Blue for state 1, red for state 2

scatter(Φ[1, :], vec(data);
    color = colors[true_labels],
    ms = 3,
    label = &quot;&quot;,
    xlabel = &quot;Input Feature 1&quot;,
    ylabel = &quot;Output&quot;,
    title = &quot;GLM-HMM Sampled Data (colored by true state)&quot;,
    alpha = 0.6
)</code></pre><img src="2be7c380.svg" alt="Example block output"/><p>Overlay the true regression lines for each state We&#39;ll plot the relationship between feature 1 and output, holding other features at 0</p><pre><code class="language-julia hljs">xvals = range(minimum(Φ[1, :]), stop=maximum(Φ[1, :]), length=100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-4.078462862649889:0.08274389598600555:4.113182839964661</code></pre><p>State 1 regression line: y = β₀ + β₁*x₁ (setting x₂=x₃=0)</p><pre><code class="language-julia hljs">β1 = emission_1.β[:, 1]
y_pred_1 = β1[1] .+ β1[2] .* xvals  # intercept + slope*x₁
plot!(xvals, y_pred_1;
    color = :dodgerblue,
    lw = 3,
    label = &quot;State 1 regression&quot;,
    legend = :topright,
)</code></pre><img src="c0e2b6dd.svg" alt="Example block output"/><p>State 2 regression line</p><pre><code class="language-julia hljs">β2 = emission_2.β[:, 1]
y_pred_2 = β2[1] .+ β2[2] .* xvals  # intercept + slope*x₁
plot!(xvals, y_pred_2;
    color = :crimson,
    lw = 3,
    label = &quot;State 2 regression&quot;,
    legend = :topright,
)</code></pre><img src="d16b2efe.svg" alt="Example block output"/><h2 id="Initialize-and-Fit-a-New-HMM-to-the-Sampled-Data"><a class="docs-heading-anchor" href="#Initialize-and-Fit-a-New-HMM-to-the-Sampled-Data">Initialize and Fit a New HMM to the Sampled Data</a><a id="Initialize-and-Fit-a-New-HMM-to-the-Sampled-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Initialize-and-Fit-a-New-HMM-to-the-Sampled-Data" title="Permalink"></a></h2><p>Now we&#39;ll pretend we don&#39;t know the true parameters and try to learn them from the observed data alone. We start with a randomly initialized HMM and use the Expectation-Maximization (EM) algorithm to learn the parameters.</p><pre><code class="language-julia hljs">println(&quot;Initializing naive HMM with random parameters...&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Initializing naive HMM with random parameters...</code></pre><p>Initialize with different parameters than the true model</p><pre><code class="language-julia hljs">A = [0.8 0.2; 0.1 0.9]          # Different transition probabilities
πₖ = [0.6; 0.4]                 # Different initial distribution</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.6
 0.4</code></pre><p>Initialize emission models with random regression coefficients</p><pre><code class="language-julia hljs">emission_1 = GaussianRegressionEmission(
    input_dim=3, output_dim=1, include_intercept=true,
    β=reshape([2.0, -1.0, 1.0, 2.0], :, 1),    # Random coefficients
    Σ=[2.0;;],                                  # Different noise variance
    λ=0.0
)

emission_2 = GaussianRegressionEmission(
    input_dim=3, output_dim=1, include_intercept=true,
    β=reshape([-2.5, -1.0, 3.5, 3.0], :, 1),   # Random coefficients
    Σ=[0.5;;],                                  # Different noise variance
    λ=0.0
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [-2.5; -1.0; 3.5; 3.0;;], [0.5;;], true, 0.0)</code></pre><p>Create the test model with naive initialization</p><pre><code class="language-julia hljs">test_model = HiddenMarkovModel(K=2, A=A, πₖ=πₖ, B=[emission_1, emission_2])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">HiddenMarkovModel{Float64, Vector{Float64}, Matrix{Float64}, Vector{GaussianRegressionEmission{Float64, Matrix{Float64}}}}([0.8 0.2; 0.1 0.9], GaussianRegressionEmission{Float64, Matrix{Float64}}[GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [2.0; -1.0; 1.0; 2.0;;], [2.0;;], true, 0.0), GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [-2.5; -1.0; 3.5; 3.0;;], [0.5;;], true, 0.0)], [0.6, 0.4], 2)</code></pre><p>Fit the model using EM algorithm</p><pre><code class="language-julia hljs">println(&quot;Running EM algorithm to learn HMM parameters...&quot;)
lls = fit!(test_model, data, Φ)

println(&quot;EM converged after $(length(lls)) iterations&quot;)
println(&quot;Initial log-likelihood: $(round(lls[1], digits=2))&quot;)
println(&quot;Final log-likelihood: $(round(lls[end], digits=2))&quot;)
println(&quot;Improvement: $(round(lls[end] - lls[1], digits=2))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running EM algorithm to learn HMM parameters...
EM converged after 7 iterations
Initial log-likelihood: -106652.07
Final log-likelihood: -11267.32
Improvement: 95384.75</code></pre><p>Plot the convergence of the log-likelihood</p><pre><code class="language-julia hljs">plot(lls)
title!(&quot;Log-likelihood over EM Iterations&quot;)
xlabel!(&quot;EM Iteration&quot;)
ylabel!(&quot;Log-Likelihood&quot;)</code></pre><img src="a45b91be.svg" alt="Example block output"/><h2 id="Visualize-the-Emission-Model-Predictions"><a class="docs-heading-anchor" href="#Visualize-the-Emission-Model-Predictions">Visualize the Emission Model Predictions</a><a id="Visualize-the-Emission-Model-Predictions-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-the-Emission-Model-Predictions" title="Permalink"></a></h2><p>Compare the true regression relationships with what our fitted model learned. This shows how well we recovered the underlying GLM parameters.</p><pre><code class="language-julia hljs">state_colors = [:dodgerblue, :crimson]  # Data points colored by true state
true_colors = [:green, :orange]         # True regression lines
pred_colors = [:teal, :yellow]          # Predicted regression lines

scatter(Φ[1, :], vec(data);
    color = state_colors[true_labels],
    ms = 3,
    alpha = 0.6,
    label = &quot;&quot;,
    xlabel = &quot;Input Feature 1&quot;,
    ylabel = &quot;Output&quot;,
    title = &quot;True vs. Predicted Regression Relationships&quot;
)

xvals = range(minimum(Φ[1, :]), stop=maximum(Φ[1, :]), length=100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-4.078462862649889:0.08274389598600555:4.113182839964661</code></pre><p>Plot true regression lines</p><pre><code class="language-julia hljs">β1_true = emission_1.β[:, 1]  # Note: this is now the fitted model&#39;s β, not true model&#39;s
y_true_1 = β1_true[1] .+ β1_true[2] .* xvals
plot!(xvals, y_true_1;
    color = true_colors[1],
    lw = 3,
    linestyle = :solid,
    label = &quot;State 1 (true)&quot;
)

β2_true = emission_2.β[:, 1]
y_true_2 = β2_true[1] .+ β2_true[2] .* xvals
plot!(xvals, y_true_2;
    color = true_colors[2],
    lw = 3,
    linestyle = :solid,
    label = &quot;State 2 (true)&quot;
)</code></pre><img src="af22c10c.svg" alt="Example block output"/><p>Plot learned regression lines</p><pre><code class="language-julia hljs">β1_pred = test_model.B[1].β[:, 1]
y_pred_1 = β1_pred[1] .+ β1_pred[2] .* xvals
plot!(xvals, y_pred_1;
    color = pred_colors[1],
    lw = 3,
    linestyle = :dash,
    label = &quot;State 1 (learned)&quot;
)

β2_pred = test_model.B[2].β[:, 1]
y_pred_2 = β2_pred[1] .+ β2_pred[2] .* xvals
plot!(xvals, y_pred_2;
    color = pred_colors[2],
    lw = 3,
    linestyle = :dash,
    label = &quot;State 2 (learned)&quot;
)</code></pre><img src="a1f0d5b5.svg" alt="Example block output"/><h2 id="Visualize-the-Latent-State-Predictions-using-Viterbi-Algorithm"><a class="docs-heading-anchor" href="#Visualize-the-Latent-State-Predictions-using-Viterbi-Algorithm">Visualize the Latent State Predictions using Viterbi Algorithm</a><a id="Visualize-the-Latent-State-Predictions-using-Viterbi-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-the-Latent-State-Predictions-using-Viterbi-Algorithm" title="Permalink"></a></h2><p>The Viterbi algorithm finds the most likely sequence of hidden states given the observed data. We&#39;ll compare the true hidden state sequence with our predictions.</p><pre><code class="language-julia hljs">println(&quot;Running Viterbi algorithm to decode hidden state sequence...&quot;)
pred_labels = viterbi(test_model, data, Φ);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running Viterbi algorithm to decode hidden state sequence...</code></pre><p>Calculate accuracy of state prediction</p><pre><code class="language-julia hljs">accuracy = mean(true_labels .== pred_labels)
println(&quot;Hidden state prediction accuracy: $(round(accuracy*100, digits=1))%&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Hidden state prediction accuracy: 99.9%</code></pre><p>Visualize a subset of the state sequences as heatmaps</p><pre><code class="language-julia hljs">true_mat = reshape(true_labels[1:1000], 1, :)
pred_mat = reshape(pred_labels[1:1000], 1, :)

p1 = heatmap(true_mat;
    colormap = :roma50,
    title = &quot;True State Labels&quot;,
    xlabel = &quot;&quot;,
    ylabel = &quot;&quot;,
    xticks = false,
    yticks = false,
    colorbar = false,
    framestyle = :box)

p2 = heatmap(pred_mat;
    colormap = :roma50,
    title = &quot;Predicted State Labels (Viterbi)&quot;,
    xlabel = &quot;Timepoints (1-1000)&quot;,
    ylabel = &quot;&quot;,
    xticks = 0:200:1000,
    yticks = false,
    colorbar = false,
    framestyle = :box)

plot(p1, p2;
    layout = (2, 1),
    size = (700, 500),
    margin = 5Plots.mm)</code></pre><img src="48c4a8ab.svg" alt="Example block output"/><h2 id="Sampling-Multiple,-Independent-Trials-of-Data-from-an-HMM"><a class="docs-heading-anchor" href="#Sampling-Multiple,-Independent-Trials-of-Data-from-an-HMM">Sampling Multiple, Independent Trials of Data from an HMM</a><a id="Sampling-Multiple,-Independent-Trials-of-Data-from-an-HMM-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-Multiple,-Independent-Trials-of-Data-from-an-HMM" title="Permalink"></a></h2><p>Real-world scenarios often involve multiple independent sequences (e.g., multiple subjects, experimental sessions, or trials). We&#39;ll generate multiple independent sequences and show how to fit HMMs to this type of data structure.</p><pre><code class="language-julia hljs">println(&quot;Generating multiple independent trials...&quot;)

all_data = Vector{Matrix{Float64}}()     # Store data from each trial
Φ_total = Vector{Matrix{Float64}}()      # Store input features from each trial
all_true_labels = []                     # Store true state sequences

num_trials = 100  # Number of independent sequences
n = 1000         # Length of each sequence

for i in 1:num_trials
    Φ = randn(rng, 3, n)
    true_labels, data = rand(rng, true_model, Φ, n=n)
    push!(all_true_labels, true_labels)
    push!(all_data, data)
    push!(Φ_total, Φ)
end

println(&quot;Generated $num_trials independent trials, each with $n time points&quot;)
println(&quot;Total data points: $(num_trials * n)&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Generating multiple independent trials...
┌ Warning: Assignment to `Φ` in soft scope is ambiguous because a global variable by the same name exists: `Φ` will be treated as a new local. Disambiguate by using `local Φ` to suppress this warning or `global Φ` to assign to the existing global variable.
└ @ gaussian_glm_hmm_example.md:377
┌ Warning: Assignment to `true_labels` in soft scope is ambiguous because a global variable by the same name exists: `true_labels` will be treated as a new local. Disambiguate by using `local true_labels` to suppress this warning or `global true_labels` to assign to the existing global variable.
└ @ gaussian_glm_hmm_example.md:378
┌ Warning: Assignment to `data` in soft scope is ambiguous because a global variable by the same name exists: `data` will be treated as a new local. Disambiguate by using `local data` to suppress this warning or `global data` to assign to the existing global variable.
└ @ gaussian_glm_hmm_example.md:378
Generated 100 independent trials, each with 1000 time points
Total data points: 100000</code></pre><h2 id="Fitting-an-HMM-to-Multiple,-Independent-Trials-of-Data"><a class="docs-heading-anchor" href="#Fitting-an-HMM-to-Multiple,-Independent-Trials-of-Data">Fitting an HMM to Multiple, Independent Trials of Data</a><a id="Fitting-an-HMM-to-Multiple,-Independent-Trials-of-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-an-HMM-to-Multiple,-Independent-Trials-of-Data" title="Permalink"></a></h2><p>When we have multiple independent sequences, the EM algorithm needs to account for the fact that each sequence starts fresh from the initial state distribution. This provides more robust parameter estimates than fitting to a single long sequence.</p><pre><code class="language-julia hljs">println(&quot;Fitting HMM to multiple independent trials...&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Fitting HMM to multiple independent trials...</code></pre><p>Initialize a new model for multi-trial fitting</p><pre><code class="language-julia hljs">A = [0.8 0.2; 0.1 0.9]
πₖ = [0.6; 0.4]
emission_1 = GaussianRegressionEmission(
    input_dim=3, output_dim=1, include_intercept=true,
    β=reshape([2.0, -1.0, 1.0, 2.0], :, 1), Σ=[2.0;;], λ=0.0
)
emission_2 = GaussianRegressionEmission(
    input_dim=3, output_dim=1, include_intercept=true,
    β=reshape([-2.5, -1.0, 3.5, 3.0], :, 1), Σ=[0.5;;], λ=0.0
)

test_model = HiddenMarkovModel(K=2, A=A, πₖ=πₖ, B=[emission_1, emission_2])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">HiddenMarkovModel{Float64, Vector{Float64}, Matrix{Float64}, Vector{GaussianRegressionEmission{Float64, Matrix{Float64}}}}([0.8 0.2; 0.1 0.9], GaussianRegressionEmission{Float64, Matrix{Float64}}[GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [2.0; -1.0; 1.0; 2.0;;], [2.0;;], true, 0.0), GaussianRegressionEmission{Float64, Matrix{Float64}}(3, 1, [-2.5; -1.0; 3.5; 3.0;;], [0.5;;], true, 0.0)], [0.6, 0.4], 2)</code></pre><p>Fit to multiple trials - the package handles the multi-trial structure automatically</p><pre><code class="language-julia hljs">lls = fit!(test_model, all_data, Φ_total)

println(&quot;Multi-trial EM converged after $(length(lls)) iterations&quot;)
println(&quot;Initial log-likelihood: $(round(lls[1], digits=2))&quot;)
println(&quot;Final log-likelihood: $(round(lls[end], digits=2))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Running EM algorithm...   2%|█                                                 |  ETA: 0:00:49 ( 0.50  s/it)Running EM algorithm... 100%|██████████████████████████████████████████████████| Time: 0:00:02 (20.88 ms/it)
Multi-trial EM converged after 8 iterations
Initial log-likelihood: -531913.8
Final log-likelihood: -57602.92</code></pre><p>Plot convergence for multi-trial fitting</p><pre><code class="language-julia hljs">plot(lls)
title!(&quot;Log-likelihood over EM Iterations (Multi-Trial)&quot;)
xlabel!(&quot;EM Iteration&quot;)
ylabel!(&quot;Log-Likelihood&quot;)</code></pre><img src="55efd981.svg" alt="Example block output"/><h2 id="Visualize-Latent-State-Predictions-for-Multiple-Trials-using-Viterbi"><a class="docs-heading-anchor" href="#Visualize-Latent-State-Predictions-for-Multiple-Trials-using-Viterbi">Visualize Latent State Predictions for Multiple Trials using Viterbi</a><a id="Visualize-Latent-State-Predictions-for-Multiple-Trials-using-Viterbi-1"></a><a class="docs-heading-anchor-permalink" href="#Visualize-Latent-State-Predictions-for-Multiple-Trials-using-Viterbi" title="Permalink"></a></h2><p>Decode hidden states for all trials and visualize the results as a multi-trial heatmap. This shows how well we can predict state sequences across different independent runs.</p><pre><code class="language-julia hljs">println(&quot;Running Viterbi decoding on all trials...&quot;)
all_pred_labels_vec = viterbi(test_model, all_data, Φ_total)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">100-element Vector{Vector{Int64}}:
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  2, 2, 2, 2, 2, 2, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 ⋮
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 2  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
 [2, 2, 2, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</code></pre><p>Reshape for visualization</p><pre><code class="language-julia hljs">all_pred_labels = hcat(all_pred_labels_vec...)&#39;      # trials × time
all_true_labels_matrix = hcat(all_true_labels...)&#39;   # trials × time</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">100×1000 adjoint(::Matrix{Int64}) with eltype Int64:
 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 2  2  2  2  2  2  2  2  2  2  2  2  2     2  2  2  2  2  2  2  2  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 2  2  2  2  2  2  2  2  2  2  2  2  2  …  1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 2  2  2  2  2  2  2  2  2  2  2  2  2     1  1  1  1  1  1  1  1  1  1  1  1
 1  2  2  2  2  2  2  2  2  2  2  2  2     1  1  1  1  1  1  1  1  1  1  1  1
 ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  2  2  2  2  …  1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1
 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  1  1  1  1  1  1  1  1  1  1
 2  2  2  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1</code></pre><p>Calculate overall accuracy across all trials</p><pre><code class="language-julia hljs">total_accuracy = mean(all_true_labels_matrix .== all_pred_labels)
println(&quot;Overall hidden state accuracy across all trials: $(round(total_accuracy*100, digits=1))%&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Overall hidden state accuracy across all trials: 99.8%</code></pre><p>Visualize a subset of trials</p><pre><code class="language-julia hljs">state_colors = [:dodgerblue, :crimson]
true_subset = all_true_labels_matrix[1:10, 1:500]   # First 10 trials, first 500 time points
pred_subset = all_pred_labels[1:10, 1:500]

p1 = heatmap(
    true_subset,
    colormap = :roma50,
    colorbar = false,
    title = &quot;True State Labels (10 trials × 500 timepoints)&quot;,
    xlabel = &quot;&quot;,
    ylabel = &quot;Trial Number&quot;,
    xticks = false,
    yticks = true,
    margin = 5Plots.mm,
    legend = false
)

p2 = heatmap(
    pred_subset,
    colormap = :roma50,
    colorbar = false,
    title = &quot;Predicted State Labels (Viterbi)&quot;,
    xlabel = &quot;Timepoints&quot;,
    ylabel = &quot;Trial Number&quot;,
    xticks = true,
    yticks = true,
    margin = 5Plots.mm,
    legend = false
)

final_plot = plot(
    p1, p2,
    layout = (2, 1),
    size = (850, 550),
    margin = 5Plots.mm,
    legend = false,
)

display(final_plot)</code></pre><h2 id="Summary-and-Model-Assessment"><a class="docs-heading-anchor" href="#Summary-and-Model-Assessment">Summary and Model Assessment</a><a id="Summary-and-Model-Assessment-1"></a><a class="docs-heading-anchor-permalink" href="#Summary-and-Model-Assessment" title="Permalink"></a></h2><pre><code class="language-julia hljs">println(&quot;\n=== Final Model Assessment ===&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">
=== Final Model Assessment ===</code></pre><p>Compare learned parameters with true parameters</p><pre><code class="language-julia hljs">true_A = [0.99 0.01; 0.05 0.95]
learned_A = test_model.A
A_error = norm(true_A - learned_A) / norm(true_A)
println(&quot;Transition matrix relative error: $(round(A_error, digits=4))&quot;)

true_π = [0.8; 0.2]
learned_π = test_model.πₖ
π_error = norm(true_π - learned_π) / norm(true_π)
println(&quot;Initial distribution relative error: $(round(π_error, digits=4))&quot;)

println(&quot;\nTrue vs Learned Regression Coefficients:&quot;)
println(&quot;State 1 - True β: [3.0, 2.0, 2.0, 3.0]&quot;)
println(&quot;State 1 - Learned β: $(round.(test_model.B[1].β[:, 1], digits=2))&quot;)
println(&quot;State 2 - True β: [-4.0, -2.0, 3.0, 2.0]&quot;)
println(&quot;State 2 - Learned β: $(round.(test_model.B[2].β[:, 1], digits=2))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Transition matrix relative error: 0.0018
Initial distribution relative error: 0.0171

True vs Learned Regression Coefficients:
State 1 - True β: [3.0, 2.0, 2.0, 3.0]
State 1 - Learned β: [3.0, 2.0, 2.0, 3.0]
State 2 - True β: [-4.0, -2.0, 3.0, 2.0]
State 2 - Learned β: [-4.01, -2.0, 3.0, 2.0]</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This tutorial demonstrated the complete workflow for Hidden Markov Models with regression emissions:</p><ol><li><strong>Model Structure</strong>: Discrete latent states with different regression relationships in each state</li><li><strong>Applications</strong>: Ideal for modeling switching dynamics, regime changes, or context-dependent relationships</li><li><strong>Single vs Multiple Trials</strong>: Showed how to handle both single long sequences and multiple independent trials</li><li><strong>Parameter Recovery</strong>: EM algorithm successfully learned transition dynamics and emission parameters</li><li><strong>State Decoding</strong>: Viterbi algorithm accurately recovered hidden state sequences</li><li><strong>Scalability</strong>: Framework handles multiple trials efficiently for robust parameter estimation</li></ol><p>GLM-HMMs provide a powerful framework for modeling data with discrete latent structure and context-dependent input-output relationships, making them valuable for many real-world applications.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../hidden_markov_model_example/">« Hidden Markov Model Example</a><a class="docs-footer-nextpage" href="../gaussian_mixture_model_example/">Gaussian Mixture Model Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 9 September 2025 00:23">Tuesday 9 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
