var documenterSearchIndex = {"docs":
[{"location":"tutorials/poisson_mixture_model_example/#Simulating-and-Fitting-a-Poisson-Mixture-Model","page":"Poisson Mixture Model Example","title":"Simulating and Fitting a Poisson Mixture Model","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"This tutorial shows how to build and fit a Poisson Mixture Model (PMM) with StateSpaceDynamics.jl using the Expectation–Maximization (EM) algorithm. It starts from simulation, fits the model, and then walks through diagnostics, interpretation, and common extensions.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#What-is-a-Poisson-Mixture-Model?","page":"Poisson Mixture Model Example","title":"What is a Poisson Mixture Model?","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"A PMM assumes each observation (xi\\in {0,1,2,\\dots}) is drawn from one of (k) Poisson distributions, with means (rates) (\\lambda1,\\dots,\\lambdak). The component is a latent categorical variable (zi\\in{1,\\dots,k}) with mixing weights (\\pi1,\\dots,\\pik) ((\\sumj \\pij = 1)). The generative process:","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Draw (z_i \\sim \\text{Categorical}(\\pi))\nGiven (zi=j), draw (xi \\sim \\text{Poisson}(\\lambda_j))","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"PMMs are handy for count data that come from a few heterogeneous sub-populations (e.g., spike counts from cells with different firing rates, click counts from multiple user segments, etc.).","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#EM-in-one-paragraph","page":"Poisson Mixture Model Example","title":"EM in one paragraph","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"EM maximizes the marginal log-likelihood (\\log p(x\\,|\\,\\pi,\\lambda)) by iterating:","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"E-step: compute responsibilities (\\gamma{ij} = p(zi=j\\,|\\,x_i,\\theta))\nM-step: update (\\pij) and (\\lambdaj) to maximize the expected complete-data log-likelihood.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"For Poisson mixtures, the closed-form M-step is: [ \\pij \\leftarrow \\frac{1}{n} \\sumi \\gamma{ij}, \\qquad \\lambdaj \\leftarrow \\frac{\\sumi \\gamma{ij} xi}{\\sumi \\gamma_{ij}}. ] fit! in StateSpaceDynamics.jl performs these steps under the hood.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing StableRNGs\nusing StatsPlots\nusing Distributions","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"We fix the RNG for reproducibility of simulated data and k-means seeding.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Create-a-true-PoissonMixtureModel-to-simulate-from","page":"Poisson Mixture Model Example","title":"Create a true PoissonMixtureModel to simulate from","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"We'll simulate from a mixture of k=3 Poisson components with distinct means and mixing weights. Feel free to change these and observe how the fitted model behaves.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"k = 3\ntrue_λs = [5.0, 10.0, 25.0]   # Poisson means (rates) per component\ntrue_πs = [0.25, 0.45, 0.30]  # Mixing weights (must sum to 1)\n\ntrue_pmm = PoissonMixtureModel(k, true_λs, true_πs)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Generate-data-from-the-true-model","page":"Poisson Mixture Model Example","title":"Generate data from the true model","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"We'll draw n IID samples. labels are the latent component indices used for simulation; in practice these are unknown and must be inferred.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"n = 500\nlabels = rand(rng, Categorical(true_πs), n)\ndata   = [rand(rng, Poisson(true_λs[labels[i]])) for i in 1:n]  # Vector{Int}","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Quick-look:-histogram-of-Poisson-samples-by-component","page":"Poisson Mixture Model Example","title":"Quick look: histogram of Poisson samples by component","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"In real applications we don't see the true labels, but plotting them here helps build intuition: components with larger λ shift mass to larger counts.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"p1 = histogram(\n    data;\n    group=labels,\n    bins=0:1:maximum(data),\n    bar_position=:dodge,\n    xlabel=\"Count\",\n    ylabel=\"Frequency\",\n    title=\"Poisson Mixture Samples by Component\",\n    alpha=0.7,\n    legend=:topright,\n)\np1","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Fit-a-new-PoissonMixtureModel-to-the-data","page":"Poisson Mixture Model Example","title":"Fit a new PoissonMixtureModel to the data","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"We construct an empty model with k components and call fit!. Options:","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"maxiter: EM iterations cap\ntol: stop if relative improvement in log-likelihood is smaller than this\ninitialize_kmeans=true: seed with k-means on the 1D counts for more stable starts","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Note: EM is sensitive to initialization; try toggling initialize_kmeans or running from multiple random starts when you care about global optima.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"fit_pmm = PoissonMixtureModel(k)\n_, lls = fit!(fit_pmm, data; maxiter=100, tol=1e-6, initialize_kmeans=true)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Convergence-diagnostics","page":"Poisson Mixture Model Example","title":"Convergence diagnostics","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"EM guarantees non-decreasing log-likelihood. Monotone ascent is a good basic check.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"p2 = plot(\n    lls;\n    xlabel=\"Iteration\",\n    ylabel=\"Log-Likelihood\",\n    title=\"EM Convergence (Poisson Mixture)\",\n    marker=:circle,\n    label=\"log_likelihood\",\n)\np2","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Visual-model-check:-mixture-PMFs-vs-data","page":"Poisson Mixture Model Example","title":"Visual model check: mixture PMFs vs data","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Overlay the fitted component probability mass functions (PMFs) and the overall mixture PMF on the normalized histogram. Components should plausibly explain the major modes and tail behavior in the data.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"p3 = histogram(\n    data;\n    bins=0:1:maximum(data),\n    normalize=true,\n    alpha=0.3,\n    label=\"Data\",\n    xlabel=\"Count\",\n    ylabel=\"Density\",\n    title=\"Poisson Mixtures: Data and PMFs\",\n)\n\nx = collect(0:maximum(data))\ncolors = [:red, :green, :blue]\n\nfor i in 1:k\n    λi = fit_pmm.λₖ[i]\n    πi = fit_pmm.πₖ[i]\n    pmf_i = πi .* pdf.(Poisson(λi), x)\n    plot!(\n        p3, x, pmf_i;\n        lw=2,\n        c=colors[i],\n        label=\"Comp $i (λ=$(round(λi, sigdigits=3)))\",\n    )\nend\n\nmix_pmf = reduce(+, (πi .* pdf.(Poisson(λi), x) for (λi, πi) in zip(fit_pmm.λₖ, fit_pmm.πₖ)))\nplot!(\n    p3, x, mix_pmf;\n    lw=3, ls=:dash, c=:black,\n    label=\"Mixture\",\n)\n\np3","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Interpreting-the-fitted-parameters","page":"Poisson Mixture Model Example","title":"Interpreting the fitted parameters","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"fit_pmm.λₖ are the estimated Poisson rates per component.\nfit_pmm.πₖ are the estimated mixing weights (sum to 1).","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Larger λ means the component puts more mass on larger counts. If two λs are close, EM may swap their order from run to run (label switching); the mixture distribution is unchanged.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Posterior-responsibilities-(soft-clustering)","page":"Poisson Mixture Model Example","title":"Posterior responsibilities (soft clustering)","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Responsibilities (\\gamma{ij}=p(zi=j\\,|\\,x_i,\\hat\\theta)) quantify how likely each point belongs to each component. These are useful for soft assignments, uncertainty-aware summaries, and inspecting ambiguous points.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"function responsibilities_pmm(λs::AbstractVector, πs::AbstractVector, x::AbstractVector)\n    k = length(λs)\n    n = length(x)\n    Γ = zeros(n, k)\n    for i in 1:n\n        for j in 1:k\n            Γ[i, j] = πs[j] * pdf(Poisson(λs[j]), x[i])\n        end\n        s = sum(Γ[i, :])\n        Γ[i, :] ./= s > 0 ? s : 1.0\n    end\n    return Γ\nend\n\nΓ = responsibilities_pmm(fit_pmm.λₖ, fit_pmm.πₖ, data)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Hard labels (if you need them) are argmax over responsibilities.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"hard_labels = map(i -> argmax(view(Γ, i, :)), 1:n)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Information-criteria:-picking-k","page":"Poisson Mixture Model Example","title":"Information criteria: picking k","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"If you don't know k, you can fit several models and compare AIC/BIC:   AIC = 2p - 2LL,   BIC = p*log(n) - 2LL with parameter count p = (k-1) mixing weights + k rates = 2k-1.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"function aic_bic(lls::AbstractVector, n::Int, k::Int)\n    ll = last(lls)\n    p = 2k - 1\n    return (AIC = 2p - 2ll, BIC = p*log(n) - 2ll)\nend\n\n(ic_aic, ic_bic) = aic_bic(lls, n, k), aic_bic(lls, n, k)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"(In practice, compute these for multiple k and choose the one with the smallest AIC/BIC, balancing parsimony and fit.)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Practical-tips-and-pitfalls","page":"Poisson Mixture Model Example","title":"Practical tips & pitfalls","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Initialization matters. Try multiple random starts or k-means seeding.\nLabel switching. Component indices are arbitrary; sort components by λ for stable presentation if needed.\nSmall/empty components. If some π_j \\approx 0, consider reducing k or adding a tiny ridge prior on λ to avoid degenerate updates.\nZero-inflation / overdispersion. If data have excess zeros or variance >> mean, consider a Negative Binomial mixture or a zero-inflated Poisson.\nTrain/validation split. Use held-out likelihood or posterior predictive checks for model assessment beyond AIC/BIC.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Next-steps-(ideas-for-exercises)","page":"Poisson Mixture Model Example","title":"Next steps (ideas for exercises)","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"Unknown k: loop over k=1:6, fit each, compare AIC/BIC.\nStress test: change true λs to be closer (e.g., [10, 12, 14]) and see how EM behaves and how responsibilities reflect ambiguity.\nPosterior predictive check: simulate new data from the fitted mixture and compare histograms / tail behavior.\nZero-inflation: inject extra zeros and try a more flexible mixture class.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Simulating-and-Fitting-a-Switching-Linear-Dynamical-System-(SLDS)","page":"Switching Linear Dynamical System Example","title":"Simulating and Fitting a Switching Linear Dynamical System (SLDS)","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"This tutorial walks through building, simulating, and fitting a Switching Linear Dynamical System (SLDS) with StateSpaceDynamics.jl. SLDS combines a discrete Hidden Markov Model (HMM) over modes with a set of linear-Gaussian state-space models (one per mode). It captures time series that switch among distinct linear dynamics (e.g., slow vs. fast oscillators).","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Model-overview","page":"Switching Linear Dynamical System Example","title":"Model overview","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"The SLDS has:","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Discrete mode (st \\in {1,\\dots,K}) with Markov dynamics (p(st\\mid s{t-1}) = A{\\text{hmm}}[s{t-1}, st]), initial (\\pi_0).\nContinuous latent state (xt \\in \\mathbb R^{dx}) evolving as (xt = A{st} x{t-1} + \\varepsilont), with (\\varepsilont \\sim \\mathcal N(0, Q{st})).\nObservations (yt \\in \\mathbb R^{dy}) via (yt = C{st} xt + \\etat), with (\\etat \\sim \\mathcal N(0, R{st})).\nInitial distribution (x0 \\sim \\mathcal N(\\mu0, P_0)).","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Inference & learning. Exact EM is intractable because of the exponential number of mode sequences. fit! uses a structured variational EM: a forward– backward step for the HMM (variational E-step) coupled with Kalman smoothing in each mode (continuous E-step), followed by M-step updates of parameters. The objective reported as mls is an ELBO that should increase monotonically.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Load-Packages","page":"Switching Linear Dynamical System Example","title":"Load Packages","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing LaTeXStrings\nusing Statistics\nusing StableRNGs\n\nrng = StableRNG(123);\nnothing #hide","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Simulate-data-from-an-SLDS","page":"Switching Linear Dynamical System Example","title":"Simulate data from an SLDS","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"state_dim = 2\nobs_dim   = 10\nK         = 2  # two modes","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"HMM (mode) parameters","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"A_hmm = [0.92 0.08; 0.06 0.94]\nπ₀    = [1.0, 0.0]","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Mode-specific state dynamics (two oscillators)","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"A₁ = 0.95 * [cos(0.05) -sin(0.05); sin(0.05) cos(0.05)]  # slower\nA₂ = 0.95 * [cos(0.55) -sin(0.55); sin(0.55) cos(0.55)]  # faster\n\nQ₁ = [0.001 0.0; 0.0 0.001]\nQ₂ = [0.1   0.0; 0.0 0.1]","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Shared initial state distribution for simplicity","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"x0 = [0.0, 0.0]\nP0 = [0.1 0.0; 0.0 0.1]","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Mode-specific observation models","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"C₁ = randn(rng, obs_dim, state_dim)\nC₂ = randn(rng, obs_dim, state_dim)\nR  = Matrix(0.1 * I(obs_dim))  # shared observation noise\n\nmodel = SwitchingLinearDynamicalSystem(\n    A_hmm,\n    [\n        LinearDynamicalSystem(GaussianStateModel(A₁, Q₁, x0, P0), GaussianObservationModel(C₁, R), state_dim, obs_dim, fill(true, 6)),\n        LinearDynamicalSystem(GaussianStateModel(A₂, Q₂, x0, P0), GaussianObservationModel(C₂, R), state_dim, obs_dim, fill(true, 6)),\n    ],\n    π₀,\n    K,\n)","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Simulate","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"T = 1000\nx, y, z = rand(rng, model, T)","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Plot-latent-dynamics-with-mode-shading","page":"Switching Linear Dynamical System Example","title":"Plot latent dynamics with mode shading","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"p1 = plot(1:T, x[1, :], label=\"x₁\", linewidth=1.5)\nplot!(1:T, x[2, :], label=\"x₂\", linewidth=1.5)","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Shade regions by discrete mode z","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"transition_points = [1; findall(diff(z) .!= 0) .+ 1; T + 1]\nfor i in 1:(length(transition_points) - 1)\n    start_idx = transition_points[i]\n    end_idx   = transition_points[i + 1] - 1\n    state_val = z[start_idx]\n    bg_color  = state_val == 1 ? :lightblue : :lightyellow\n    vspan!([start_idx, end_idx], fillalpha=0.5, color=bg_color, label=(i == 1 ? \"State $state_val\" : \"\"))\nend\n\ntitle!(\"Latent Dynamics with Mode Shading\")\nxlabel!(\"Time\")\nylabel!(\"State Value\")\nylims!(-3, 3)\n\np1","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Initialize-and-fit-an-SLDS-to-the-observations","page":"Switching Linear Dynamical System Example","title":"Initialize and fit an SLDS to the observations","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Good initialization helps. We'll set a moderately sticky HMM and rough dynamics, then call fit! for variational EM.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"A = [0.9 0.1; 0.1 0.9]\nA ./= sum(A, dims=2)   # row-stochastic\n\nπₖ = rand(rng, K); πₖ ./= sum(πₖ)\n\nQ  = Matrix(0.001 * I(state_dim))\nP0 = Matrix(0.001 * I(state_dim))\nC  = randn(rng, obs_dim, state_dim)\nR  = Matrix(0.1 * I(obs_dim))\n\nB = [\n    LinearDynamicalSystem(\n        GaussianStateModel(0.95 * [cos(f) -sin(f); sin(f) cos(f)], Q, x0, P0),\n        GaussianObservationModel(C, R),\n        state_dim, obs_dim, fill(true, 6),\n    ) for f in (0.7, 0.1)\n]\n\nlearned_model = SwitchingLinearDynamicalSystem(A, B, πₖ, K)","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Fit with variational EM","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"mls, param_diff, FB, FS = fit!(learned_model, y; max_iter=25)","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"mls is the ELBO trace; param_diff can be used as an additional stopping metric if desired; FB holds HMM posteriors and FS holds per-mode Kalman smoothing.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#ELBO-over-iterations","page":"Switching Linear Dynamical System Example","title":"ELBO over iterations","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"plot(mls, label=\"ELBO\", linewidth=1.5)\nxlabel!(\"Iteration\")\nylabel!(\"ELBO\")","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Compare-true-vs.-learned-latent-states","page":"Switching Linear Dynamical System Example","title":"Compare true vs. learned latent states","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"We combine mode-specific smoothed states using HMM responsibilities as weights.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"latents = zeros(state_dim, T)\nresp    = exp.(FB.γ)\nfor t in 1:T\n    for k in 1:K\n        latents[:, t] += FS[k].x_smooth[:, t] .* resp[k, t]\n    end\nend\n\nplt = plot(size=(800, 500), background_color=:white, margin=5Plots.mm)\nplot!(x[1, :] .+ 2, label=\"x₁ (True)\",   linewidth=2, color=:black,    alpha=0.8)\nplot!(x[2, :] .- 2, label=\"x₂ (True)\",   linewidth=2, color=:black,    alpha=0.8)\nplot!(latents[1, :] .+ 2, label=\"x₁ (Learned)\", linewidth=1.5, color=:firebrick)\nplot!(latents[2, :] .- 2, label=\"x₂ (Learned)\", linewidth=1.5, color=:royalblue)\n\ntitle!(\"SLDS: True vs Learned Latent States\")\nxlabel!(\"Time\")\nylabel!(\"\")\nyticks!([-2, 2], [\"x₂\", \"x₁\"])\nhline!([2],  color=:gray, alpha=0.3, linestyle=:dash, label=\"\")\nhline!([-2], color=:gray, alpha=0.3, linestyle=:dash, label=\"\")\nxlims!(0, T)\n\nplt","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Decoding-modes-and-basic-accuracy-metrics","page":"Switching Linear Dynamical System Example","title":"Decoding modes and basic accuracy metrics","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Hard-decoded modes by argmax responsibilities, and a simple confusion rate with the simulated ground truth (up to label permutation).","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"z_hat = map(t -> argmax(view(resp, :, t)), 1:T)","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Since labels are arbitrary, align them to best match truth via a 2x2 sweep.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"function align_labels_2way(z_true::AbstractVector{<:Integer}, z_pred::AbstractVector{<:Integer})\n    acc1 = mean(z_true .== z_pred)\n    acc2 = mean(z_true .== (3 .- z_pred))  # flip 1<->2\n    if acc2 > acc1\n        return (3 .- z_pred), acc2\n    else\n        return z_pred, acc1\n    end\nend\n\nz_aligned, acc = align_labels_2way(vec(z), z_hat)\n@info \"Mode decoding accuracy (up to permutation)\" acc","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Practical-tips-and-pitfalls","page":"Switching Linear Dynamical System Example","title":"Practical tips & pitfalls","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Stickiness: If modes switch too frequently, increase self-transition mass in A_hmm or add a stickiness prior in the M-step.\nScaling/identifiability: With per-mode C, Q, R all free, degeneracies can appear. Consider tying certain parameters across modes (e.g., shared R).\nInitialization: Seed Ahmm near diagonal; initialize Ak with different frequencies/directions to avoid identical modes; run from multiple starts.\nDiagnostics: Monitor ELBO and parameter differences; visualize responsibilities over time and check that each mode explains distinct dynamics.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/#Exercises","page":"Switching Linear Dynamical System Example","title":"Exercises","text":"","category":"section"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"Increase K to 3 and create a third oscillator; verify that modes separate.\nMake R state-specific and observe the tradeoff between Q and R in explaining variability.\nStart from nearly identical Ak and show that without good initialization the model collapses to a single effective mode; then fix with sticky Ahmm.\nPlot the responsibilities resp[k, :] over time and compare against ground truth.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"End of tutorial.","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"","category":"page"},{"location":"tutorials/switching_linear_dynamical_system_example/","page":"Switching Linear Dynamical System Example","title":"Switching Linear Dynamical System Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Misc/#Misc","page":"Miscellaneous","title":"Misc","text":"","category":"section"},{"location":"Misc/#StateSpaceDynamics.AbstractHMM","page":"Miscellaneous","title":"StateSpaceDynamics.AbstractHMM","text":"Abstract type for HMMs \n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.AutoRegressiveEmission","page":"Miscellaneous","title":"StateSpaceDynamics.AutoRegressiveEmission","text":"Special case of regression emission models that are autoregressive.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.FilterSmooth","page":"Miscellaneous","title":"StateSpaceDynamics.FilterSmooth","text":"\"     FilterSmooth{T<:Real}\n\nA mutable structure for storing smoothed estimates and associated covariance matrices in a filtering or smoothing algorithm.\n\nType Parameters\n\nT<:Real: The numerical type used for all fields (e.g., Float64, Float32).\n\nFields\n\nx_smooth::Matrix{T}   The matrix containing smoothed state estimates over time. Each column typically represents the state vector at a given time step.\np_smooth::Array{T, 3}   The posterior covariance matrices with dimensions (latentdim, latentdim, time_steps)\nE_z::Array{T, 3}   The expected latent states, size (statedim, T, ntrials).\nE_zz::Array{T, 4}   The expected value of zt * zt', size (statedim, statedim, T, n_trials).\nE_zz_prev::Array{T, 4}   The expected value of zt * z{t-1}', size (statedim, statedim, T, n_trials).\n\nExample\n\n```julia\n\nInitialize a FilterSmooth object with Float64 type\n\nfilter = FilterSmooth{Float64}(     xsmooth = zeros(10, 100),     psmooth = zeros(10, 10, 100),     Ez = zeros(10, 5, 100),     Ezz = zeros(10, 10, 5, 100),     Ezzprev = zeros(10, 10, 5, 100) )\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ForwardBackward","page":"Miscellaneous","title":"StateSpaceDynamics.ForwardBackward","text":"ForwardBackward{T<:Real}\n\nA mutable struct that encapsulates the forward–backward algorithm outputs for a hidden Markov model (HMM).\n\nFields\n\nloglikelihoods::Matrix{T}: Matrix of log-likelihoods for each observation and state.\nα::Matrix{T}: The forward probabilities (α) for each time step and state.\nβ::Matrix{T}: The backward probabilities (β) for each time step and state.\nγ::Matrix{T}: The state occupancy probabilities (γ) for each time step and state.\nξ::Array{T,3}: The pairwise state occupancy probabilities (ξ) for consecutive time steps and state pairs.\n\nTypically, α and β are computed by the forward–backward algorithm to find the likelihood of an observation sequence. γ and ξ are derived from these calculations to estimate how states transition over time.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.MixtureModel","page":"Miscellaneous","title":"StateSpaceDynamics.MixtureModel","text":"Abstract type for Mixture Models. I.e. GMM's, etc.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.RegressionModel","page":"Miscellaneous","title":"StateSpaceDynamics.RegressionModel","text":"Abstract type for Regression Models. I.e. GaussianRegression, BernoulliRegression, etc.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.GaussianHMM-Union{Tuple{}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.GaussianHMM","text":"GaussianHMM(; K::Int, output_dim::Int, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Hidden Markov Model with Gaussian Emissions\n\nArguments\n\nK::Int: The number of hidden states\noutput_dim::Int: The dimensionality of the observation\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization)\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization)\n\nReturns\n\n::HiddenMarkovModel: Hidden Markov Model Object with Gaussian Emissions\n\n```\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingAutoRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingAutoRegression","text":"SwitchingAutoRegression(; K::Int, output_dim::Int, order::Int, include_intercept::Bool=true, β::Matrix{<:Real}=if include_intercept zeros(output_dim * order + 1, output_dim) else zeros(output_dim * order, output_dim) end, Σ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim), λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching AutoRegression Model\n\nArguments\n\nK::Int: The number of hidden states.\noutput_dim::Int: The dimensionality of the output data.\norder::Int: The order of the autoregressive model.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model.\nβ::Matrix{<:Real}: The autoregressive coefficients (defaults to zeros).\nΣ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim): The covariance matrix for the autoregressive model (defaults to an identity matrix).\nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (Defaults to a random initialization). \nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (Defaults to a random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching AutoRegression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingBernoulliRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingBernoulliRegression","text":"SwitchingBernoulliRegression(; K::Int, input_dim::Int, include_intercept::Bool=true, β::Vector{<:Real}=if include_intercept zeros(input_dim + 1) else zeros(input_dim) end, λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching Bernoulli Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input data.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model (defaults to true).\nβ::Vector{<:Real}: The regression coefficients (defaults to zeros). \nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization).\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Bernoulli Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingGaussianRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingGaussianRegression","text":"SwitchingGaussianRegression(; \n    K::Int,\n    input_dim::Int,\n    output_dim::Int,\n    include_intercept::Bool = true,\n    β::Matrix{<:Real} = if include_intercept\n        zeros(input_dim + 1, output_dim)\n    else\n        zeros(input_dim, output_dim)\n    end,\n    Σ::Matrix{<:Real} = Matrix{Float64}(I, output_dim, output_dim),\n    λ::Float64 = 0.0,\n    A::Matrix{<:Real} = initialize_transition_matrix(K),\n    πₖ::Vector{Float64} = initialize_state_distribution(K)\n)\n\nCreate a Switching Gaussian Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input features.\noutput_dim::Int: The dimensionality of the output predictions.\ninclude_intercept::Bool: Whether to include an intercept in the regression model (default is true).\nβ::Matrix{<:Real}: The regression coefficients (defaults to zeros based on input_dim and output_dim).\nΣ::Matrix{<:Real}: The covariance matrix of the Gaussian emissions (defaults to an identity matrix).\nλ::Float64: The regularization parameter for the regression (default is 0.0).\nA::Matrix{<:Real}: The transition matrix of the Hidden Markov Model (defaults to random initialization).\nπₖ::Vector{Float64}: The initial state distribution of the Hidden Markov Model (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Gaussian Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#Base.getproperty-Tuple{StateSpaceDynamics.AutoRegressiveEmission, Symbol}","page":"Miscellaneous","title":"Base.getproperty","text":"getproperty(model::AutoRegressiveEmission, sym::Symbol)\n\nGet various properties of 'innerGaussianRegression`. \n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridgm-Union{Tuple{T}, Tuple{Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridgm","text":"block_tridgm(main_diag::Vector{Matrix{T}}, upper_diag::Vector{Matrix{T}}, lower_diag::Vector{Matrix{T}}) where {T<:Real}\n\nConstruct a block tridiagonal matrix from three vectors of matrices.\n\nThrows\n\nErrorException if the lengths of upper_diag and lower_diag are not one less than the length of main_diag.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse-Union{Tuple{T}, Tuple{Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse","text":"block_tridiagonal_inverse(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix.\n\nNotes: This implementation is from the paper:\n\n\"An Accelerated Lambda Iteration Method for Multilevel Radiative Transfer” Rybicki, G.B., and Hummer, D.G., Astronomy and Astrophysics, 245, 171–181 (1991), Appendix B.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse_static-Union{Tuple{N}, Tuple{T}, Tuple{Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}, Val{N}}} where {T<:Real, N}","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse_static","text":"block_tridiagonal_inverse_static(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix using static matrices. See block_tridiagonal_inverse for details.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.check_same_type-Tuple","page":"Miscellaneous","title":"StateSpaceDynamics.check_same_type","text":"check_same_type(args...)\n\nUtility function to check if n arguments share the same types. \n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.euclidean_distance-Union{Tuple{T2}, Tuple{T1}, Tuple{AbstractVector{T1}, AbstractVector{T2}}} where {T1<:Real, T2<:Real}","page":"Miscellaneous","title":"StateSpaceDynamics.euclidean_distance","text":"euclidean_distance(a::AbstractVector{Float64}, b::AbstractVector{Float64})\n\nCalculate the Euclidean distance between two points.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.gaussian_entropy-Tuple{LinearAlgebra.Symmetric{BigFloat, <:SparseArrays.AbstractSparseMatrix}}","page":"Miscellaneous","title":"StateSpaceDynamics.gaussian_entropy","text":"gaussian_entropy(H::Symmetric{BigFloat, <:SparseMatrix})\n\nSpecialized method for BigFloat sparse matrices using logdet.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.gaussian_entropy-Union{Tuple{LinearAlgebra.Symmetric{T, S} where S<:(AbstractMatrix{<:T})}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.gaussian_entropy","text":"gaussian_entropy(H::Symmetric{T}) where {T<:Real}\n\nCalculate the entropy of a Gaussian distribution with Hessian (i.e. negative precision) matrix H.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}, Tuple{AbstractMatrix{T}, Int64, Int64}, Tuple{AbstractMatrix{T}, Int64, Int64, Float64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::AbstractMatrix{T}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6) where {T<:Real}\n\nPerform K-means clustering on column-major data.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering-Union{Tuple{T}, Tuple{AbstractVector{T}, Int64}, Tuple{AbstractVector{T}, Int64, Int64}, Tuple{AbstractVector{T}, Int64, Int64, Float64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::AbstractVector{T}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6)\n\nPerform K-means clustering on vector data.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::AbstractMatrix{T}, k_means::Int) where {T<:Real}\n\nPerform K-means++ initialization for cluster centroids (column-major input).\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Union{Tuple{T}, Tuple{AbstractVector{T}, Int64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::AbstractVector{T}, k_means::Int)\n\nK-means++ initialization for vector data.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.logistic-Tuple{Real}","page":"Miscellaneous","title":"StateSpaceDynamics.logistic","text":"logistic(x::Real)\n\nCalculate the logistic function in a numerically stable way.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.make_posdef!-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.make_posdef!","text":"make_posdef!(A::AbstractMatrix{T}) where {T<:Real}\n\nEnsure that a matrix is positive definite by adjusting its eigenvalues.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.random_rotation_matrix","page":"Miscellaneous","title":"StateSpaceDynamics.random_rotation_matrix","text":"random_rotation_matrix(n)\n\nGenerate a random rotation matrix of size n x n.\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.row_matrix-Tuple{AbstractVector}","page":"Miscellaneous","title":"StateSpaceDynamics.row_matrix","text":"row_matrix(x::AbstractVector)\n\nConvert a vector to a row matrix.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.stabilize_covariance_matrix-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.stabilize_covariance_matrix","text":"stabilize_covariance_matrix(Σ::Matrix{<:Real})\n\nStabilize a covariance matrix by ensuring it is symmetric and positive definite.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#What-are-Emission-Models?","page":"EmissionModels","title":"What are Emission Models?","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Emission models describe how observations are generated from latent states in a state space model. These models define the conditional distribution of the observed data given the hidden state or input features. In StateSpaceDynamics.jl, a flexible suite of emission models is supported, including both simple parametric distributions and regression-based models.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"At a high level, emission models encode:","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The distribution of observations (e.g., Gaussian, Poisson, Bernoulli)\nHow observations relate to inputs or latent states, either directly or via regression","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.EmissionModel","page":"EmissionModels","title":"StateSpaceDynamics.EmissionModel","text":"Base type hierarchy for emission models. Each emission model must implement:\n\nsample()\nloglikelihood()\nfit!()\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.RegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.RegressionEmission","text":"Base type hierarchy for regression emission models.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Gaussian-Emission-Model","page":"EmissionModels","title":"Gaussian Emission Model","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The GaussianEmission is a basic model where the observations are drawn from a multivariate normal distribution with a fixed mean and covariance.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim mathcalN(mu Sigma)","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"This emission model is often used when the observed data is real-valued and homoscedastic.","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianEmission","text":"mutable struct GaussianEmission <: EmissionModel\n\nGaussianEmission model with mean and covariance.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Union{Tuple{T}, Tuple{GaussianEmission, AbstractMatrix{T}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianEmission, Y::AbstractMatrix{T}) where {T<:Real}\n\nCalculate the log likelihood of the data Y given the Gaussian emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Base.rand-Tuple{GaussianEmission}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::GaussianEmission; kwargs...)\nRandom.rand(rng::AbstractRNG, model::GaussianEmission; n::Int=1)\n\nGenerate random samples from  a Gaussian emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{GaussianEmission, AbstractMatrix{T}}, Tuple{GaussianEmission, AbstractMatrix{T}, AbstractVector{T}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.fit!","text":"function fit!(model::GaussianEmission, \n        Y::AbstractMatrix{T}, \n        w::AbstractVector{T}=ones(size(Y, 1))) where {T<:Real}\n\nFit a GaussianEmission model to the data Y weighted by weights w.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Regression-Based-Emission-Models","page":"EmissionModels","title":"Regression-Based Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Regression-based emissions allow the output to depend on an input matrix Phi. The regression relationship is defined by a coefficient matrix beta, optionally with an intercept and regularization.","category":"page"},{"location":"EmissionModels/#Gaussian-Regression-Emission","page":"EmissionModels","title":"Gaussian Regression Emission","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"In the GaussianRegressionEmission, the outputs are real-valued and modeled via linear regression with additive Gaussian noise.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim mathcalN(Phi_t beta Sigma)","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianRegressionEmission","text":"GaussianRegressionEmission\n\nStore a Gaussian regression Emission model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\noutput_dim::Int: Dimension of the output data.\ninclude_intercept::Bool: Whether to include an intercept term; if true, the first column of β is assumed to be the intercept/bias.\nβ::AbstractMatrix{<:Real} = if include_intercept zeros(input_dim + 1, output_dim) else zeros(input_dim, output_dim) end: Coefficient matrix of the model. Shape inputdim by outputdim. The first row are the intercept terms, if included.\nΣ::AbstractMatrix{<:Real}: Covariance matrix of the model.\nλ:<Real: Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{GaussianRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::GaussianRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\nRandom.rand(rng::AbstractRNG, model::GaussianRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\n\nGenerate samples from a Gaussian regression model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Union{Tuple{T}, Tuple{GaussianRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}}, Tuple{GaussianRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}, Union{Nothing, AbstractVector{T}}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianRegressionEmission,\n    Φ::AbstractMatrix{T},\n    Y::AbstractMatrix{T},\n    w::AbstractVector{T}=ones(size(Y, 1))) where {T<:Real}\n\nCalculate the log likelihood of the data Y given the Gaussian regression emission model and the input features Φ.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Bernoulli-Regression-Emission","page":"EmissionModels","title":"Bernoulli Regression Emission","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The BernoulliRegressionEmission is appropriate for binary data. The probability of success is modeled via a logistic function.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"p(y_t = 1 mid Phi_t) = sigma(Phi_t beta)","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Where sigma(z) = 1  (1 + e^-z) is the logistic function.","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.BernoulliRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.BernoulliRegressionEmission","text":"BernoulliRegressionEmission\n\nStore a Bernoulli regression model.\n\nFields\n\ninput_dim::Int: Dimensionality of the input data.\noutput_dim::Int: Dimensionality of the outputd data.\ninclude_intercept::Bool: Whether to include an intercept term.\nβ::AbstractMatrix{<:Real}: Bernoulli regression coefficients.\nλ<:Real: L2 Regularization parameter.\n\n```\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{BernoulliRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::BernoulliRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\nRandom.rand(rng::AbstractRNG, model::BernoulliRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\n\nGenerate samples from a Bernoulli regression emission.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"function loglikelihood(\n    model::BernoulliRegressionEmission,\n    Φ::AbstractMatrix{T1},\n    Y::AbstractMatrix{T2},\n    w::AbstractVector{T3}=ones(size(Y, 1))) where {T1<:Real, T2<:Real, T3<:Real}\n\nCalculate the log likelihood of the data Y given the Bernoulli regression emission model and the input features Φ. Optionally, a vector of weights w can be provided.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#Poisson-Regression-Emission","page":"EmissionModels","title":"Poisson Regression Emission","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The PoissonRegressionEmission is ideal for count data, such as spike counts in neuroscience. It models the intensity of the Poisson distribution as an exponential function of the linear predictors.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim textPoisson(lambda_t) quad lambda_t = exp(Phi_t beta)","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.PoissonRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.PoissonRegressionEmission","text":"PoissonRegressionEmission\n\nA Poisson regression model.\n\nFields\n\ninput_dim::Int: Dimensionality of the input data.\noutput_dim::Int: Dimensionality of the output data.\ninclude_intercept::Bool: Whether to include a regression intercept.\nβ::AbstractMatrix{<:Real}: The regression coefficients matrix.\nλ::Real;: L2 Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{PoissonRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::PoissonRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\nRandom.rand(rng::AbstractRNG, model::PoissonRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\n\nGenerate samples from a Poisson regression emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-2","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(\n    model::PoissonRegressionEmission,\n    Φ::AbstractMatrix{T1},\n    Y::AbstractMatrix{T2},\n    w::AbstractVector{T3}=ones(size(Y, 1))) where {T1<:Real, T2<:Real, T3<:Real}\n\nCalculate the log-likelihood of a Poisson regression model.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#Autoregressive-Emission-Models","page":"EmissionModels","title":"Autoregressive Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The AutoRegressionEmission models the observation at time t as depending on previous observations (i.e., an autoregressive structure), using a wrapped GaussianRegressionEmission.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim mathcalN(sum_i=1^p A_i y_t-i Sigma)","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Where p is the autoregressive order and A_i are regression weights.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"This model is useful when modeling temporal dependencies in the emission process, independent of latent dynamics.","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.AutoRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.AutoRegressionEmission","text":"AutoRegressionEmission <: EmissionModel\n\nStore an autoregressive emission model, which wraps around a GaussianRegressionEmission.\n\nFields\n\noutput_dim::Int: The dimensionality of the output data\norder::Int: The order of the Autoregressive process\ninnerGaussianRegression::GaussianRegressionEmission: The underlying Gaussian regression model used for the emissions.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{AutoRegressionEmission, Matrix{<:Real}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::AutoRegressionEmission, X::Matrix{<:Real})\nRandom.rand(rng::AbstractRNG, model::AutoRegressionEmission, X::Matrix{<:Real})\n\nGenerate samples from an autoregressive emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Union{Tuple{T}, Tuple{AutoRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}}, Tuple{AutoRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}, Union{Nothing, AbstractVector{T}}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(\n    model::AutoRegressionEmission,\n    X::AbstractMatrix{T},\n    Y::AbstractMatrix{T},\n    w::Vector{T}=ones(size(Y, 1))) where {T<:Real}\n\nCalculate the log likelihood of the data Y given the autoregressive emission model and the previous observations X.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Fitting-Regression-Emission-Models","page":"EmissionModels","title":"Fitting Regression Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"All regression-based emissions can be fitted using maximum likelihood with optional weights and L2 regularization. Internally, StateSpaceDynamics.jl formulates this as an optimization problem, solved using gradient-based methods (e.g., LBFGS).","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.fit!-Union{Tuple{T3}, Tuple{T2}, Tuple{T1}, Tuple{RegressionEmission, AbstractMatrix{T1}, AbstractMatrix{T2}}, Tuple{RegressionEmission, AbstractMatrix{T1}, AbstractMatrix{T2}, AbstractVector{T3}}} where {T1<:Real, T2<:Real, T3<:Real}","page":"EmissionModels","title":"StateSpaceDynamics.fit!","text":"fit!(\n    model::RegressionEmission,\n    X::AbstractMatrix{T1},\n    y::AbstractMatrix{T2},\n    w::AbstractVector{T3}=ones(size(y, 1))) where {T1<:Real, T2<:Real, T3<:Real}\n\nFit a regression emission model give input data X, output data y, and weights w.\n\nArguments\n\n- `model::RegressionEmission`: A regression emission model.\n- `X::AbstractMatrix{<:Real}:`: Input data.\n- `y::AbstractMatrix{<:Real}`: Output data.\n- `w::AbstractVector{<:Real}`: Weights to define each point's contribution to the fit.\n\nReturns\n\n- `model::RegressionEmission`: The regression model with the newly updated parameters.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#What-is-a-Linear-Dynamical-System?","page":"Linear Dynamical Systems","title":"What is a Linear Dynamical System?","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A Linear Dynamical System (LDS) is a mathematical model used to describe how a system evolves over time. These systems are a subset of state-space models, where the hidden state dynamics are continuous. What makes these models linear is that the latent dynamics evolve according to a linear function of the previous state. The observations, however, can be related to the hidden state through a nonlinear link function.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"At its core, an LDS defines:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A state transition function: how the internal state evolves from one time step to the next.\nAn observation function: how the internal state generates the observed data.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.LinearDynamicalSystem","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.LinearDynamicalSystem","text":"LinearDynamicalSystem{T<:Real, S<:AbstractStateModel{T}, O<:AbstractObservationModel{T}}\n\nRepresents a unified Linear Dynamical System with customizable state and observation models.\n\nFields\n\nstate_model::S: The state model (e.g., GaussianStateModel)\nobs_model::O: The observation model (e.g., GaussianObservationModel or PoissonObservationModel)\nlatent_dim::Int: Dimension of the latent state\nobs_dim::Int: Dimension of the observations\nfit_bool::Vector{Bool}: Vector indicating which parameters to fit during optimization\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#The-Gaussian-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Gaussian Linear Dynamical System — typically just referred to as an LDS — is a specific type of linear dynamical system where both the state transition and observation functions are linear, and all noise is Gaussian.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The generative model is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t sim mathcalN(A x_t-1 Q) \n    y_t sim mathcalN(C x_t R)\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x_t is the hidden state at time t\ny_t is the observed data at time t  \nA is the state transition matrix\nC is the observation matrix\nQ is the process noise covariance\nR is the observation noise covariance","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This can equivalently be written in equation form:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t = A x_t-1 + epsilon_t \n    y_t = C x_t + eta_t\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"ε_t ~ N(0, Q) is the process noise\nη_t ~ N(0, R) is the observation noise","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianStateModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianStateModel","text":"GaussianStateModel{T<:Real. M<:AbstractMatrix{T}, V<:AbstractVector{T}}}\n\nRepresents the state model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nA::M: Transition matrix (size latent_dim×latent_dim). \nQ::M: Process noise covariance matrix \nx0::V: Initial state vector (length latent_dim).\nP0::M: Initial state covariance matrix (size `latentdim×latentdim\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianObservationModel","text":"GaussianObservationModel{T<:Real, M<:AbstractMatrix{T}}\n\nRepresents the observation model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nC::M: Observation matrix of size (obs_dim × latent_dim). Maps latent states into observation space. \nR::M: Observation noise covariance of size (obs_dim × obs_dim). \n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#The-Poisson-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Poisson Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Poisson Linear Dynamical System is a variant of the LDS where the observations are modeled as counts. This is useful in fields like neuroscience where we are often interested in modeling spike count data. To relate the spiking data to the Gaussian latent variable, we use a nonlinear link function, specifically the exponential function. ","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The generative model is given by: ","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t sim mathcalN(A x_t-1 Q) \n    y_t sim textPoisson(exp(Cx_t + b))\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where b is a bias term.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.PoissonObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.PoissonObservationModel","text":"PoissonObservationModel{T<:Real, M<:AbstractMatrix{T}, V<:AbstractVector{T}} <: AbstractObservationModel{T}\n\nRepresents the observation model of a Linear Dynamical System with Poisson observations.\n\nFields\n\nC::AbstractMatrix{T}: Observation matrix of size (obs_dim × latent_dim). Maps latent states into observation space.\nlog_d::AbstractVector{T}: Mean firing rate vector (log space) of size (obs_dim × obs_dim). \n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#Sampling-from-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Sampling from Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"You can generate synthetic data from fitted LDS models:","category":"page"},{"location":"LinearDynamicalSystems/#Base.rand-Tuple{LinearDynamicalSystem}","page":"Linear Dynamical Systems","title":"Base.rand","text":"Random.rand(lds::LinearDynamicalSystem; tsteps::Int, ntrials::Int)\nRandom.rand(rng::AbstractRNG, lds::LinearDynamicalSystem; tsteps::Int, ntrials::Int)\n\nSample from a Linear Dynamical System.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#Inference-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Inference in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In StateSpaceDynamics.jl, we directly maximize the complete-data log-likelihood function with respect to the latent states given the data and the parameters of the model. In other words, the maximum a priori (MAP) estimate of the latent state path is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"undersetxtextargmax  left log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t) right","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This MAP estimation approach has the same computational complexity as traditional Kalman filtering and smoothing — mathcalO(T) — but is significantly more flexible. Notably, it can handle nonlinear observations and non-Gaussian noise while still yielding exact MAP estimates, unlike approximate techniques such as the Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF).","category":"page"},{"location":"LinearDynamicalSystems/#Newton's-Method-for-Latent-State-Optimization","page":"Linear Dynamical Systems","title":"Newton's Method for Latent State Optimization","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"To find the MAP trajectory, we iteratively optimize the latent states using Newton's method. The update equation at each iteration is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x^(i+1) = x^(i) - left nabla^2 mathcalL(x^(i)) right^-1 nabla mathcalL(x^(i))","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x) is the complete-data log-likelihood:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x) = log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t)","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"nabla mathcalL(x) is the gradient of the full log-likelihood with respect to all latent states\nnabla^2 mathcalL(x) is the Hessian of the full log-likelihood","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This update is performed over the entire latent state sequence x_1T, and repeated until convergence.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"For Gaussian models, mathcalL(x) is quadratic and Newton's method converges in a single step — recovering the exact Kalman smoother solution. For non-Gaussian models, the Hessian is not constant and the optimization is more complex. However, the MAP estimate can still be computed efficiently using the same approach as the optimization problem is still convex.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.smooth","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.smooth","text":"smooth(lds, y)\n\nThis function performs direct smoothing for a linear dynamical system (LDS) given the system parameters and the observed data for a single trial\n\nArguments\n\nlds::LinearDynamicalSystem{T,S,O}: The LDS object representing the system parameters.\ny::AbstractMatrix{T}: The observed data matrix.\nw::Union{Nothing,AbstractVector{T}}: coeffcients to weight the data.\n\nReturns\n\nx::AbstractMatrix{T}: The optimal state estimate.\np_smooth::Array{T, 3}: The posterior covariance matrix.\ninverse_offdiag::Array{T, 3}: The inverse off-diagonal matrix.\nQ_val::T: The Q-function value.\n\n\n\n\n\n","category":"function"},{"location":"LinearDynamicalSystems/#Laplace-Approximation-of-Posterior-for-Non-Conjugate-Observation-Models","page":"Linear Dynamical Systems","title":"Laplace Approximation of Posterior for Non-Conjugate Observation Models","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In the case of non-Gaussian observations, we can use a Laplace approximation to compute the posterior distribution of the latent states. For Gaussian observations (which are conjugate with the Gaussian state model), the posterior is also Gaussian and is the exact posterior. However, for non-Gaussian observations, we can approximate the posterior using a Gaussian distribution centered at the MAP estimate of the latent states. This approximation is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"p(x mid y) approx mathcalN(x^* -left nabla^2 mathcalL(x^*) right^-1)","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x^* is the MAP estimate of the latent states\nnabla^2 mathcalL(x^*) is the Hessian of the log-likelihood at the MAP estimate","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Despite the requirement of inverting a Hessian of dimension (d times T) times (d times T), this is still computationally efficient, as the Markov structure of the model renders the Hessian block-tridiagonal, and thus the inversion is tractable.","category":"page"},{"location":"LinearDynamicalSystems/#Learning-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Learning in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Given the latent structure of state-space models, we must rely on either the Expectation-Maximization (EM) or Variational Inference (VI) approaches to learn the parameters of the model. StateSpaceDynamics.jl supports both EM and VI. For LDS models, we can use Laplace EM, where we approximate the posterior of the latent state path using the Laplace approximation as outlined above. Using these approximate posteriors (or exact ones in the Gaussian case), we can apply closed-form updates for the model parameters.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.fit!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{LinearDynamicalSystem{T, S, O}, AbstractArray{T, 3}}} where {T<:Real, S<:(GaussianStateModel{T, M, V} where {M<:AbstractMatrix{T}, V<:AbstractVector{T}}), O<:AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(lds, y; max_iter::Int=1000, tol::Real=1e-12) \nwhere {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nFit a Linear Dynamical System using the Expectation-Maximization (EM) algorithm with Kalman smoothing over multiple trials\n\nArguments\n\nlds::LinearDynamicalSystem{T,S,O}: The Linear Dynamical System to be fitted.\ny::AbstractArray{T,3}: Observed data, size(obsdim, Tsteps, n_trials)\n\nKeyword Arguments\n\nmax_iter::Int=1000: Maximum number of EM iterations.\ntol::T=1e-12: Convergence tolerance for log-likelihood change.\n\nReturns\n\nmls::Vector{T}: Vector of log-likelihood values for each iteration.\nparam_diff::Vector{T}: Vector of parameter deltas for each EM iteration. \n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#What-is-a-Mixture-Model?","page":"Mixture Models","title":"What is a Mixture Model?","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"A mixture model is a probabilistic model that represents a population consisting of multiple subpopulations, where each observation is assumed to come from one of several component distributions. Mixture models are particularly useful for modeling data that exhibits multimodal behavior or comes from heterogeneous sources.","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Formally, a mixture model with K components is defined as:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"f_mix(x Theta pi) = sum_k=1^K pi_k f_k(x theta_k)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"f_k(x theta_k)\nis the k-th component distribution with parameters theta_k\npi_k\nis the mixing coefficient (or mixing weight) for component k\nTheta = theta_1 ldots theta_K\nare the component parameters\npi = pi_1 ldots pi_K\nare the mixing coefficients with sum_k=1^K pi_k = 1 and pi_k geq 0","category":"page"},{"location":"MixtureModels/#Generative-Process","page":"Mixture Models","title":"Generative Process","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"The generative process for a mixture model can be described as follows:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"beginalign*\n    z_i sim textCat(pi) \n    x_i mid z_i = k sim f_k(x theta_k)\nendalign*","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"z_i\nis a latent assignment variable indicating which component generated observation x_i\nz_i = k\nmeans observation x_i was generated by component k\ntextCat(pi)\nis the categorical distribution with probabilities pi","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"This two-step process first selects a component according to the mixing weights, then generates an observation from that component's distribution.","category":"page"},{"location":"MixtureModels/#Key-Properties","page":"Mixture Models","title":"Key Properties","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Identifiability: Mixture models are generally identifiable up to permutation of the component labels. This means that swapping component labels and their associated parameters yields an equivalent model.","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Model Selection: The number of components K is typically unknown and must be determined using model selection criteria such as AIC, BIC, or cross-validation.","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Latent Structure: The latent variables z_i provide a natural clustering interpretation, where observations are probabilistically assigned to components.","category":"page"},{"location":"MixtureModels/#Gaussian-Mixture-Model","page":"Mixture Models","title":"Gaussian Mixture Model","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"A Gaussian Mixture Model (GMM) is a mixture model where each component is a multivariate Gaussian distribution. GMMs are among the most widely used mixture models due to their mathematical tractability and flexibility in modeling continuous data.","category":"page"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel\n\nA Gaussian Mixture Model for clustering and density estimation.\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel-Tuple{Int64, Int64}","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel(k::Int, data_dim::Int)\n\nConstructor for GaussianMixtureModel. Initializes Σₖ's covariance matrices to the  identity, πₖ to a uniform distribution, and μₖ's means to zeros.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#Mathematical-Formulation","page":"Mixture Models","title":"Mathematical Formulation","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"For a GMM with K components in D dimensions, the mixture density is:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"f_GMM(x Theta pi) = sum_k=1^K pi_k mathcalN(x mu_k Sigma_k)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where each component is a multivariate Gaussian:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"mathcalN(x mu_k Sigma_k) = frac1(2pi)^D2 Sigma_k^12 expleft(-frac12(x - mu_k)^T Sigma_k^-1 (x - mu_k)right)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"The parameters for each component k are:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"mu_k in mathbbR^D\n: the mean vector\nSigma_k in mathbbR^D times D\n: the covariance matrix (positive definite)","category":"page"},{"location":"MixtureModels/#Applications","page":"Mixture Models","title":"Applications","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"GMMs are particularly effective for:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Density estimation for continuous multimodal data\nSoft clustering where observations can belong to multiple clusters with different probabilities\nDimensionality reduction when combined with factor analysis\nAnomaly detection by identifying low-probability regions","category":"page"},{"location":"MixtureModels/#Poisson-Mixture-Model","page":"Mixture Models","title":"Poisson Mixture Model","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"A Poisson Mixture Model (PMM) is designed for modeling count data that exhibits overdispersion or multimodality. Each component follows a Poisson distribution, making it suitable for discrete, non-negative integer observations.","category":"page"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel\n\nA Poisson Mixture Model for clustering and density estimation.\n\nFields\n\nk::Int: Number of poisson-distributed clusters.\nλₖ::Vector{Float64}: Means of each cluster.\nπₖ::Vector{Float64}: Mixing coefficients for each cluster.\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel-Tuple{Int64}","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel(k::Int)\n\nConstructor for PoissonMixtureModel. Initializes λₖ's means to  ones and πₖ to a uniform distribution.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#Mathematical-Formulation-2","page":"Mixture Models","title":"Mathematical Formulation","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"For a PMM with K components, the mixture density is:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"f_PMM(x lambda pi) = sum_k=1^K pi_k textPoisson(x lambda_k)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where each component is a Poisson distribution:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"textPoisson(x lambda_k) = fraclambda_k^x e^-lambda_kx","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"The parameter for each component k is:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"lambda_k  0\n: the rate parameter (both mean and variance of the Poisson distribution)","category":"page"},{"location":"MixtureModels/#Applications-2","page":"Mixture Models","title":"Applications","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"PMMs are commonly used for:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Count data modeling with heterogeneous populations\nModeling overdispersed count data where variance exceeds the mean\nAnalyzing arrival processes with multiple underlying rates\nBiological applications such as modeling gene expression counts","category":"page"},{"location":"MixtureModels/#Learning-in-Mixture-Models","page":"Mixture Models","title":"Learning in Mixture Models","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"StateSpaceDynamics.jl implements the Expectation-Maximization (EM) algorithm for parameter estimation in mixture models. EM is an iterative algorithm that finds maximum likelihood estimates in the presence of latent variables.","category":"page"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(gmm::GaussianMixtureModel, data::AbstractMatrix{<:Real}; <keyword arguments>)\n\nFits a Gaussian Mixture Model (GMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\ngmm::GaussianMixtureModel: The Gaussian Mixture Model to be fitted.\ndata::AbstractMatrix{<:Real}: The dataset on which the model will be fitted, where each row represents a data point.\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the GMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(pmm::PoissonMixtureModel, data::AbstractMatrix{<:Integer}; maxiter::Int=50, tol::Float64=1e-3, initialize_kmeans::Bool=false)\n\nFits a Poisson Mixture Model (PMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\npmm::PoissonMixtureModel: The Poisson Mixture Model to be fitted.\ndata::AbstractMatrix{<:Integer}: The dataset on which the model will be fitted, where each row represents a data point.\n\nKeyword Arguments\n\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the PMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#Expectation-Maximization-Algorithm","page":"Mixture Models","title":"Expectation-Maximization Algorithm","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"The EM algorithm alternates between two steps:","category":"page"},{"location":"MixtureModels/#Expectation-Step-(E-step)","page":"Mixture Models","title":"Expectation Step (E-step)","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Calculate the posterior probabilities (responsibilities) that each observation belongs to each component:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"gamma_ik = p(z_i = k mid x_i theta^(t)) = fracpi_k^(t) f_k(x_i theta_k^(t))sum_j=1^K pi_j^(t) f_j(x_i theta_j^(t))","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where gamma_ik represents the responsibility of component k for observation x_i.","category":"page"},{"location":"MixtureModels/#Maximization-Step-(M-step)","page":"Mixture Models","title":"Maximization Step (M-step)","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Update the parameters by maximizing the expected complete data log-likelihood:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Mixing coefficients:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"pi_k^(t+1) = frac1N sum_i=1^N gamma_ik","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"For Gaussian components:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"beginalign*\nmu_k^(t+1) = fracsum_i=1^N gamma_ik x_isum_i=1^N gamma_ik \nSigma_k^(t+1) = fracsum_i=1^N gamma_ik (x_i - mu_k^(t+1))(x_i - mu_k^(t+1))^Tsum_i=1^N gamma_ik\nendalign*","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"For Poisson components:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"lambda_k^(t+1) = fracsum_i=1^N gamma_ik x_isum_i=1^N gamma_ik","category":"page"},{"location":"MixtureModels/#Convergence-and-Initialization","page":"Mixture Models","title":"Convergence and Initialization","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"The EM algorithm is guaranteed to converge to a local maximum of the likelihood function. However, the quality of the solution depends heavily on initialization:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Random initialization: Parameters are randomly initialized\nK-means initialization: Use K-means clustering to initialize component means and mixing weights\nMultiple random starts: Run EM from multiple random initializations and select the best solution","category":"page"},{"location":"MixtureModels/#Model-Evaluation-and-Selection","page":"Mixture Models","title":"Model Evaluation and Selection","text":"","category":"section"},{"location":"MixtureModels/#Log-Likelihood","page":"Mixture Models","title":"Log-Likelihood","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"The log-likelihood of the data under the fitted model provides a measure of model fit:","category":"page"},{"location":"MixtureModels/#StateSpaceDynamics.loglikelihood-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(gmm::GaussianMixtureModel, data::AbstractMatrix{<:Real})\n\nCompute the log-likelihood of the data given the Gaussian Mixture Model (GMM). The data matrix should be of shape (# observations, # features).\n\nArguments\n\ngmm::GaussianMixtureModel: The Gaussian Mixture Model instance \ndata::AbstractMatrix{<:Real}: data matrix to calculate the Log-Likelihood \n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.loglikelihood-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(pmm::PoissonMixtureModel, data::AbstractMatrix{<:Integer})\n\nCompute the log-likelihood of the data given the Poisson Mixture Model (PMM). The data matrix should be of shape (# features, # obs).\n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"ell(Theta pi) = sum_i=1^N log left( sum_k=1^K pi_k f_k(x_i theta_k) right)","category":"page"},{"location":"MixtureModels/#Information-Criteria","page":"Mixture Models","title":"Information Criteria","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"For model selection (choosing the optimal number of components K):","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Akaike Information Criterion (AIC):","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"textAIC = -2ell(hatTheta hatpi) + 2p","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Bayesian Information Criterion (BIC):","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"textBIC = -2ell(hatTheta hatpi) + p log(N)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where p is the number of free parameters in the model.","category":"page"},{"location":"MixtureModels/#Inference-and-Applications","page":"Mixture Models","title":"Inference and Applications","text":"","category":"section"},{"location":"MixtureModels/#Posterior-Probabilities","page":"Mixture Models","title":"Posterior Probabilities","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"After fitting, the posterior probability that observation x_i belongs to component k is:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"p(z_i = k mid x_i hatTheta hatpi) = frachatpi_k f_k(x_i hattheta_k)sum_j=1^K hatpi_j f_j(x_i hattheta_j)","category":"page"},{"location":"MixtureModels/#Hard-Assignment","page":"Mixture Models","title":"Hard Assignment","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"For clustering applications, observations can be assigned to their most likely component:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"hatz_i = argmax_k p(z_i = k mid x_i hatTheta hatpi)","category":"page"},{"location":"MixtureModels/#Sampling","page":"Mixture Models","title":"Sampling","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"New observations can be generated from the fitted mixture model by:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Sampling a component k sim textCat(hatpi)\nSampling from that component: x sim f_k(x hattheta_k)","category":"page"},{"location":"MixtureModels/#Practical-Considerations","page":"Mixture Models","title":"Practical Considerations","text":"","category":"section"},{"location":"MixtureModels/#Computational-Complexity","page":"Mixture Models","title":"Computational Complexity","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Time complexity: O(NKD cdot textiterations) where N is the number of observations, K is the number of components, and D is the dimensionality\nSpace complexity: O(NK + KD^2) for storing responsibilities and covariance matrices (GMM)","category":"page"},{"location":"MixtureModels/#Common-Issues","page":"Mixture Models","title":"Common Issues","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Singular covariance matrices: In GMMs, components may collapse to single points, leading to singular covariance matrices. Regularization techniques include adding a small value to the diagonal.","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Empty components: Some components may receive very little probability mass during EM iterations. This can be addressed by reinitializing empty components.","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Convergence to local optima: EM is sensitive to initialization. Multiple random starts and careful initialization strategies are recommended.","category":"page"},{"location":"MixtureModels/#Reference","page":"Mixture Models","title":"Reference","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"For comprehensive mathematical foundations and advanced topics in mixture models, we recommend:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Pattern Recognition and Machine Learning, Chapter 9 by Christopher Bishop\nThe Elements of Statistical Learning, Chapter 6 by Hastie, Tibshirani, and Friedman\nFinite Mixture Models by Geoffrey McLachlan and David Peel","category":"page"},{"location":"SLDS/#What-is-a-Switching-Linear-Dynamical-System?","page":"Switching Linear Dynamical Systems","title":"What is a Switching Linear Dynamical System?","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"A Switching Linear Dynamical System (SLDS) is a powerful probabilistic model that combines the temporal structure of linear dynamical systems with the discrete switching behavior of Hidden Markov Models. SLDS can model complex time series data that exhibits multiple dynamical regimes, where the system can switch between different linear dynamics over time.","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"An SLDS extends the standard Linear Dynamical System (LDS) by introducing a discrete latent state that determines which linear dynamics are active at each time step. This makes SLDS particularly suitable for modeling systems with:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Multiple operational modes (e.g., different flight phases of an aircraft)\nRegime changes (e.g., economic cycles, behavioral states)\nNon-stationary dynamics where linear dynamics change over time\nHybrid systems combining discrete and continuous states","category":"page"},{"location":"SLDS/#StateSpaceDynamics.SwitchingLinearDynamicalSystem","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.SwitchingLinearDynamicalSystem","text":"Switching Linear Dynamical System\n\nStruct to Encode a Hidden Markov model that switches among K distinct LinearDyanmicalSystems\n\nFields\n\nA::V: Transition matrix for mode switching. \nB::VL: Vector of Linear Dynamical System models. \nπₖ::V: Initial state distribution.\nK::Int: Number of modes. \n\n\n\n\n\n","category":"type"},{"location":"SLDS/#Mathematical-Formulation","page":"Switching Linear Dynamical Systems","title":"Mathematical Formulation","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"An SLDS with K discrete states is defined by the following generative model:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"beginalign*\n    s_1 sim textCat(pi_k) \n    z_1 sim mathcalN(mu_0 P_0) \n    s_t mid s_t-1 sim textCat(A_s_t-1 ) \n    z_t mid z_t-1 s_t sim mathcalN(F_s_t z_t-1 Q_s_t) \n    y_t mid z_t s_t sim mathcalN(C_s_t z_t R_s_t)\nendalign*","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"s_t in 1 2 ldots K\nis the discrete switching state at time t\nz_t in mathbbR^D\nis the continuous latent state at time t  \ny_t in mathbbR^P\nis the observed data at time t\npi_k\nis the initial discrete state distribution\nA\nis the discrete state transition matrix\nF_s_t\nis the state-dependent dynamics matrix for discrete state s_t\nQ_s_t\nis the state-dependent process noise covariance for discrete state s_t\nC_s_t\nis the state-dependent observation matrix for discrete state s_t\nR_s_t\nis the state-dependent observation noise covariance for discrete state s_t","category":"page"},{"location":"SLDS/#Implementation-Structure","page":"Switching Linear Dynamical Systems","title":"Implementation Structure","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"In StateSpaceDynamics.jl, an SLDS is represented as:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"mutable struct SwitchingLinearDynamicalSystem <: AbstractHMM\n    A::M     # Transition matrix for mode switching (K × K)\n    B::VL    # Vector of LinearDynamicalSystem models (length K)\n    πₖ::V    # Initial state distribution (length K)\n    K::Int   # Number of modes\nend","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Each mode in the B vector contains its own LinearDynamicalSystem with:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"State model: Defines the continuous latent dynamics F_k, Q_k\nObservation model: Defines the emission process C_k, R_k","category":"page"},{"location":"SLDS/#Sampling-from-SLDS","page":"Switching Linear Dynamical Systems","title":"Sampling from SLDS","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"You can generate synthetic data from an SLDS to test algorithms or create simulated datasets:","category":"page"},{"location":"SLDS/#Base.rand-Tuple{AbstractRNG, SwitchingLinearDynamicalSystem, Int64}","page":"Switching Linear Dynamical Systems","title":"Base.rand","text":"Random.rand(rng, slds, T)\n\nGenerate synthetic data with switching LDS models\n\n#Arguments \n\nrng:AbstractRNG: Random number generator\nslds::SwitchingLinearDynamicalSystem: The switching LDS model \nT::Int: Number of time steps to sample\n\nReturns\n\nTuple{Array,Array, Array}: Latent states (x), observations (y), and mode sequences (z). \n\n\n\n\n\n","category":"method"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"The sampling process follows the generative model:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Initialize: Sample initial discrete state from pi_k and initial continuous state\nFor each time step:\nSample next discrete state based on current state and transition matrix A\nSample continuous state using the dynamics of the current discrete state\nGenerate observation using the observation model of the current discrete state","category":"page"},{"location":"SLDS/#Learning-in-SLDS","page":"Switching Linear Dynamical Systems","title":"Learning in SLDS","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"StateSpaceDynamics.jl implements a Variational Expectation-Maximization (EM) algorithm for parameter estimation in SLDS. This approach handles the interaction between discrete and continuous latent variables efficiently.","category":"page"},{"location":"SLDS/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{AbstractHMM, AbstractMatrix{T}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(slds::SwitchingLinearDynamicalSystem, y::Matrix{T}; \n     max_iter::Int=1000, \n     tol::Real=1e-12, \n     ) where {T<:Real}\n\nFit a Switching Linear Dynamical System using the variational Expectation-Maximization (EM) algorithm with Kalman smoothing.\n\nArguments\n\nslds::SwitchingLinearDynamicalSystem: The Switching Linear Dynamical System to be fitted.\ny::Matrix{T}: Observed data, size (obsdim, Tsteps).\n\nKeyword Arguments\n\nmax_iter::Int=1000: Maximum number of EM iterations.\ntol::Real=1e-12: Convergence tolerance for log-likelihood change.\n\nReturns\n\nmls::Vector{T}: Vector of log-likelihood values for each iteration.\nparam_diff::Vector{T}: Vector of parameter differences over each iteration. \nFB::ForwardBackward: ForwardBackward struct \nFS::Vector{FilterSmooth}: Vector of FilterSmooth structs\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#Variational-EM-Algorithm","page":"Switching Linear Dynamical Systems","title":"Variational EM Algorithm","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"The variational EM algorithm maximizes the Evidence Lower Bound (ELBO) instead of the intractable marginal likelihood. The algorithm alternates between:","category":"page"},{"location":"SLDS/#Variational-Expectation-Step","page":"Switching Linear Dynamical Systems","title":"Variational Expectation Step","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"The E-step iteratively updates the variational distributions until convergence. This involves two coupled updates:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"1. Update continuous state posteriors (q(z_1T)): For each discrete state k, run weighted Kalman smoothing:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"q(z_1T mid s_1T = k) = prod_t=1^T mathcalN(z_t hatz_tT^(k) P_tT^(k))","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"2. Update discrete state posteriors (q(s_1T)): Run forward-backward algorithm with observation likelihoods computed from current continuous posteriors:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"q(s_t = k) = gamma_t(k) = p(s_t = k mid y_1T q(z_1T))","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"The E-step converges when the ELBO stabilizes, ensuring consistency between discrete and continuous posteriors.","category":"page"},{"location":"SLDS/#Maximization-Step","page":"Switching Linear Dynamical Systems","title":"Maximization Step","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"The M-step updates all model parameters using weighted maximum likelihood:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Discrete state parameters:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Initial distribution: pi_k^(new) = gamma_1(k)\nTransition matrix: A_ij^(new) = fracsum_t=1^T-1 xi_tt+1(ij)sum_t=1^T-1 gamma_t(i)","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Continuous state parameters for each mode k:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Using sufficient statistics from weighted Kalman smoothing:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"Dynamics matrix: F_k^(new) from weighted regression\nProcess covariance: Q_k^(new) from weighted residuals\nObservation matrix: C_k^(new) from weighted regression\nInitial state parameters: mu_0^(k) P_0^(k)","category":"page"},{"location":"SLDS/#Evidence-Lower-Bound-(ELBO)","page":"Switching Linear Dynamical Systems","title":"Evidence Lower Bound (ELBO)","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"The ELBO consists of contributions from both discrete and continuous components:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"textELBO = underbracemathbbE_q(s_1T)log p(s_1T) - mathbbE_q(s_1T)log q(s_1T)_textDiscrete HMM contribution + sum_k=1^K underbracemathbbE_q(z_1Ts_1T=k)log p(y_1T z_1T  s_1T=k) + Hq(z_1Ts_1T=k)_textContinuous LDS contribution for mode  k","category":"page"},{"location":"SLDS/#References","page":"Switching Linear Dynamical Systems","title":"References","text":"","category":"section"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"For theoretical foundations and algorithmic details:","category":"page"},{"location":"SLDS/","page":"Switching Linear Dynamical Systems","title":"Switching Linear Dynamical Systems","text":"\"Learning and Inference in Switching Linear Dynamical Systems\" by Zoubin Ghahramani and Geoffrey Hinton\n\"Variational Learning for Switching State-Space Models\" by Zoubin Ghahramani and Sam Roweis  \n\"A Unifying Review of Linear Gaussian Models\" by Sam Roweis and Zoubin Ghahramani\n\"Probabilistic Machine Learning: Advanced Topics, Chapter 8\" by Kevin Murphy","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Simulating-and-Fitting-a-Linear-Dynamical-System","page":"Gaussian LDS Example","title":"Simulating and Fitting a Linear Dynamical System","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to simulate a latent linear dynamical system and fit it using the EM algorithm. We'll walk through the complete workflow: defining a true model, generating synthetic data, initializing a naive model, and then learning the parameters through iterative optimization.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Load-Required-Packages","page":"Gaussian LDS Example","title":"Load Required Packages","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"We begin by loading all the necessary packages for our analysis. StateSpaceDynamics.jl provides the core functionality, while the other packages handle linear algebra, random number generation, plotting, and mathematical notation.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing LaTeXStrings\nusing StableRNGs","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Set a stable random number generator for reproducible results","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"rng = StableRNG(123);\nnothing #hide","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Create-a-State-Space-Model","page":"Gaussian LDS Example","title":"Create a State-Space Model","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"We start by defining the dimensions of our system. A linear dynamical system (LDS) models how a low-dimensional latent state evolves over time and generates high-dimensional observations. Here we use a 2D latent space (which we can visualize easily) that generates 10-dimensional observations.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"obs_dim = 10      # Number of observed variables at each time step\nlatent_dim = 2    # Number of latent state variables","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Define the state transition matrix A. This matrix governs how the latent state evolves from one time step to the next: x{t+1} = A * xt + noise. We create a rotation matrix scaled by 0.95, which creates a stable spiral dynamic that slowly contracts toward the origin.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"A = 0.95 * [cos(0.25) -sin(0.25); sin(0.25) cos(0.25)]","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Process noise covariance Q controls how much random variation we add to the latent state transitions. A smaller Q means more predictable dynamics.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Q = Matrix(0.1 * I(2))","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Initial state parameters: where the latent trajectory starts and how uncertain we are about this initial position.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"x0 = [0.0; 0.0]           # Mean of initial state\nP0 = Matrix(0.1 * I(2))   # Covariance of initial state","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Observation parameters: how the latent states map to observed data. C is the observation matrix (latent-to-observed mapping), and R is the observation noise covariance.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"C = randn(rng, obs_dim, latent_dim)  # Random linear mapping from 2D latent to 10D observed\nR = Matrix(0.5 * I(obs_dim))         # Independent noise on each observation dimension","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Construct the state and observation model components","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"true_gaussian_sm = GaussianStateModel(;A=A, Q=Q, x0=x0, P0=P0)\ntrue_gaussian_om = GaussianObservationModel(;C=C, R=R)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Combine them into a complete Linear Dynamical System The fit_bool parameter indicates which parameters should be learned during fitting","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"true_lds = LinearDynamicalSystem(;\n    state_model=true_gaussian_sm,\n    obs_model=true_gaussian_om,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)  # Fit all 6 parameter matrices: A, Q, C, R, x0, P0\n)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Simulate-Latent-and-Observed-Data","page":"Gaussian LDS Example","title":"Simulate Latent and Observed Data","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Now we generate synthetic data from our true model. This gives us both the latent states (which we'll later try to recover) and the observations (which is all a real algorithm would see).","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"tSteps = 500  # Number of time points to simulate","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"The rand function generates both latent trajectories and corresponding observations","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"latents, observations = rand(rng, true_lds; tsteps=tSteps, ntrials=1)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Plot-Vector-Field-of-Latent-Dynamics","page":"Gaussian LDS Example","title":"Plot Vector Field of Latent Dynamics","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"To better understand the dynamics encoded by our transition matrix A, we'll create a vector field plot. This shows how the latent state would evolve from any starting point in the 2D latent space.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Create a grid of starting points","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"x = y = -3:0.5:3\nX = repeat(x', length(y), 1)\nY = repeat(y, 1, length(x))","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Calculate the flow field: at each point (x,y), compute where it would move in one time step under the dynamics x{t+1} = A * xt","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"U = zeros(size(X))  # x-component of flow\nV = zeros(size(Y))  # y-component of flow\n\nfor i in 1:size(X, 1)\n    for j in 1:size(X, 2)\n        v = A * [X[i,j], Y[i,j]]\n        U[i,j] = v[1] - X[i,j]  # Change in x\n        V[i,j] = v[2] - Y[i,j]  # Change in y\n    end\nend","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Normalize arrows for cleaner visualization","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"magnitude = @. sqrt(U^2 + V^2)\nU_norm = U ./ magnitude\nV_norm = V ./ magnitude","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Create the vector field plot with the actual trajectory overlaid","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"p = quiver(X, Y, quiver=(U_norm, V_norm), color=:blue, alpha=0.3,\n           linewidth=1, arrow=arrow(:closed, :head, 0.1, 0.1))\nplot!(latents[1, :, 1], latents[2, :, 1], xlabel=\"x₁\", ylabel=\"x₂\",\n      color=:black, linewidth=1.5, title=\"Latent Dynamics\", legend=false)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Plot-Latent-States-and-Observations","page":"Gaussian LDS Example","title":"Plot Latent States and Observations","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Let's visualize both the latent states (which evolve smoothly according to our dynamics) and the observations (which are noisy linear combinations of the latents).","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"states = latents[:, :, 1]      # Extract the latent trajectory\nemissions = observations[:, :, 1]  # Extract the observed data","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Create a two-panel plot: latent states on top, observations below","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"plot(size=(800, 600), layout=@layout[a{0.3h}; b])","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Plot latent states (offset vertically for clarity)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"lim_states = maximum(abs.(states))\nfor d in 1:latent_dim\n    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black,\n          linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, tSteps), title=\"Simulated Latent States\",\n      yformatter=y->\"\", tickfontsize=12)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Plot observations (also offset vertically since there are many dimensions)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"lim_emissions = maximum(abs.(emissions))\nfor n in 1:obs_dim\n    plot!(1:tSteps, emissions[n, :] .- lim_emissions * (n-1), color=:black,\n          linewidth=2, label=\"\", subplot=2)\nend\n\nplot!(subplot=2, yticks=(-lim_emissions .* (obs_dim-1:-1:0), [L\"y_{%$n}\" for n in 1:obs_dim]),\n      xlabel=\"time\", xlims=(0, tSteps), title=\"Simulated Emissions\",\n      yformatter=y->\"\", tickfontsize=12)\n\nplot!(link=:x, size=(800, 600), left_margin=10Plots.mm)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Initialize-a-Model-and-Perform-Smoothing","page":"Gaussian LDS Example","title":"Initialize a Model and Perform Smoothing","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"In a real scenario, we would only observe the emissions, not the latent states. Our goal is to learn the parameters A, Q, C, R from the observations alone. We start by creating a \"naive\" model with random initial parameters.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Initialize with random parameters (this simulates not knowing the true system)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"A_init = random_rotation_matrix(2, rng)    # Random rotation matrix for dynamics\nQ_init = Matrix(0.1 * I(2))                # Same process noise variance (could be random too)\nC_init = randn(rng, obs_dim, latent_dim)   # Random observation mapping\nR_init = Matrix(0.5 * I(obs_dim))          # Same observation noise (could vary)\nx0_init = zeros(latent_dim)                # Start from origin\nP0_init = Matrix(0.1 * I(latent_dim))      # Same initial uncertainty","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Create the naive model components","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"gaussian_sm_init = GaussianStateModel(;A=A_init, Q=Q_init, x0=x0_init, P0=P0_init)\ngaussian_om_init = GaussianObservationModel(;C=C_init, R=R_init)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Assemble the complete naive system","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"naive_ssm = LinearDynamicalSystem(;\n    state_model=gaussian_sm_init,\n    obs_model=gaussian_om_init,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)  # We'll learn all parameters\n)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Before fitting, let's see how well our randomly initialized model can infer the latent states. We use the \"smoothing\" algorithm, which estimates the latent states given all observations (past, present, and future).","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"x_smooth, p_smooth = StateSpaceDynamics.smooth(naive_ssm, observations)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Plot the true latent states vs. our initial (poor) estimates","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"plot()\nfor d in 1:latent_dim\n    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:tSteps, x_smooth[d, :, 1] .+ lim_states * (d-1), color=:firebrick, linewidth=2, label=\"\", subplot=1)\nend\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, tSteps), yformatter=y->\"\", tickfontsize=12,\n      title=\"True vs. Predicted Latent States (Pre-EM)\")","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Fit-Model-Using-EM-Algorithm","page":"Gaussian LDS Example","title":"Fit Model Using EM Algorithm","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Now comes the crucial step: parameter learning via the Expectation-Maximization (EM) algorithm. EM alternates between two steps:","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"E-step: Estimate latent states given current parameters\nM-step: Update parameters given current state estimates","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"This process iteratively improves both the parameter estimates and state inferences.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"println(\"Starting EM algorithm to learn parameters...\")\nelbo, _ = fit!(naive_ssm, observations; max_iter=100, tol=1e-6)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"After EM has converged, let's see how much better our latent state estimates are","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"x_smooth, p_smooth = StateSpaceDynamics.smooth(naive_ssm, observations)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"Plot the results: true states vs. post-EM estimates","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"plot()\nfor d in 1:latent_dim\n    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:tSteps, x_smooth[d, :, 1] .+ lim_states * (d-1), color=:firebrick, linewidth=2, label=\"\", subplot=1)\nend\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, tSteps), yformatter=y->\"\", tickfontsize=12,\n      title=\"True vs. Predicted Latent States (Post-EM)\")","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Confirm-Model-Convergence","page":"Gaussian LDS Example","title":"Confirm Model Convergence","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"The Evidence Lower BOund (ELBO) is a measure of how well our model explains the data. In EM, this should increase monotonically and plateau when the algorithm has converged to a local optimum.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"plot(elbo, xlabel=\"iteration\", ylabel=\"ELBO\", title=\"ELBO (Marginal Loglikelihood)\", legend=false)\n\nprintln(\"EM converged after $(length(elbo)) iterations\")\nprintln(\"Final ELBO: $(elbo[end])\")","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Summary","page":"Gaussian LDS Example","title":"Summary","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"This tutorial demonstrated the complete workflow for fitting a Linear Dynamical System:","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"We defined a true LDS with known parameters and generated synthetic data\nWe initialized a naive model with random parameters\nWe used EM to iteratively improve our parameter estimates\nWe visualized how the latent state inference improved after learning","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"The EM algorithm successfully recovered the underlying dynamics from observations alone, as evidenced by the improved match between true and estimated latent states and the convergence of the ELBO objective function.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Simulating-and-Fitting-a-Hidden-Markov-Model","page":"Gaussian GLM-GMM Example","title":"Simulating and Fitting a Hidden Markov Model","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create, sample from, and fit Hidden Markov Models (HMMs). Unlike Linear Dynamical Systems which have continuous latent states, HMMs have discrete latent states that switch between a finite number of modes. This makes them ideal for modeling data with distinct behavioral regimes, switching dynamics, or categorical latent structure.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"We'll focus on a Gaussian generalized linear model HMM (GLM-HMM), where each hidden state corresponds to a different regression relationship between inputs and outputs. This is particularly useful for modeling data where the input-output relationship changes over time in discrete jumps.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Load-Required-Packages","page":"Gaussian GLM-GMM Example","title":"Load Required Packages","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"We load the essential packages for HMM modeling, visualization, and reproducible random sampling.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"using LinearAlgebra\nusing Plots\nusing Random\nusing StateSpaceDynamics\nusing StableRNGs\nusing Statistics: mean","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Set up reproducible random number generation","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Create-a-Gaussian-Generalized-Linear-Model-Hidden-Markov-Model-(GLM-HMM)","page":"Gaussian GLM-GMM Example","title":"Create a Gaussian Generalized Linear Model-Hidden Markov Model (GLM-HMM)","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"In a GLM-HMM, each hidden state defines a different regression model. The system switches between these regression models according to Markovian dynamics. This is useful for modeling scenarios where the relationship between predictors and outcomes changes over time.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Define emission models for each hidden state State 1: Positive relationship between input and output","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"emission_1 = GaussianRegressionEmission(\n    input_dim=3,                                    # Number of input features\n    output_dim=1,                                   # Number of output dimensions\n    include_intercept=true,                         # Include intercept term\n    β=reshape([3.0, 2.0, 2.0, 3.0], :, 1),        # Regression coefficients [intercept, β1, β2, β3]\n    Σ=[1.0;;],                                     # Observation noise variance\n    λ=0.0                                          # Regularization parameter\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"State 2: Different relationship (negative intercept, different slopes)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"emission_2 = GaussianRegressionEmission(\n    input_dim=3,\n    output_dim=1,\n    include_intercept=true,\n    β=reshape([-4.0, -2.0, 3.0, 2.0], :, 1),      # Different regression coefficients\n    Σ=[1.0;;],                                     # Same noise level\n    λ=0.0\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Define the state transition matrix A A[i,j] = probability of transitioning from state i to state j Diagonal elements are high (states are persistent), off-diagonal elements are low","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"A = [0.99 0.01;    # From state 1: 99% stay, 1% switch to state 2\n     0.05 0.95]    # From state 2: 5% switch to state 1, 95% stay","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Initial state distribution: probability of starting in each state","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"πₖ = [0.8; 0.2]    # 80% chance of starting in state 1, 20% in state 2","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Construct the complete HMM","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"true_model = HiddenMarkovModel(\n    K=2,                        # Number of hidden states\n    A=A,                        # Transition matrix\n    πₖ=πₖ,                     # Initial state distribution\n    B=[emission_1, emission_2]  # Emission models for each state\n)\n\nprintln(\"Created GLM-HMM with 2 states and 3 input features\")\nprintln(\"State 1 regression: y = 3.0 + 2.0*x₁ + 2.0*x₂ + 3.0*x₃ + ε\")\nprintln(\"State 2 regression: y = -4.0 - 2.0*x₁ + 3.0*x₂ + 2.0*x₃ + ε\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Sample-from-the-GLM-HMM","page":"Gaussian GLM-GMM Example","title":"Sample from the GLM-HMM","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Generate synthetic data from our true model. This will give us both the observed data (inputs and outputs) and the true hidden state sequence, which we'll use to evaluate our parameter recovery.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"n = 20000  # Number of time points\nprintln(\"Generating $n samples from the GLM-HMM...\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Generate random input features (predictors)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Φ = randn(rng, 3, n)  # 3 features × n time points","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Sample from the HMM: returns both hidden states and observations","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"true_labels, data = rand(rng, true_model, Φ, n=n)\n\nprintln(\"Generated data summary:\")\nprintln(\"  - Input features shape: $(size(Φ))\")\nprintln(\"  - Output data shape: $(size(data))\")\nprintln(\"  - True labels shape: $(size(true_labels))\")\nprintln(\"  - State 1 proportion: $(round(mean(true_labels .== 1), digits=3))\")\nprintln(\"  - State 2 proportion: $(round(mean(true_labels .== 2), digits=3))\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-the-Sampled-Dataset","page":"Gaussian GLM-GMM Example","title":"Visualize the Sampled Dataset","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Create a scatter plot showing how the input-output relationship differs between the two hidden states. Points are colored by their true hidden state.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"colors = [:dodgerblue, :crimson]  # Blue for state 1, red for state 2\n\nscatter(Φ[1, :], vec(data);\n    color = colors[true_labels],\n    ms = 3,\n    label = \"\",\n    xlabel = \"Input Feature 1\",\n    ylabel = \"Output\",\n    title = \"GLM-HMM Sampled Data (colored by true state)\",\n    alpha = 0.6\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Overlay the true regression lines for each state We'll plot the relationship between feature 1 and output, holding other features at 0","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"xvals = range(minimum(Φ[1, :]), stop=maximum(Φ[1, :]), length=100)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"State 1 regression line: y = β₀ + β₁*x₁ (setting x₂=x₃=0)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"β1 = emission_1.β[:, 1]\ny_pred_1 = β1[1] .+ β1[2] .* xvals  # intercept + slope*x₁\nplot!(xvals, y_pred_1;\n    color = :dodgerblue,\n    lw = 3,\n    label = \"State 1 regression\",\n    legend = :topright,\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"State 2 regression line","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"β2 = emission_2.β[:, 1]\ny_pred_2 = β2[1] .+ β2[2] .* xvals  # intercept + slope*x₁\nplot!(xvals, y_pred_2;\n    color = :crimson,\n    lw = 3,\n    label = \"State 2 regression\",\n    legend = :topright,\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Initialize-and-Fit-a-New-HMM-to-the-Sampled-Data","page":"Gaussian GLM-GMM Example","title":"Initialize and Fit a New HMM to the Sampled Data","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Now we'll pretend we don't know the true parameters and try to learn them from the observed data alone. We start with a randomly initialized HMM and use the Expectation-Maximization (EM) algorithm to learn the parameters.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"println(\"Initializing naive HMM with random parameters...\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Initialize with different parameters than the true model","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"A = [0.8 0.2; 0.1 0.9]          # Different transition probabilities\nπₖ = [0.6; 0.4]                 # Different initial distribution","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Initialize emission models with random regression coefficients","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"emission_1 = GaussianRegressionEmission(\n    input_dim=3, output_dim=1, include_intercept=true,\n    β=reshape([2.0, -1.0, 1.0, 2.0], :, 1),    # Random coefficients\n    Σ=[2.0;;],                                  # Different noise variance\n    λ=0.0\n)\n\nemission_2 = GaussianRegressionEmission(\n    input_dim=3, output_dim=1, include_intercept=true,\n    β=reshape([-2.5, -1.0, 3.5, 3.0], :, 1),   # Random coefficients\n    Σ=[0.5;;],                                  # Different noise variance\n    λ=0.0\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Create the test model with naive initialization","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"test_model = HiddenMarkovModel(K=2, A=A, πₖ=πₖ, B=[emission_1, emission_2])","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Fit the model using EM algorithm","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"println(\"Running EM algorithm to learn HMM parameters...\")\nlls = fit!(test_model, data, Φ)\n\nprintln(\"EM converged after $(length(lls)) iterations\")\nprintln(\"Initial log-likelihood: $(round(lls[1], digits=2))\")\nprintln(\"Final log-likelihood: $(round(lls[end], digits=2))\")\nprintln(\"Improvement: $(round(lls[end] - lls[1], digits=2))\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Plot the convergence of the log-likelihood","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"plot(lls)\ntitle!(\"Log-likelihood over EM Iterations\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-the-Emission-Model-Predictions","page":"Gaussian GLM-GMM Example","title":"Visualize the Emission Model Predictions","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Compare the true regression relationships with what our fitted model learned. This shows how well we recovered the underlying GLM parameters.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"state_colors = [:dodgerblue, :crimson]  # Data points colored by true state\ntrue_colors = [:green, :orange]         # True regression lines\npred_colors = [:teal, :yellow]          # Predicted regression lines\n\nscatter(Φ[1, :], vec(data);\n    color = state_colors[true_labels],\n    ms = 3,\n    alpha = 0.6,\n    label = \"\",\n    xlabel = \"Input Feature 1\",\n    ylabel = \"Output\",\n    title = \"True vs. Predicted Regression Relationships\"\n)\n\nxvals = range(minimum(Φ[1, :]), stop=maximum(Φ[1, :]), length=100)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Plot true regression lines","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"β1_true = emission_1.β[:, 1]  # Note: this is now the fitted model's β, not true model's\ny_true_1 = β1_true[1] .+ β1_true[2] .* xvals\nplot!(xvals, y_true_1;\n    color = true_colors[1],\n    lw = 3,\n    linestyle = :solid,\n    label = \"State 1 (true)\"\n)\n\nβ2_true = emission_2.β[:, 1]\ny_true_2 = β2_true[1] .+ β2_true[2] .* xvals\nplot!(xvals, y_true_2;\n    color = true_colors[2],\n    lw = 3,\n    linestyle = :solid,\n    label = \"State 2 (true)\"\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Plot learned regression lines","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"β1_pred = test_model.B[1].β[:, 1]\ny_pred_1 = β1_pred[1] .+ β1_pred[2] .* xvals\nplot!(xvals, y_pred_1;\n    color = pred_colors[1],\n    lw = 3,\n    linestyle = :dash,\n    label = \"State 1 (learned)\"\n)\n\nβ2_pred = test_model.B[2].β[:, 1]\ny_pred_2 = β2_pred[1] .+ β2_pred[2] .* xvals\nplot!(xvals, y_pred_2;\n    color = pred_colors[2],\n    lw = 3,\n    linestyle = :dash,\n    label = \"State 2 (learned)\"\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-the-Latent-State-Predictions-using-Viterbi-Algorithm","page":"Gaussian GLM-GMM Example","title":"Visualize the Latent State Predictions using Viterbi Algorithm","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"The Viterbi algorithm finds the most likely sequence of hidden states given the observed data. We'll compare the true hidden state sequence with our predictions.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"println(\"Running Viterbi algorithm to decode hidden state sequence...\")\npred_labels = viterbi(test_model, data, Φ);\nnothing #hide","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Calculate accuracy of state prediction","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"accuracy = mean(true_labels .== pred_labels)\nprintln(\"Hidden state prediction accuracy: $(round(accuracy*100, digits=1))%\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Visualize a subset of the state sequences as heatmaps","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"true_mat = reshape(true_labels[1:1000], 1, :)\npred_mat = reshape(pred_labels[1:1000], 1, :)\n\np1 = heatmap(true_mat;\n    colormap = :roma50,\n    title = \"True State Labels\",\n    xlabel = \"\",\n    ylabel = \"\",\n    xticks = false,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\np2 = heatmap(pred_mat;\n    colormap = :roma50,\n    title = \"Predicted State Labels (Viterbi)\",\n    xlabel = \"Timepoints (1-1000)\",\n    ylabel = \"\",\n    xticks = 0:200:1000,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\nplot(p1, p2;\n    layout = (2, 1),\n    size = (700, 500),\n    margin = 5Plots.mm)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Sampling-Multiple,-Independent-Trials-of-Data-from-an-HMM","page":"Gaussian GLM-GMM Example","title":"Sampling Multiple, Independent Trials of Data from an HMM","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Real-world scenarios often involve multiple independent sequences (e.g., multiple subjects, experimental sessions, or trials). We'll generate multiple independent sequences and show how to fit HMMs to this type of data structure.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"println(\"Generating multiple independent trials...\")\n\nall_data = Vector{Matrix{Float64}}()     # Store data from each trial\nΦ_total = Vector{Matrix{Float64}}()      # Store input features from each trial\nall_true_labels = []                     # Store true state sequences\n\nnum_trials = 100  # Number of independent sequences\nn = 1000         # Length of each sequence\n\nfor i in 1:num_trials\n    Φ = randn(rng, 3, n)\n    true_labels, data = rand(rng, true_model, Φ, n=n)\n    push!(all_true_labels, true_labels)\n    push!(all_data, data)\n    push!(Φ_total, Φ)\nend\n\nprintln(\"Generated $num_trials independent trials, each with $n time points\")\nprintln(\"Total data points: $(num_trials * n)\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Fitting-an-HMM-to-Multiple,-Independent-Trials-of-Data","page":"Gaussian GLM-GMM Example","title":"Fitting an HMM to Multiple, Independent Trials of Data","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"When we have multiple independent sequences, the EM algorithm needs to account for the fact that each sequence starts fresh from the initial state distribution. This provides more robust parameter estimates than fitting to a single long sequence.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"println(\"Fitting HMM to multiple independent trials...\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Initialize a new model for multi-trial fitting","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"A = [0.8 0.2; 0.1 0.9]\nπₖ = [0.6; 0.4]\nemission_1 = GaussianRegressionEmission(\n    input_dim=3, output_dim=1, include_intercept=true,\n    β=reshape([2.0, -1.0, 1.0, 2.0], :, 1), Σ=[2.0;;], λ=0.0\n)\nemission_2 = GaussianRegressionEmission(\n    input_dim=3, output_dim=1, include_intercept=true,\n    β=reshape([-2.5, -1.0, 3.5, 3.0], :, 1), Σ=[0.5;;], λ=0.0\n)\n\ntest_model = HiddenMarkovModel(K=2, A=A, πₖ=πₖ, B=[emission_1, emission_2])","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Fit to multiple trials - the package handles the multi-trial structure automatically","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"lls = fit!(test_model, all_data, Φ_total)\n\nprintln(\"Multi-trial EM converged after $(length(lls)) iterations\")\nprintln(\"Initial log-likelihood: $(round(lls[1], digits=2))\")\nprintln(\"Final log-likelihood: $(round(lls[end], digits=2))\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Plot convergence for multi-trial fitting","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"plot(lls)\ntitle!(\"Log-likelihood over EM Iterations (Multi-Trial)\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-Latent-State-Predictions-for-Multiple-Trials-using-Viterbi","page":"Gaussian GLM-GMM Example","title":"Visualize Latent State Predictions for Multiple Trials using Viterbi","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Decode hidden states for all trials and visualize the results as a multi-trial heatmap. This shows how well we can predict state sequences across different independent runs.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"println(\"Running Viterbi decoding on all trials...\")\nall_pred_labels_vec = viterbi(test_model, all_data, Φ_total)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Reshape for visualization","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"all_pred_labels = hcat(all_pred_labels_vec...)'      # trials × time\nall_true_labels_matrix = hcat(all_true_labels...)'   # trials × time","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Calculate overall accuracy across all trials","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"total_accuracy = mean(all_true_labels_matrix .== all_pred_labels)\nprintln(\"Overall hidden state accuracy across all trials: $(round(total_accuracy*100, digits=1))%\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Visualize a subset of trials","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"state_colors = [:dodgerblue, :crimson]\ntrue_subset = all_true_labels_matrix[1:10, 1:500]   # First 10 trials, first 500 time points\npred_subset = all_pred_labels[1:10, 1:500]\n\np1 = heatmap(\n    true_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"True State Labels (10 trials × 500 timepoints)\",\n    xlabel = \"\",\n    ylabel = \"Trial Number\",\n    xticks = false,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\np2 = heatmap(\n    pred_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"Predicted State Labels (Viterbi)\",\n    xlabel = \"Timepoints\",\n    ylabel = \"Trial Number\",\n    xticks = true,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\nfinal_plot = plot(\n    p1, p2,\n    layout = (2, 1),\n    size = (850, 550),\n    margin = 5Plots.mm,\n    legend = false,\n)\n\ndisplay(final_plot)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Summary-and-Model-Assessment","page":"Gaussian GLM-GMM Example","title":"Summary and Model Assessment","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"println(\"\\n=== Final Model Assessment ===\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Compare learned parameters with true parameters","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"true_A = [0.99 0.01; 0.05 0.95]\nlearned_A = test_model.A\nA_error = norm(true_A - learned_A) / norm(true_A)\nprintln(\"Transition matrix relative error: $(round(A_error, digits=4))\")\n\ntrue_π = [0.8; 0.2]\nlearned_π = test_model.πₖ\nπ_error = norm(true_π - learned_π) / norm(true_π)\nprintln(\"Initial distribution relative error: $(round(π_error, digits=4))\")\n\nprintln(\"\\nTrue vs Learned Regression Coefficients:\")\nprintln(\"State 1 - True β: [3.0, 2.0, 2.0, 3.0]\")\nprintln(\"State 1 - Learned β: $(round.(test_model.B[1].β[:, 1], digits=2))\")\nprintln(\"State 2 - True β: [-4.0, -2.0, 3.0, 2.0]\")\nprintln(\"State 2 - Learned β: $(round.(test_model.B[2].β[:, 1], digits=2))\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Summary","page":"Gaussian GLM-GMM Example","title":"Summary","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"This tutorial demonstrated the complete workflow for Hidden Markov Models with regression emissions:","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"Model Structure: Discrete latent states with different regression relationships in each state\nApplications: Ideal for modeling switching dynamics, regime changes, or context-dependent relationships\nSingle vs Multiple Trials: Showed how to handle both single long sequences and multiple independent trials\nParameter Recovery: EM algorithm successfully learned transition dynamics and emission parameters\nState Decoding: Viterbi algorithm accurately recovered hidden state sequences\nScalability: Framework handles multiple trials efficiently for robust parameter estimation","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"GLM-HMMs provide a powerful framework for modeling data with discrete latent structure and context-dependent input-output relationships, making them valuable for many real-world applications.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"HiddenMarkovModels/#What-is-a-Hidden-Markov-Model?","page":"Hidden Markov Models","title":"What is a Hidden Markov Model?","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model (HMM) is a graphical model that describes how systems change over time. When modeling a time series with T observations using an HMM, we assume that the observed data y_1T depends on hidden states x_1T that are not observed. Specifically, an HMM is a type of state-space model in which the hidden states are discrete.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The three components of an HMM are as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"An initial state distribution (pi): which hidden states we are likely to start in.\nA transition matrix (A): how the hidden states evolve over time.\nAn emission model: how the hidden states generate the observed data.","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.HiddenMarkovModel","page":"Hidden Markov Models","title":"StateSpaceDynamics.HiddenMarkovModel","text":"HiddenMarkovModel\n\nStore a Hidden Markov Model (HMM) with custom emissions.\n\nFields\n\nA::AbstractMatrix{<:Real}: Transition matrix.\nB::AbstractVector{<:EmissionModel}: State-dependent emission models.\nπₖ::AbstractVector{<:Real}: Initial state distribution.\nK::Int: Number of states.\n\n\n\n\n\n","category":"type"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is given by:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beginalign*\n    x_1 sim textCat(pi) \n    x_t mid x_t-1 sim textCat(A_x_t-1 ) \n    y_t mid x_t sim p(y_t mid theta_x_t)\nendalign*","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t is the hidden (discrete) state at time t\ny_t is the observed data at time t\npi is the initial state distribution\nmathbfA is the state transition matrix\ntheta_x_t are the parameters of the emission distribution for state x_t","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The emission model can take many forms: Gaussian, Poisson, Bernoulli, categorical, etc... In the case of a Gaussian emission distribution, this becomes:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) sim mathcalN(mu_k Sigma_k)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"mu_k is the mean of the emission distribution for state k\nSigma_k is the covariance of the emission distribution for state k","category":"page"},{"location":"HiddenMarkovModels/#What-is-a-Generalized-Linear-Model-Hidden-Markov-Model","page":"Hidden Markov Models","title":"What is a Generalized Linear Model - Hidden Markov Model","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model - Generalized Linear Model (GLM-HMM) - also known as Switching Regression Model - is an extension to classic HMMs where the emission models are state-dependent GLMs that link an observed input to an observed output. This formulation allows each hidden state to define its own regression relationship between inputs and outputs, enabling the model to capture complex, state-dependent dynamics in the data. Currently, StateSpaceDynamics.jl support Gaussian, Bernoulli, Poisson, and Autoregressive GLMs as emission models.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beginalign*\n    x_1 sim textCat(pi) \n    x_t mid x_t-1 sim textCat(A_x_t-1 ) \n    y_t mid x_t u_t sim p(y_t mid theta_x_t u_t)\nendalign*","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t is the hidden (discrete) state at time t\ny_t is the observed output at time t\nu_t is the observed input (covariate) at time t\ntheta_x_t are the parameters of the GLM emission model for state x_t","category":"page"},{"location":"HiddenMarkovModels/#Example-Emission-Models","page":"Hidden Markov Models","title":"Example Emission Models","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For example, if the emission is a Gaussian GLM:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) u_t sim mathcalN(mu_k + beta_k^top u_t sigma_k^2)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k are the regression weights for state k\nsigma_k^2 is the state-dependent variance\nmu_k is the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"If the emission is Bernoulli (for binary outputs):","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) u_t sim textBernoulli left( sigma left( mu_k + beta_k^top u_t right) right)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k are the regression weights for state k\nsigma(cdot) is the logistic sigmoid function for binary outputs\nmu_k is the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/#Sampling-from-an-HMM","page":"Hidden Markov Models","title":"Sampling from an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"You can generate synthetic data from an HMM:","category":"page"},{"location":"HiddenMarkovModels/#Base.rand","page":"Hidden Markov Models","title":"Base.rand","text":"Random.rand(\n    rng::AbstractRNG,\n    model::HiddenMarkovModel,\n    X::Union{Matrix{<:Real}, Nothing}=nothing;\n    n::Int,\n    autoregressive::Bool=false)\n\nGenerate n samples from a Hidden Markov Model. Returns a tuple of the state sequence and the observation sequence.\n\nArguments\n\nrng::AbstractRNG: The seed.\nmodel::HiddenMarkovModel: The Hidden Markov Model to sample from.\nX: The input data for switching regression models.\nn::Int: The number of samples to generate.\n\nReturns\n\nstate_sequence::Vector{Int}: The state sequence, where each element is an integer 1:K.\nobservation_sequence::Matrix{Float64}: The observation sequence. This takes the form of the emission model's output.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#Learning-in-an-HMM","page":"Hidden Markov Models","title":"Learning in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"StateSpaceDynamics.jl implements Expectation-Maximization (EM) for parameter learning in both HMMs and GLM-HMMs. EM is an iterative method for finding maximum likelihood estimates of the parameters in graphical models with hidden variables. ","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{<:Real}}}} where T<:Real","page":"Hidden Markov Models","title":"StateSpaceDynamics.fit!","text":"fit!(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real}, Nothing}=nothing; max_iters::Int=100, tol::Float64=1e-6)\n\nFit the Hidden Markov Model using the EM algorithm.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data.\nX::Union{Matrix{<:Real}, Nothing}=nothing: Optional input data for fitting Switching Regression Models\nmax_iters::Int=100: The maximum number of iterations to run the EM algorithm.\ntol::Float64=1e-6: When the log likelihood is improving by less than this value, the algorithm will stop.\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#Expectation-Step-(E-step)","page":"Hidden Markov Models","title":"Expectation Step (E-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the expectation step (E-step), we calculate the posterior distribution of the latent states given the current parameters of the model:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"p(X mid Y theta_textold)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"We use dynamic programming to efficiently calculate this posterior using the forward and backward recursions for HMMs. This posterior is then used to construct the expectation of the complete data log-likelihood, also known as the Q-function:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Q(theta theta_textold) = sum_X p(X mid Y theta_textold) ln p(Y X mid theta)","category":"page"},{"location":"HiddenMarkovModels/#Maximization-Step-(M-step)","page":"Hidden Markov Models","title":"Maximization Step (M-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the maximization step (M-step), we maximize this expectation with respect to the parameters theta. Specifically:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For the initial state distribution and the transition matrix, we use analytical updates for the parameters, derived using Lagrange multipliers.\nFor emission models in the case of HMMs, we also implement analytical updates.\nIf the emission model is a GLM, we use Optim.jl to numerically optimize the objective function.","category":"page"},{"location":"HiddenMarkovModels/#Inference-in-an-HMM","page":"Hidden Markov Models","title":"Inference in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For state inference in Hidden Markov Models (HMMs), we implement two common algorithms:","category":"page"},{"location":"HiddenMarkovModels/#Forward-Backward-Algorithm","page":"Hidden Markov Models","title":"Forward-Backward Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Forward-Backward algorithm is used to compute the posterior state probabilities at each time step. Given the observed data, it calculates the probability of being in each possible hidden state at each time step, marginalizing over all possible state sequences.","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.class_probabilities-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{<:Real}}}} where T<:Real","page":"Hidden Markov Models","title":"StateSpaceDynamics.class_probabilities","text":"function class_probabilities(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nCalculate the class probabilities at each time point using forward backward algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nclass_probabilities::Matrix{Float64}: The class probabilities at each timepoint\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#Viterbi-Algorithm","page":"Hidden Markov Models","title":"Viterbi Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Viterbi algorithm is used for best state sequence labeling. It finds the most likely sequence of hidden states given the observed data. This is done by dynamically computing the highest probability path through the state space, which maximizes the likelihood of the observed sequence.","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.viterbi-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{<:Real}}}} where T<:Real","page":"Hidden Markov Models","title":"StateSpaceDynamics.viterbi","text":"viterbi(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nGet most likely class labels using the Viterbi algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nbest_path::Vector{Float64}: The most likely state label at each timepoint\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#Reference","page":"Hidden Markov Models","title":"Reference","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For a complete mathematical formulation of the relevant HMM and HMM-GLM learning and inference algorithms, we recommend Pattern Recognition and Machine Learning, Chapter 13 by Christopher Bishop.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Simulating-and-Fitting-a-Probabilistic-PCA-(PPCA)-Model","page":"Probabilistic PCA Example","title":"Simulating and Fitting a Probabilistic PCA (PPCA) Model","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"This tutorial walks through Probabilistic PCA (PPCA) in StateSpaceDynamics.jl: simulating data, fitting with EM, and interpreting the results. PPCA is a maximum-likelihood, probabilistic version of PCA with a simple latent-variable generative model and an explicit noise model.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#The-PPCA-model-at-a-glance","page":"Probabilistic PCA Example","title":"The PPCA model at a glance","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"The generative story for (x\\in\\mathbb R^D) with (k) latent factors: [ z \\sim \\mathcal N(0, Ik),\\qquad x \\mid z \\sim \\mathcal N(\\mu + W z,\\ \\sigma^2 ID), ] where (W\\in\\mathbb R^{D\\times k}) (factor loadings), (\\mu\\in\\mathbb R^D), and (\\sigma^2>0) (isotropic noise). The marginal covariance is (\\operatorname{Cov}(x)=W W^\\top + \\sigma^2 I). When (\\sigma^2\\to 0), PPCA approaches standard PCA. Rotations of (W) ((W R) for orthogonal (R)) span the same principal subspace—this is the usual rotational non-identifiability.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Posterior over latents. Given an observed (x), the posterior is Gaussian with [ M = I_k + \\tfrac{1}{\\sigma^2} W^\\top W,\\qquad \\mathbb E[z\\mid x] = M^{-1} W^\\top (x-\\mu)/\\sigma^2,\\qquad \\operatorname{Cov}(z\\mid x) = M^{-1}. ] fit! in StateSpaceDynamics.jl performs EM to maximize the likelihood.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Load-Packages","page":"Probabilistic PCA Example","title":"Load Packages","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing StatsPlots\nusing StableRNGs\nusing Distributions","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Reproducible randomness for simulation and initialization.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Create-a-PPCA-model-and-simulate","page":"Probabilistic PCA Example","title":"Create a PPCA model and simulate","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"We'll work in 2D with two latent factors for easy visualization.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"D = 2\nk = 2","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"True parameters used to generate synthetic data","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"W_true = [\n   -1.64   0.2;\n    0.9   -2.8\n]\n\nσ²_true = 0.5\nμ_true  = [1.65, -1.3]\n\nppca = ProbabilisticPCA(W_true, σ²_true, μ_true)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Draw IID samples from the model","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"num_obs = 500\nX, z = rand(rng, ppca, num_obs)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Visualize-the-simulated-data","page":"Probabilistic PCA Example","title":"Visualize the simulated data","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"We'll color points by the dominant latent dimension (for intuition only—latent variables are unobserved in real data).","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"x1 = X[1, :]\nx2 = X[2, :]\nlabels = map(i -> (abs(z[1,i]) > abs(z[2,i]) ? 1 : 2), 1:size(z,2))\n\np = plot()\nscatter!(\n    p, x1, x2;\n    group      = labels,\n    xlabel     = \"X₁\",\n    ylabel     = \"X₂\",\n    title      = \"Samples grouped by dominant latent factor\",\n    label      = [\"Latent 1\" \"Latent 2\"],\n    legend     = :topright,\n    markersize = 5,\n)\n\np","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Parameter-recovery:-fit-PPCA-with-EM","page":"Probabilistic PCA Example","title":"Parameter recovery: fit PPCA with EM","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"We'll start from random loadings/mean and a reasonable noise variance, then call fit! to run EM until convergence.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"(Re)define defaults to emphasize the fitting path","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"D = 2\nk = 2\nW = randn(rng, D, k)\nσ² = 0.5\nμ_vector = randn(rng, 2)\n\nfit_ppca = ProbabilisticPCA(W, σ², μ_vector)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Fit-with-EM","page":"Probabilistic PCA Example","title":"Fit with EM","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"fit! returns the log-likelihood trace. Monotone ascent is a good sanity check.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"lls = fit!(fit_ppca, X)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Log-likelihood-diagnostic","page":"Probabilistic PCA Example","title":"Log-likelihood diagnostic","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"ll_plot = plot(\n    lls;\n    xlabel=\"Iteration\",\n    ylabel=\"Log-Likelihood\",\n    title=\"EM Convergence (PPCA)\",\n    marker=:circle,\n    label=\"log_likelihood\",\n    reuse=false,\n)\n\nll_plot","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Interpreting-the-learned-parameters","page":"Probabilistic PCA Example","title":"Interpreting the learned parameters","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"fit_ppca.W are the learned loading directions. Columns span the principal subspace (up to rotation). For k=2 in 2D, they form a basis centered at μ.\nfit_ppca.μ is the learned mean of the data.\nfit_ppca.σ² is the isotropic residual variance.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"x1, x2 = X[1, :], X[2, :]\nμ1, μ2  = fit_ppca.μ\nW_fit   = fit_ppca.W\nw1      = W_fit[:, 1]\nw2      = W_fit[:, 2]\n\nP = plot()\nscatter!(\n    P, x1, x2;\n    xlabel     = \"X₁\",\n    ylabel     = \"X₂\",\n    title      = \"Data with PPCA loading directions\",\n    label      = \"Data\",\n    alpha      = 0.5,\n    markersize = 4,\n)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Draw loading vectors from the mean in both directions for visibility","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"quiver!(P, [μ1], [μ2]; quiver=([ w1[1]], [ w1[2]]), arrow=:arrow, lw=3, color=:red,   label=\"W₁\")\nquiver!(P, [μ1], [μ2]; quiver=([-w1[1]], [-w1[2]]), arrow=:arrow, lw=3, color=:red,   label=\"\")\nquiver!(P, [μ1], [μ2]; quiver=([ w2[1]], [ w2[2]]), arrow=:arrow, lw=3, color=:green, label=\"W₂\")\nquiver!(P, [μ1], [μ2]; quiver=([-w2[1]], [-w2[2]]), arrow=:arrow, lw=3, color=:green, label=\"\")\n\nP","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Posterior-latents-and-reconstructions","page":"Probabilistic PCA Example","title":"Posterior latents and reconstructions","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Compute \\nE[z|x]\\n and optional reconstructions (\\hat x = \\mu + W\\,\\mathbb E[z|x]).","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"function ppca_posterior_means(W::AbstractMatrix, σ²::Real, μ::AbstractVector, X::AbstractMatrix)\n    D, N = size(X)\n    k    = size(W, 2)\n    M = I(k) + (W' * W) / σ²            # k×k\n    B = M \\ (W' / σ²)                  # k×D, equals M^{-1} W^T / σ²\n    Zmean = B * (X .- μ)               # k×N\n    return Zmean\nend\n\nẐ = ppca_posterior_means(W_fit, fit_ppca.σ², fit_ppca.μ, X)\nX̂ = fit_ppca.μ .+ W_fit * Ẑ","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Example: reconstruction error (per-dimension MSE)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"recon_mse = mean(norm.(eachcol(X - X̂)).^2) / size(X,1)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Variance-explained-and-choosing-k","page":"Probabilistic PCA Example","title":"Variance explained and choosing k","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"A quick check is to compare the sample covariance eigenvalues to the PPCA model. For PPCA with k factors, the top-k eigenvalues should be captured by W W^T and the remainder approximated by σ².","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Σ̂ = cov(permutedims(X))            # D×D sample covariance\nλs = sort(eigvals(Symmetric(Σ̂)); rev=true)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Proportion of variance explained by top-k sample eigenvalues","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"pve_sample = sum(λs[1:k]) / sum(λs)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"PPCA-implied total variance: tr(WW^T) + D*σ²","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"pve_ppca = (tr(W_fit * W_fit') ) / (tr(W_fit * W_fit') + size(X,1) * 0 + length(μ1:μ2) * fit_ppca.σ²)  # placeholder formula to keep inline; see note below","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"NOTE: For PPCA in D dims, total variance is tr(WW^T) + Dσ². If you prefer, compute: pveppca = tr(WfitWfit') / (tr(WfitW_fit') + Dfit_ppca.σ²)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Practical-tips-and-pitfalls","page":"Probabilistic PCA Example","title":"Practical tips & pitfalls","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Scaling matters. Standardize your features if units differ.\nInitialization: Multiple random starts can help avoid poor local optima.\nRotational ambiguity: For presentation, you can orthonormalize W or align it with PCA loadings via Procrustes.\nChoosing k: Use scree plots, PVE, or information criteria (AIC/BIC) on the marginal likelihood returned by fit!.\nOutliers/heavy tails: Consider robust variants (e.g., t-PCA) if needed.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Where-this-fits-in-StateSpaceDynamics.jl","page":"Probabilistic PCA Example","title":"Where this fits in StateSpaceDynamics.jl","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"PPCA is an IID latent-factor model. In the ecosystem, it bridges to time-series models with latent structure, such as LDS/PLDS where factors evolve over time. The workflow mirrors those models: specify (W, σ², μ), simulate, fit with EM, and validate with likelihood curves and posterior diagnostics.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Exercises","page":"Probabilistic PCA Example","title":"Exercises","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Compare to PCA: Compute the top-2 eigenvectors of the sample covariance and align fit_ppca.W to them with an orthogonal Procrustes transform.\nVary noise: Increase σ²_true and see how loading directions and convergence behave.\nModel selection: Fit k=1..D and report AIC/BIC (parameter count is p = D*k + 1 (σ²) + D (μ)).\nHeld-out likelihood: Split X into train/validation and compare models.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"End of tutorial.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Simulating-and-Fitting-a-Hidden-Markov-Model-with-Gaussian-Emissions","page":"Hidden Markov Model Example","title":"Simulating and Fitting a Hidden Markov Model with Gaussian Emissions","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create, sample from, and fit Hidden Markov Models (HMMs) with Gaussian emission distributions. This is the classical HMM formulation where each hidden state generates observations from a different multivariate Gaussian distribution.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Unlike the GLM-HMM in the previous tutorial, this model doesn't have input features - each state simply emits observations from its own characteristic Gaussian distribution. This makes it ideal for clustering time series data, identifying behavioral regimes, or modeling switching dynamics in systems where each state has a distinct statistical signature.","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Load-Required-Packages","page":"Hidden Markov Model Example","title":"Load Required Packages","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"We load the essential packages for HMM modeling, visualization, and reproducible analysis.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"using LinearAlgebra\nusing Plots\nusing Random\nusing StateSpaceDynamics\nusing StableRNGs\nusing Statistics: mean, std","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Set up reproducible random number generation","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Create-a-Gaussian-Emission-HMM","page":"Hidden Markov Model Example","title":"Create a Gaussian Emission HMM","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"We'll create an HMM with two hidden states, each emitting 2D Gaussian observations. This creates a simple but illustrative model where the hidden states correspond to different regions in the observation space.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"output_dim = 2;  # Each observation is a 2D vector\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Define the state transition dynamics High diagonal values mean states are \"sticky\" (tend to persist)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"A = [0.99 0.01;    # From state 1: 99% stay in state 1, 1% switch to state 2\n     0.05 0.95];   # From state 2: 5% switch to state 1, 95% stay in state 2\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Initial state probabilities (equal probability of starting in either state)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"πₖ = [0.5; 0.5];\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Define emission distributions for each hidden state State 1: Centered at (-1, -1) with small variance (tight cluster)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"μ_1 = [-1.0, -1.0]                                          # Mean vector\nΣ_1 = 0.1 * Matrix{Float64}(I, output_dim, output_dim)      # Covariance matrix (0.1 * Identity)\nemission_1 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1);\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"State 2: Centered at (1, 1) with larger variance (more spread out)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"μ_2 = [1.0, 1.0]                                           # Mean vector\nΣ_2 = 0.2 * Matrix{Float64}(I, output_dim, output_dim)     # Covariance matrix (0.2 * Identity)\nemission_2 = GaussianEmission(output_dim=output_dim, μ=μ_2, Σ=Σ_2);\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Construct the complete HMM","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"model = HiddenMarkovModel(\n    K=2,                        # Number of hidden states\n    B=[emission_1, emission_2], # Emission distributions\n    A=A,                        # State transition matrix\n    πₖ=πₖ                      # Initial state distribution\n);\n\nprintln(\"Created Gaussian HMM with 2 states:\")\nprintln(\"  State 1: μ = $μ_1, σ² = $(Σ_1[1,1]) (tight cluster in lower-left)\")\nprintln(\"  State 2: μ = $μ_2, σ² = $(Σ_2[1,1]) (looser cluster in upper-right)\")\nprintln(\"  Transition probabilities encourage state persistence\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Sample-from-the-HMM","page":"Hidden Markov Model Example","title":"Sample from the HMM","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Generate synthetic data from our true model. Unlike GLM-HMMs, we don't need input features - each state generates observations from its own Gaussian distribution.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"num_samples = 10000\nprintln(\"Generating $num_samples samples from the Gaussian HMM...\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Sample both hidden state sequence and corresponding observations","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"true_labels, data = rand(rng, model, n=num_samples)\n\nprintln(\"Generated data summary:\")\nprintln(\"  Data shape: $(size(data)) (dimensions × time)\")\nprintln(\"  Labels shape: $(size(true_labels))\")\nprintln(\"  State 1 proportion: $(round(mean(true_labels .== 1), digits=3))\")\nprintln(\"  State 2 proportion: $(round(mean(true_labels .== 2), digits=3))\")\nprintln(\"  Data range: [$(round(minimum(data), digits=2)), $(round(maximum(data), digits=2))]\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Visualize-the-Sampled-Dataset","page":"Hidden Markov Model Example","title":"Visualize the Sampled Dataset","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Create a 2D scatter plot showing the observations colored by their true hidden state. This illustrates how each state generates observations from a distinct region of space.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"x_vals = data[1, 1:num_samples]  # First dimension\ny_vals = data[2, 1:num_samples]  # Second dimension\nlabels_slice = true_labels[1:num_samples]\n\nstate_colors = [:dodgerblue, :crimson]  # Blue for state 1, red for state 2\n\nplt = plot();\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Plot observations for each state separately to get proper legends","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"for state in 1:2\n    idx = findall(labels_slice .== state)\n    scatter!(x_vals[idx], y_vals[idx];\n        color=state_colors[state],\n        label=\"State $state\",\n        markersize=4,\n        alpha=0.6)\nend;\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Add a trajectory line to show temporal evolution (faded)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"plot!(x_vals[1:1000], y_vals[1:1000];  # Show first 1000 points for clarity\n    color=:gray,\n    lw=1.5,\n    linealpha=0.4,\n    label=\"Trajectory\");\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Mark start and end points","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"scatter!([x_vals[1]], [y_vals[1]];\n    color=:green,\n    markershape=:star5,\n    markersize=10,\n    label=\"Start\");\n\nscatter!([x_vals[end]], [y_vals[end]];\n    color=:black,\n    markershape=:diamond,\n    markersize=8,\n    label=\"End\")\n\nxlabel!(\"Output Dimension 1\")\nylabel!(\"Output Dimension 2\")\ntitle!(\"HMM Emissions Colored by True Hidden State\")\n\nprintln(\"Note: The trajectory line shows the temporal sequence connecting observations\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Initialize-and-Fit-a-New-HMM-to-the-Sampled-Data","page":"Hidden Markov Model Example","title":"Initialize and Fit a New HMM to the Sampled Data","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Now we'll simulate the realistic scenario where we observe only the data, not the hidden states. We'll initialize an HMM with incorrect parameters and use EM to learn the true parameters from the observations alone.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"Initializing naive HMM with incorrect parameters...\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Initialize with biased/incorrect parameters","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"μ_1 = [-0.25, -0.25]  # Closer to center than true value\nΣ_1 = 0.3 * Matrix{Float64}(I, output_dim, output_dim)  # Larger variance than true\nemission_1 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1)\n\nμ_2 = [0.25, 0.25]    # Closer to center than true value\nΣ_2 = 0.5 * Matrix{Float64}(I, output_dim, output_dim)  # Much larger variance than true","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Note: There's a bug in the original code - emission2 uses μ1 and Σ_1, let's fix it:","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"emission_2 = GaussianEmission(output_dim=output_dim, μ=μ_2, Σ=Σ_2)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Different transition matrix and initial distribution","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"A = [0.8 0.2;     # Less persistent than true model\n     0.05 0.95]   # Asymmetric transitions\nπₖ = [0.6, 0.4]   # Biased toward state 1","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Create the test model","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"test_model = HiddenMarkovModel(K=2, B=[emission_1, emission_2], A=A, πₖ=πₖ)\n\nprintln(\"Initial guesses:\")\nprintln(\"  State 1: μ = $μ_1, σ² = $(Σ_1[1,1])\")\nprintln(\"  State 2: μ = $μ_2, σ² = $(Σ_2[1,1])\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Fit the model using the Expectation-Maximization algorithm","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"Running EM algorithm to learn parameters...\")\nlls = fit!(test_model, data)\n\nprintln(\"EM algorithm converged after $(length(lls)) iterations\")\nprintln(\"Log-likelihood improvement: $(round(lls[end] - lls[1], digits=2))\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Plot the convergence of the log-likelihood","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"plot(lls)\ntitle!(\"Log-likelihood over EM Iterations\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Display learned parameters","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"Learned parameters:\")\nprintln(\"  State 1: μ = $(round.(test_model.B[1].μ, digits=3)), σ² = $(round(test_model.B[1].Σ[1,1], digits=3))\")\nprintln(\"  State 2: μ = $(round.(test_model.B[2].μ, digits=3)), σ² = $(round(test_model.B[2].Σ[1,1], digits=3))\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Visualize-the-Latent-State-Predictions-using-Viterbi-Algorithm","page":"Hidden Markov Model Example","title":"Visualize the Latent State Predictions using Viterbi Algorithm","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"The Viterbi algorithm finds the most likely sequence of hidden states given the observed data and learned parameters. We'll compare this with the true sequence.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"Running Viterbi algorithm to decode most likely state sequence...\")\npred_labels = viterbi(test_model, data);\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Calculate the accuracy of state prediction","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"accuracy = mean(true_labels .== pred_labels)\nprintln(\"State sequence prediction accuracy: $(round(accuracy*100, digits=1))%\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Handle potential label switching (EM can converge with states swapped) Check if swapping labels gives better accuracy","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"swapped_pred = 3 .- pred_labels  # Convert 1→2, 2→1\nswapped_accuracy = mean(true_labels .== swapped_pred)\n\nif swapped_accuracy > accuracy\n    println(\"Detected label switching - corrected accuracy: $(round(swapped_accuracy*100, digits=1))%\")\n    pred_labels = swapped_pred\n    accuracy = swapped_accuracy\nend","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Visualize state sequences as heatmaps","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"true_mat = reshape(true_labels[1:1000], 1, :)\npred_mat = reshape(pred_labels[1:1000], 1, :)\n\np1 = heatmap(true_mat;\n    colormap = :roma50,\n    title = \"True State Labels (first 1000 timepoints)\",\n    xlabel = \"\",\n    ylabel = \"\",\n    xticks = false,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\np2 = heatmap(pred_mat;\n    colormap = :roma50,\n    title = \"Predicted State Labels (Viterbi)\",\n    xlabel = \"Timepoints\",\n    ylabel = \"\",\n    xticks = 0:200:1000,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\nplot(p1, p2;\n    layout = (2, 1),\n    size = (700, 500),\n    margin = 5Plots.mm)","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Sampling-Multiple,-Independent-Trials-of-Data-from-an-HMM","page":"Hidden Markov Model Example","title":"Sampling Multiple, Independent Trials of Data from an HMM","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"In many real applications, we have multiple independent sequences rather than one long sequence. For example: multiple subjects in an experiment, multiple recording sessions, or multiple independent time series. We'll demonstrate how to handle this scenario.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"Generating multiple independent trials...\")\n\nn_trials = 100    # Number of independent sequences\nn_samples = 1000  # Length of each sequence","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Pre-allocate storage for efficiency","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"all_true_labels = Vector{Vector{Int}}(undef, n_trials)\nall_data = Vector{Matrix{Float64}}(undef, n_trials)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Generate independent sequences","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"for i in 1:n_trials\n    true_labels, data = rand(rng, model, n=n_samples)\n    all_true_labels[i] = true_labels\n    all_data[i] = data\nend\n\nprintln(\"Generated $n_trials independent trials of length $n_samples each\")\nprintln(\"Total data points: $(n_trials * n_samples)\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Calculate statistics across trials","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"total_state1_prop = mean([mean(labels .== 1) for labels in all_true_labels])\nprintln(\"Average proportion of state 1 across trials: $(round(total_state1_prop, digits=3))\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Fitting-an-HMM-to-Multiple,-Independent-Trials-of-Data","page":"Hidden Markov Model Example","title":"Fitting an HMM to Multiple, Independent Trials of Data","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"When fitting to multiple independent sequences, the EM algorithm must account for the fact that each sequence starts independently from the initial state distribution. This typically provides more robust parameter estimates.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"Fitting HMM to multiple independent trials...\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Initialize a fresh model for multi-trial fitting","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"μ_1 = [-0.25, -0.25]\nΣ_1 = 0.3 * Matrix{Float64}(I, output_dim, output_dim)\nemission_1 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1)\n\nμ_2 = [0.25, 0.25]\nΣ_2 = 0.5 * Matrix{Float64}(I, output_dim, output_dim)\nemission_2 = GaussianEmission(output_dim=output_dim, μ=μ_2, Σ=Σ_2)\n\nA = [0.8 0.2; 0.05 0.95]\nπₖ = [0.6, 0.4]\ntest_model = HiddenMarkovModel(K=2, B=[emission_1, emission_2], A=A, πₖ=πₖ)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Fit to all trials simultaneously The package automatically handles the multi-trial structure","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"lls = fit!(test_model, all_data)\n\nprintln(\"Multi-trial EM converged after $(length(lls)) iterations\")\nprintln(\"Final log-likelihood: $(round(lls[end], digits=2))\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Plot convergence for multi-trial case","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"plot(lls)\ntitle!(\"Log-likelihood over EM Iterations (Multi-Trial Fitting)\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")\n\nprintln(\"Final learned parameters (multi-trial):\")\nprintln(\"  State 1: μ = $(round.(test_model.B[1].μ, digits=3)), σ² = $(round(test_model.B[1].Σ[1,1], digits=3))\")\nprintln(\"  State 2: μ = $(round.(test_model.B[2].μ, digits=3)), σ² = $(round(test_model.B[2].Σ[1,1], digits=3))\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Visualize-Latent-State-Predictions-for-Multiple-Trials-using-Viterbi","page":"Hidden Markov Model Example","title":"Visualize Latent State Predictions for Multiple Trials using Viterbi","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Decode the hidden state sequences for all trials and visualize the results as a heatmap showing state assignments across multiple independent sequences.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"Running Viterbi decoding on all trials...\")\nall_pred_labels_vec = viterbi(test_model, all_data)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Reshape data for easier analysis and visualization","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"all_pred_labels = hcat(all_pred_labels_vec...)'      # trials × time\nall_true_labels_matrix = hcat(all_true_labels...)'   # trials × time","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Calculate overall accuracy across all trials and timepoints","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"overall_accuracy = mean(all_true_labels_matrix .== all_pred_labels)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Check for label switching across the entire dataset","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"swapped_pred_all = 3 .- all_pred_labels\nswapped_accuracy_all = mean(all_true_labels_matrix .== swapped_pred_all)\n\nif swapped_accuracy_all > overall_accuracy\n    println(\"Detected label switching in multi-trial analysis\")\n    all_pred_labels = swapped_pred_all\n    overall_accuracy = swapped_accuracy_all\nend\n\nprintln(\"Overall state prediction accuracy across all trials: $(round(overall_accuracy*100, digits=1))%\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Calculate per-trial accuracies for robustness assessment","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"trial_accuracies = [mean(all_true_labels_matrix[i, :] .== all_pred_labels[i, :]) for i in 1:n_trials]\nprintln(\"Mean per-trial accuracy: $(round(mean(trial_accuracies)*100, digits=1))% ± $(round(std(trial_accuracies)*100, digits=1))%\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Visualize a subset of trials to show consistency across independent sequences","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"state_colors = [:dodgerblue, :crimson]\ntrue_subset = all_true_labels_matrix[1:10, 1:500]   # First 10 trials, first 500 timepoints\npred_subset = all_pred_labels[1:10, 1:500]\n\np1 = heatmap(\n    true_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"True State Labels (10 trials × 500 timepoints)\",\n    xlabel = \"\",\n    ylabel = \"Trial Number\",\n    xticks = false,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\np2 = heatmap(\n    pred_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"Predicted State Labels (Viterbi Decoding)\",\n    xlabel = \"Timepoints\",\n    ylabel = \"Trial Number\",\n    xticks = true,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\nfinal_plot = plot(\n    p1, p2,\n    layout = (2, 1),\n    size = (850, 550),\n    margin = 5Plots.mm,\n    legend = false,\n)\n\ndisplay(final_plot)","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Parameter-Recovery-Analysis","page":"Hidden Markov Model Example","title":"Parameter Recovery Analysis","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"println(\"\\n=== Parameter Recovery Assessment ===\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Compare true vs learned emission parameters","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"true_μ1, true_μ2 = [-1.0, -1.0], [1.0, 1.0]\nlearned_μ1 = test_model.B[1].μ\nlearned_μ2 = test_model.B[2].μ\n\nμ1_error = norm(true_μ1 - learned_μ1) / norm(true_μ1)\nμ2_error = norm(true_μ2 - learned_μ2) / norm(true_μ2)\n\nprintln(\"Mean vector recovery errors:\")\nprintln(\"  State 1: $(round(μ1_error*100, digits=1))%\")\nprintln(\"  State 2: $(round(μ2_error*100, digits=1))%\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Compare transition matrices","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"true_A = [0.99 0.01; 0.05 0.95]\nlearned_A = test_model.A\nA_error = norm(true_A - learned_A) / norm(true_A)\nprintln(\"Transition matrix recovery error: $(round(A_error*100, digits=1))%\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Compare covariance matrices","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"true_Σ1, true_Σ2 = 0.1, 0.2\nlearned_Σ1 = test_model.B[1].Σ[1,1]\nlearned_Σ2 = test_model.B[2].Σ[1,1]\n\nΣ1_error = abs(true_Σ1 - learned_Σ1) / true_Σ1\nΣ2_error = abs(true_Σ2 - learned_Σ2) / true_Σ2\n\nprintln(\"Variance recovery errors:\")\nprintln(\"  State 1: $(round(Σ1_error*100, digits=1))%\")\nprintln(\"  State 2: $(round(Σ2_error*100, digits=1))%\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Summary","page":"Hidden Markov Model Example","title":"Summary","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"This tutorial demonstrated the complete workflow for Gaussian emission Hidden Markov Models:","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Model Structure: Discrete hidden states with Gaussian emission distributions\nApplications: Time series clustering, regime detection, behavioral state analysis\nParameter Learning: EM algorithm successfully recovered emission parameters and transition dynamics\nState Inference: Viterbi algorithm accurately decoded hidden state sequences\nMulti-Trial Analysis: Robust parameter estimation from multiple independent sequences\nLabel Switching: Demonstrated detection and handling of the label switching problem\nScalability: Efficient handling of large datasets with multiple trials","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"Gaussian HMMs provide a fundamental framework for modeling time series with discrete latent structure, forming the foundation for more complex state-space models and serving as a powerful tool for exploratory data analysis in temporal datasets.","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Simulating-and-Fitting-a-Poisson-Linear-Dynamical-System","page":"Poisson LDS Example","title":"Simulating and Fitting a Poisson Linear Dynamical System","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to simulate and fit a Linear Dynamical System (LDS) with Poisson observations using the Laplace-EM algorithm. Unlike the standard Gaussian LDS, this model is designed for count data (e.g., neural spike counts, customer arrivals, or any discrete event data) where observations are non-negative integers that follow Poisson distributions.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Load-Required-Packages","page":"Poisson LDS Example","title":"Load Required Packages","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"We begin by loading the necessary packages. The core difference from the Gaussian case is that we'll be working with Poisson observation models, which require more sophisticated inference algorithms (Laplace approximations) due to the non-conjugate nature of Poisson likelihoods with Gaussian latent states.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing LaTeXStrings\nusing StableRNGs","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Set up reproducible random number generation","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"rng = StableRNG(123);\nnothing #hide","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Create-a-Poisson-Linear-Dynamical-System","page":"Poisson LDS Example","title":"Create a Poisson Linear Dynamical System","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"We define a system where continuous latent dynamics generate discrete count observations. This is particularly relevant in neuroscience (modeling neural spike trains) and other domains where we observe discrete events generated by underlying continuous processes.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"obs_dim = 10      # Number of observed count variables (e.g., neurons)\nlatent_dim = 2    # Number of latent state dimensions","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Define the latent dynamics: same spiral structure as the Gaussian case The latent states evolve smoothly and continuously according to linear dynamics","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"A = 0.95 * [cos(0.25) -sin(0.25); sin(0.25) cos(0.25)]  # Rotation with contraction\nQ = Matrix(0.1 * I(latent_dim))     # Process noise covariance\nx0 = zeros(latent_dim)              # Initial state mean\nP0 = Matrix(0.1 * I(latent_dim))    # Initial state covariance","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Poisson observation model parameters: For Poisson observations, the rate parameter λ is typically modeled as: log(λi) = Ci^T * xt + logdi where C maps latent states to log-rates and logd provides baseline log-rates","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"log_d = log.(fill(0.1, obs_dim))    # Log baseline rates (small positive rates)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Observation matrix C: maps 2D latent states to log-rates for each observed dimension We use positive values to ensure that latent activity increases firing rates","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"C = permutedims([abs.(randn(rng, obs_dim))'; abs.(randn(rng, obs_dim))'])","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Construct the model components","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"state_model = GaussianStateModel(; A, Q, x0, P0)          # Gaussian latent dynamics\nobs_model = PoissonObservationModel(; C, log_d)           # Poisson observations","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Create the complete Poisson Linear Dynamical System","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"true_plds = LinearDynamicalSystem(;\n    state_model=state_model,\n    obs_model=obs_model,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)  # Learn all parameters: A, Q, C, log_d, x0, P0\n)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Simulate-Latent-States-and-Observations","page":"Poisson LDS Example","title":"Simulate Latent States and Observations","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Generate synthetic data from our Poisson LDS. The latent states evolve according to the linear dynamics, while observations are drawn from Poisson distributions whose rates depend on the current latent state.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"tSteps = 500\nprintln(\"Simulating $tSteps time steps of Poisson LDS data...\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Generate both latent trajectories and count observations","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"latents, observations = rand(rng, true_plds; tsteps=tSteps, ntrials=1)\n\nprintln(\"Generated latent states with range: [$(minimum(latents)), $(maximum(latents))]\")\nprintln(\"Generated count observations with range: [$(minimum(observations)), $(maximum(observations))]\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Plot-Vector-Field-of-Latent-Dynamics","page":"Poisson LDS Example","title":"Plot Vector Field of Latent Dynamics","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Visualize the underlying continuous dynamics that drive the discrete observations. This vector field shows how latent states evolve deterministically (ignoring noise).","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Create a grid of starting points in latent space","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"x = y = -3:0.5:3\nX = repeat(x', length(y), 1)\nY = repeat(y, 1, length(x))\nU = zeros(size(X))  # Flow in x-direction\nV = zeros(size(Y))  # Flow in y-direction","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Compute the deterministic flow at each grid point","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"for i in 1:size(X, 1), j in 1:size(X, 2)\n    v = A * [X[i,j], Y[i,j]]\n    U[i,j] = v[1] - X[i,j]\n    V[i,j] = v[2] - Y[i,j]\nend","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Normalize arrow lengths for cleaner visualization","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"magnitude = @. sqrt(U^2 + V^2)\nU_norm = U ./ magnitude\nV_norm = V ./ magnitude","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Plot the vector field with the actual simulated trajectory","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"p = quiver(X, Y, quiver=(U_norm, V_norm), color=:blue, alpha=0.3,\n           linewidth=1, arrow=arrow(:closed, :head, 0.1, 0.1))\nplot!(latents[1, :, 1], latents[2, :, 1], xlabel=\"x₁\", ylabel=\"x₂\",\n      color=:black, linewidth=1.5, title=\"Latent Dynamics\", legend=false)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Plot-Latent-States-and-Observations","page":"Poisson LDS Example","title":"Plot Latent States and Observations","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Create visualizations that highlight the key difference between continuous latent dynamics and discrete count observations. The latent states are smooth curves, while observations are spike trains (discrete events over time).","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"states = latents[:, :, 1]\nemissions = observations[:, :, 1]\ntime_bins = size(states, 2)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Two-panel layout: latent states above, spike rasters below","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"plot(size=(800, 600), layout=@layout[a{0.3h}; b])","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Plot smooth latent state trajectories","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"lim_states = maximum(abs.(states))\nfor d in 1:latent_dim\n    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black,\n          linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, time_bins), title=\"Simulated Latent States\",\n      yformatter=y->\"\", tickfontsize=12)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Plot discrete observations as spike rasters Each row represents one observed dimension, spikes shown as vertical lines","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"colors = palette(:default, obs_dim)\nfor f in 1:obs_dim\n    spike_times = findall(x -> x > 0, emissions[f, :])\n    for t in spike_times\n        plot!([t, t], [f-0.4, f+0.4], color=colors[f], linewidth=1, label=\"\", subplot=2)\n    end\nend\n\nplot!(subplot=2, yticks=(1:obs_dim, [L\"y_{%$d}\" for d in 1:obs_dim]),\n      xlims=(0, time_bins), ylims=(0.5, obs_dim + 0.5), title=\"Simulated Emissions (Spike Raster)\",\n      xlabel=\"Time\", tickfontsize=12, grid=false)\n\nprintln(\"Total spike count across all dimensions: $(sum(emissions))\")\nprintln(\"Average firing rate: $(sum(emissions)/(obs_dim * time_bins)) spikes per time bin\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Initialize-Model-and-Perform-Initial-Smoothing","page":"Poisson LDS Example","title":"Initialize Model and Perform Initial Smoothing","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"In practice, we only observe the spike counts, not the latent states. Our goal is to infer both the latent dynamics and the mapping from latent states to observed firing rates. We start with a randomly initialized model.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Random initialization (simulating lack of prior knowledge)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"A_init = random_rotation_matrix(latent_dim, rng)  # Random rotation matrix\nQ_init = Matrix(0.1 * I(latent_dim))              # Process noise guess\nC_init = randn(rng, obs_dim, latent_dim)          # Random observation mapping\nlog_d_init = log.(fill(0.1, obs_dim))             # Baseline log-rate guess\nx0_init = zeros(latent_dim)                       # Start from origin\nP0_init = Matrix(0.1 * I(latent_dim))             # Initial uncertainty","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Construct the naive model","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"sm_init = GaussianStateModel(; A=A_init, Q=Q_init, x0=x0_init, P0=P0_init)\nom_init = PoissonObservationModel(; C=C_init, log_d=log_d_init)\n\nnaive_plds = LinearDynamicalSystem(;\n    state_model=sm_init,\n    obs_model=om_init,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)\n)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Perform smoothing with the randomly initialized model For Poisson observations, this requires Laplace approximations since the posterior is no longer Gaussian (unlike the linear-Gaussian case)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"println(\"Performing initial smoothing with random parameters...\")\nsmoothed_x, smoothed_p = smooth(naive_plds, observations)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Compare true vs. initial estimated latent states","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"plot()\nfor d in 1:latent_dim\n    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:time_bins, smoothed_x[d, :, 1] .+ lim_states * (d-1), color=:red, linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, time_bins), title=\"True vs. Predicted Latent States (Pre-EM)\",\n      yformatter=y->\"\", tickfontsize=12)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Fit-the-Poisson-LDS-Using-Laplace-EM","page":"Poisson LDS Example","title":"Fit the Poisson LDS Using Laplace-EM","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Now we use the Laplace-EM algorithm to learn the parameters. This is more complex than standard EM because:","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"The E-step requires Laplace approximations to handle non-Gaussian posteriors\nThe M-step updates must account for the Poisson likelihood structure\nConvergence can be slower due to the non-conjugate nature of the model","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"println(\"Starting Laplace-EM algorithm...\")\nprintln(\"Note: Poisson LDS fitting is more computationally intensive than Gaussian LDS\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Fit the model - using fewer iterations than Gaussian case due to computational cost","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"elbo, _ = fit!(naive_plds, observations; max_iter=25, tol=1e-6)\n\nprintln(\"Laplace-EM completed after $(length(elbo)) iterations\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Perform smoothing with the learned parameters","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"smoothed_x, smoothed_p = smooth(naive_plds, observations)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Compare true vs. learned latent state estimates","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"plot()\nfor d in 1:latent_dim\n    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:time_bins, smoothed_x[d, :, 1] .+ lim_states * (d-1), color=:red, linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, time_bins), title=\"True vs. Predicted Latent States (Post-EM)\",\n      yformatter=y->\"\", tickfontsize=12)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Monitor-ELBO-Convergence","page":"Poisson LDS Example","title":"Monitor ELBO Convergence","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"The Evidence Lower BOund (ELBO) tracks the algorithm's progress. For Poisson LDS, the ELBO includes both the data likelihood and the Laplace approximation terms. Convergence may be less smooth than in the Gaussian case due to the approximations.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"plot(elbo, xlabel=\"iteration\", ylabel=\"ELBO\", title=\"ELBO over Iterations\", legend=false)\n\nprintln(\"Initial ELBO: $(elbo[1])\")\nprintln(\"Final ELBO: $(elbo[end])\")\nprintln(\"ELBO improvement: $(elbo[end] - elbo[1])\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Model-Comparison-and-Validation","page":"Poisson LDS Example","title":"Model Comparison and Validation","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Let's examine how well we recovered the true parameters","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"println(\"\\n=== Parameter Recovery Assessment ===\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Compare true vs learned observation matrix C","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"C_error = norm(true_plds.obs_model.C - naive_plds.obs_model.C) / norm(true_plds.obs_model.C)\nprintln(\"Relative error in observation matrix C: $(round(C_error, digits=3))\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Compare true vs learned dynamics matrix A","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"A_error = norm(true_plds.state_model.A - naive_plds.state_model.A) / norm(true_plds.state_model.A)\nprintln(\"Relative error in dynamics matrix A: $(round(A_error, digits=3))\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Compare latent state trajectories","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"state_error = norm(states - smoothed_x[:,:,1]) / norm(states)\nprintln(\"Relative error in latent state estimation: $(round(state_error, digits=3))\")","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Summary","page":"Poisson LDS Example","title":"Summary","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"This tutorial demonstrated fitting a Poisson Linear Dynamical System:","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Model Structure: Continuous Gaussian latent dynamics generate discrete Poisson observations\nApplications: Ideal for count data like neural spikes, customer arrivals, or event sequences\nAlgorithm: Laplace-EM handles the non-conjugate Poisson-Gaussian combination\nChallenges: More computationally intensive than Gaussian LDS due to required approximations\nResults: Successfully recovered both latent dynamics and observation parameters from spike data","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"The Poisson LDS extends the linear dynamical system framework to discrete observation models, enabling state-space modeling of count data while maintaining interpretable latent dynamics.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Simulating-and-Fitting-a-Gaussian-Mixture-Model","page":"Gaussian Mixture Model Example","title":"Simulating and Fitting a Gaussian Mixture Model","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create a Gaussian Mixture Model (GMM) and fit it using the EM algorithm. Unlike Hidden Markov Models which model temporal sequences, GMMs are designed for clustering and density estimation of independent observations. Each data point is assumed to come from one of several Gaussian components, but there's no temporal dependence.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"GMMs are fundamental in machine learning for tasks like:","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Unsupervised clustering of data\nDensity estimation for anomaly detection\nDimensionality reduction (when combined with factor analysis)\nAs building blocks for more complex models","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"The key insight is that complex data distributions can often be well-approximated as mixtures of simpler Gaussian distributions, each representing a different \"mode\" or cluster in the data.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Load-Required-Packages","page":"Gaussian Mixture Model Example","title":"Load Required Packages","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"We need several packages for GMM modeling, data generation, and comprehensive visualization.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"using StateSpaceDynamics  # Core GMM functionality\nusing LinearAlgebra       # Matrix operations\nusing Random             # Random number generation\nusing Plots              # Basic plotting\nusing StableRNGs         # Reproducible randomness\nusing Distributions      # Statistical distributions\nusing StatsPlots         # Enhanced statistical plotting\nusing Combinatorics","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Set up reproducible random number generation","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Create-a-True-Gaussian-Mixture-Model-to-Simulate-From","page":"Gaussian Mixture Model Example","title":"Create a True Gaussian Mixture Model to Simulate From","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"We'll create a \"ground truth\" GMM with known parameters, generate data from it, then see how well we can recover these parameters using only the observed data.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"k = 3  # Number of mixture components (clusters)\nD = 2  # Data dimensionality (2D for easy visualization)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Define the true component means Each column represents the mean vector for one component","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"true_μs = [\n    -1.0  1.0  0.0;   # x₁ coordinates of the 3 component centers\n    -1.0 -1.5  2.0    # x₂ coordinates of the 3 component centers\n]  # Shape: (D, K) = (2, 3)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Define covariance matrices for each component Using isotropic (spherical) covariances for simplicity","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"true_Σs = [Matrix{Float64}(0.3 * I(2)) for _ in 1:k]  # All components have same shape","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Define mixing weights (must sum to 1) These represent the probability that a random sample comes from each component","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"true_πs = [0.5, 0.2, 0.3]  # Component 1 is most likely, component 2 least likely","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Construct the complete GMM","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"true_gmm = GaussianMixtureModel(k, true_μs, true_Σs, true_πs)\n\nprintln(\"Created true GMM with $k components in $D dimensions:\")\nfor i in 1:k\n    println(\"  Component $i: μ = $(true_μs[:, i]), π = $(true_πs[i])\")\nend\nprintln(\"  All components have isotropic covariance with σ² = 0.3\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Sample-Data-from-the-True-GMM","page":"Gaussian Mixture Model Example","title":"Sample Data from the True GMM","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Generate synthetic data from our true model. We'll sample both the component assignments (for visualization) and the actual observations.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"n = 500  # Number of data points to generate\nprintln(\"Generating $n samples from the true GMM...\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"First, determine which component each sample comes from","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"labels = rand(rng, Categorical(true_πs), n)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Count samples per component","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"component_counts = [sum(labels .== i) for i in 1:k]\nprintln(\"Samples per component: $(component_counts)\")\nprintln(\"Empirical mixing proportions: $(round.(component_counts ./ n, digits=3))\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Generate the actual data points","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"X = Matrix{Float64}(undef, D, n)\nfor i in 1:n\n    component = labels[i]\n    X[:, i] = rand(rng, MvNormal(true_μs[:, component], true_Σs[component]))\nend\n\nprintln(\"Generated data summary:\")\nprintln(\"  Data shape: $(size(X)) (dimensions × samples)\")\nprintln(\"  Data range: x₁ ∈ [$(round(minimum(X[1,:]), digits=2)), $(round(maximum(X[1,:]), digits=2))], x₂ ∈ [$(round(minimum(X[2,:]), digits=2)), $(round(maximum(X[2,:]), digits=2))]\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Visualize the generated data colored by true component membership","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"p1 = scatter(\n    X[1, :], X[2, :];\n    group=labels,                    # Color by true component\n    title=\"GMM Samples (colored by true component)\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    markersize=4,\n    alpha=0.8,\n    legend=:topright,\n    palette=:Set1_3\n)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Add component centers for reference","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"for i in 1:k\n    scatter!(p1, [true_μs[1, i]], [true_μs[2, i]];\n        marker=:star, markersize=10, color=i,\n        markerstrokewidth=2, markerstrokecolor=:black,\n        label=\"Center $i\")\nend\n\ndisplay(p1)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Fit-a-New-Gaussian-Mixture-Model-to-the-Data","page":"Gaussian Mixture Model Example","title":"Fit a New Gaussian Mixture Model to the Data","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Now we simulate the realistic scenario: we observe only the data points X, not the true component labels or parameters. Our goal is to recover the underlying mixture structure using the EM algorithm.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"println(\"Initializing GMM for fitting...\")\nprintln(\"Note: We assume we know the correct number of components k=$k\")\nprintln(\"      (In practice, this often requires model selection)\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Initialize a GMM with the correct number of components but unknown parameters","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"fit_gmm = GaussianMixtureModel(k, D)\n\nprintln(\"Running EM algorithm to learn GMM parameters...\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Fit the model using EM algorithm","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"maxiter: maximum number of EM iterations\ntol: convergence tolerance (change in log-likelihood)\ninitialize_kmeans: use k-means to initialize component centers","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"class_probabilities, lls = fit!(fit_gmm, X;\n    maxiter=100,\n    tol=1e-6,\n    initialize_kmeans=true  # This often helps convergence\n)\n\nprintln(\"EM algorithm completed:\")\nprintln(\"  Converged after $(length(lls)) iterations\")\nprintln(\"  Final log-likelihood: $(round(lls[end], digits=2))\")\nprintln(\"  Log-likelihood improvement: $(round(lls[end] - lls[1], digits=2))\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Display learned parameters","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"println(\"\\nLearned GMM parameters:\")\nfor i in 1:k\n    println(\"  Component $i: μ = $(round.(fit_gmm.μₖ[:, i], digits=3)), π = $(round(fit_gmm.πₖ[i], digits=3))\")\nend","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Plot-Log-Likelihoods-to-Visualize-EM-Convergence","page":"Gaussian Mixture Model Example","title":"Plot Log-Likelihoods to Visualize EM Convergence","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"The EM algorithm should monotonically increase the log-likelihood at each iteration. Plotting this helps us verify convergence and understand the optimization process.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"p2 = plot(\n    lls;\n    xlabel=\"EM Iteration\",\n    ylabel=\"Log-Likelihood\",\n    title=\"EM Algorithm Convergence\",\n    label=\"Log-Likelihood\",\n    marker=:circle,\n    markersize=4,\n    linewidth=2,\n    grid=true\n)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Add annotations about convergence behavior","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"if length(lls) > 1\n    initial_rate = lls[min(5, end)] - lls[1]\n    final_rate = lls[end] - lls[max(1, end-5)]\n\n    annotate!(p2, length(lls)*0.7, lls[end]*0.95,\n        text(\"Final LL: $(round(lls[end], digits=1))\", 10))\n\n    if length(lls) < 100  # Converged before max iterations\n        annotate!(p2, length(lls)*0.7, lls[end]*0.90,\n            text(\"Converged in $(length(lls)) iterations\", 10))\n    end\nend\n\ndisplay(p2)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Visualize-Model-Contours-Over-the-Data","page":"Gaussian Mixture Model Example","title":"Visualize Model Contours Over the Data","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Create a comprehensive visualization showing both the data and the fitted model. We'll plot probability density contours for each learned component.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"println(\"Creating visualization of fitted GMM...\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Create a grid for plotting contours","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"x_range = range(minimum(X[1, :]) - 1, stop=maximum(X[1, :]) + 1, length=150)\ny_range = range(minimum(X[2, :]) - 1, stop=maximum(X[2, :]) + 1, length=150)\nxs = collect(x_range)\nys = collect(y_range)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Start with a scatter plot of the data (without true labels this time)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"p3 = scatter(\n    X[1, :], X[2, :];\n    markersize=3,\n    alpha=0.5,\n    color=:gray,\n    xlabel=\"x₁\",\n    ylabel=\"x₂\",\n    title=\"Data with Fitted GMM Components\",\n    legend=:topright,\n    label=\"Data points\"\n)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Plot probability density contours for each learned component","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"colors = [:red, :green, :blue]\nfor i in 1:fit_gmm.k\n    comp_dist = MvNormal(fit_gmm.μₖ[:, i], fit_gmm.Σₖ[i])\n    Z_i = [fit_gmm.πₖ[i] * pdf(comp_dist, [x, y]) for y in ys, x in xs]\n\n    contour!(\n        p3, xs, ys, Z_i;\n        levels=8,\n        linewidth=2,\n        c=colors[i],\n        label=\"Component $i (π=$(round(fit_gmm.πₖ[i], digits=2)))\"\n    )\n\n    scatter!(p3, [fit_gmm.μₖ[1, i]], [fit_gmm.μₖ[2, i]];\n        marker=:star, markersize=8, color=colors[i],\n        markerstrokewidth=2, markerstrokecolor=:black,\n        label=\"\")\nend\n\ndisplay(p3)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Analyze-Component-Assignments","page":"Gaussian Mixture Model Example","title":"Analyze Component Assignments","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Use the fitted model to assign each data point to its most likely component and compare with the true assignments.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"println(\"Analyzing component assignments...\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Get posterior probabilities for each data point class_probabilities[i, j] = P(component i | data point j)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"predicted_labels = [argmax(class_probabilities[:, j]) for j in 1:n]","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Calculate assignment accuracy (accounting for possible label permutation) Since EM can converge with components in different order, we need to find the best permutation of labels","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"function best_permutation_accuracy(true_labels, pred_labels, k)\n    best_acc = 0.0\n    best_perm = collect(1:k)\n\n    for perm in Combinatorics.permutations(1:k)\n        mapped_pred = [perm[pred_labels[i]] for i in 1:length(pred_labels)]\n        acc = mean(true_labels .== mapped_pred)\n        if acc > best_acc\n            best_acc = acc\n            best_perm = perm\n        end\n    end\n\n    return best_acc, best_perm\nend","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Calculate accuracy with best label permutation","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"accuracy, best_perm = best_permutation_accuracy(labels, predicted_labels, k)\nprintln(\"Component assignment accuracy: $(round(accuracy*100, digits=1))%\")\nprintln(\"Best label permutation: $best_perm\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Parameter-Recovery-Analysis","page":"Gaussian Mixture Model Example","title":"Parameter Recovery Analysis","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"println(\"\\n=== Parameter Recovery Assessment ===\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Compare true vs learned parameters (accounting for label permutation)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"mapped_μs = fit_gmm.μₖ[:, best_perm]\nmapped_πs = fit_gmm.πₖ[best_perm]\nmapped_Σs = fit_gmm.Σₖ[best_perm]","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Mean vector errors","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"μ_errors = [norm(true_μs[:, i] - mapped_μs[:, i]) for i in 1:k]\nprintln(\"Mean vector recovery errors:\")\nfor i in 1:k\n    println(\"  Component $i: $(round(μ_errors[i], digits=3))\")\nend","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Mixing weight errors","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"π_errors = [abs(true_πs[i] - mapped_πs[i]) for i in 1:k]\nprintln(\"Mixing weight recovery errors:\")\nfor i in 1:k\n    println(\"  Component $i: $(round(π_errors[i], digits=3))\")\nend","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Covariance matrix errors (Frobenius norm)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Σ_errors = [norm(true_Σs[i] - mapped_Σs[i]) for i in 1:k]\nprintln(\"Covariance matrix recovery errors:\")\nfor i in 1:k\n    println(\"  Component $i: $(round(Σ_errors[i], digits=3))\")\nend","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Create-Final-Comparison-Visualization","page":"Gaussian Mixture Model Example","title":"Create Final Comparison Visualization","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Create a side-by-side comparison of true vs learned GMM","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"p_true = scatter(X[1, :], X[2, :]; group=labels, title=\"True GMM\",\n                xlabel=\"x₁\", ylabel=\"x₂\", markersize=3, alpha=0.7,\n                palette=:Set1_3, legend=false)\n\np_learned = scatter(X[1, :], X[2, :]; group=predicted_labels, title=\"Learned GMM\",\n                   xlabel=\"x₁\", ylabel=\"x₂\", markersize=3, alpha=0.7,\n                   palette=:Set1_3, legend=false)\n\nfinal_comparison = plot(p_true, p_learned, layout=(1, 2), size=(800, 400))\ndisplay(final_comparison)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Model-Selection-Considerations","page":"Gaussian Mixture Model Example","title":"Model Selection Considerations","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"println(\"\\n=== Model Selection Notes ===\")\nprintln(\"In this tutorial, we assumed the correct number of components k=$k was known.\")\nprintln(\"In practice, you would need to select k using techniques like:\")\nprintln(\"  - Information criteria (AIC, BIC)\")\nprintln(\"  - Cross-validation\")\nprintln(\"  - Gap statistic\")\nprintln(\"  - Elbow method on within-cluster sum of squares\")\nprintln(\"\")\nprintln(\"The EM algorithm is guaranteed to converge to a local optimum, but not\")\nprintln(\"necessarily the global optimum. Multiple random initializations are often\")\nprintln(\"used to find better solutions.\")","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Summary","page":"Gaussian Mixture Model Example","title":"Summary","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"This tutorial demonstrated the complete workflow for Gaussian Mixture Models:","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"Model Structure: Independent observations from a mixture of Gaussian distributions\nParameter Learning: EM algorithm iteratively improves component parameters and mixing weights\nInitialization: K-means initialization helps EM converge to better solutions\nVisualization: Contour plots reveal the learned probability landscape\nEvaluation: Component assignment accuracy and parameter recovery assessment\nLabel Permutation: Handling the identifiability issue where components can be reordered\nConvergence Monitoring: Log-likelihood plots verify proper algorithm convergence","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"GMMs provide a flexible framework for clustering and density estimation, serving as building blocks for more complex probabilistic models while remaining interpretable and efficient to fit.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl is a comprehensive Julia package for state space modeling, designed specifically with neuroscientific applications in mind. The package provides efficient implementations of various state space models along with tools for parameter estimation, state inference, and model selection.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install StateSpaceDynamics.jl, start up Julia and type the following code-snipped into the REPL. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"StateSpaceDynamics\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"or alternatively, you can enter the package manager by typing ] and then run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add StateSpaceDynamics","category":"page"},{"location":"#What-are-State-Space-Models?","page":"Home","title":"What are State Space Models?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"State space models are a class of probabilistic models that describe the evolution of a system through two main components - a latent and observation process. The latent process is a stochastic process that is not directly observed, but is used to generate the observed data. The observation process is a conditional distribution that describes how the observed data is generated from the latent process.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In their most general form, state space models can be written as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim p(x_t+1  x_t) \n    y_t sim p(y_t  x_t)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x_t is the latent state at time t and y_t is the observed data at time t.","category":"page"},{"location":"#Example:-Linear-Dynamical-Systems","page":"Home","title":"Example: Linear Dynamical Systems","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A fundamental example is the Linear Dynamical System (LDS), which combines linear dynamics with Gaussian noise. The LDS can be expressed in two equivalent forms:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Equation form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 = A x_t + b + epsilon_t \n    y_t = C x_t + d + delta_t\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfA is the state transition matrix\nmathbfC is the observation matrix  \nmathbfb and mathbfd are bias terms\nboldsymbolepsilon_t and boldsymboldelta_t are Gaussian noise terms with covariances mathbfQ and mathbfR respectively","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distributional form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim mathcalN(A x_t + b Q) \n    y_t sim mathcalN(C x_t + d R)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where mathbfQ and mathbfR are the state and observation noise covariance matrices, respectively.","category":"page"},{"location":"#Models-Implemented","page":"Home","title":"Models Implemented","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl implements several types of state space models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Linear Dynamical Systems (LDS)\nGaussian LDS\nPoisson LDS\nHidden Markov Models (HMM)\nGaussian emissions\nRegression-based emissions\nGaussian regression\nBernoulli regression\nPoisson regression\nAutoregressive emissions","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a simple example on how to create a Gaussian SSM.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StateSpaceDynamics\nusing LinearAlgebra\n\n# Define model dimensions\nlatent_dim = 3\nobs_dim = 10\n\n# Define state model parameters\nA = 0.95 * I(latent_dim)\nQ = 0.01 * I(latent_dim)\nx0 = zeros(latent_dim)\nP0 = I(latent_dim)\nstate_model = GaussianStateModel(A, Q, x0, P0)\n\n# Define observation model parameters\nC = randn(obs_dim, latent_dim)\nR = 0.1 * I(obs_dim)\nobs_model = GaussianObservationModel(C, R)\n\n# Construct the LDS\nlds = LinearDynamicalSystem(state_model, obs_model, latent_dim, obs_dim, fill(true, 6))","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you encounter a bug or would like to contribute to the package, please open an issue on our GitHub repository. Once the suggested change has received positive feedback feel free to submit a PR adhering to the blue style guide.","category":"page"},{"location":"#Citing-StateSpaceDynamics.jl","page":"Home","title":"Citing StateSpaceDynamics.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Our work is currently under review in the Journal of Open Source Software. For now, if you use StateSpaceDynamics.jl in your research, please use the following bibtex citation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@software{Senne_Zenodo_SSD,\n  author       = {Ryan Senne and Zachary Loschinskey and James Fourie and Carson Loughridge and Brian DePasquale},\n  title        = {StateSpaceDynamics.jl},\n  month        = jun,\n  year         = 2025,\n  publisher    = {Zenodo},\n  version      = {v1.0.0},\n  doi          = {10.5281/zenodo.15668420},\n  url          = {https://doi.org/10.5281/zenodo.15668420}\n}","category":"page"}]
}
