var documenterSearchIndex = {"docs":
[{"location":"MixtureModels/#Mixture-Models","page":"Mixture Models","title":"Mixture Models","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"A mixture model is a probability distribution which, given a finite k  0, samples from k different distributions f_i(x)  i in 1k randomly, where the probability of sampling from f_i(x) is pi_i. Generally, a mixture model is written in the form of:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"f_mix(x Theta pi) = sum_k=1^K pi_k f_k(x)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where f_i(x) is called the ith component and pi_i is called the ith mixing coeffiecent.","category":"page"},{"location":"MixtureModels/#Gaussian-Mixture-Model","page":"Mixture Models","title":"Gaussian Mixture Model","text":"","category":"section"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel\n\nA Gaussian Mixture Model for clustering and density estimation.\n\nFields\n\nk::Int: Number of clusters.\nμₖ::Matrix{<:Real}: Means of each cluster (dimensions: data_dim x k).\nΣₖ::Array{Matrix{<:Real}, 1}: Covariance matrices of each cluster.\nπₖ::Vector{Float64}: Mixing coefficients for each cluster.\n\nExamples\n\ngmm = GaussianMixtureModel(3, 2) # Create a Gaussian Mixture Model with 3 clusters and 2-dimensional data\nfit!(gmm, data)\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel-Tuple{Int64, Int64}","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel(k::Int, data_dim::Int)\n\nConstructor for GaussianMixtureModel. Initializes Σₖ's covariance matrices to the  identity, πₖ to a uniform distribution, and μₖ's means to zeros.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(gmm::GaussianMixtureModel, data::Matrix{<:Real}; <keyword arguments>)\n\nFits a Gaussian Mixture Model (GMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\ngmm::GaussianMixtureModel: The Gaussian Mixture Model to be fitted.\ndata::Matrix{<:Real}: The dataset on which the model will be fitted, where each row represents a data point.\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the GMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\nExample\n\ndata = rand(2, 100)  # Generate some random data\ngmm = GaussianMixtureModel(k=3, d=2)  # Initialize a GMM with 3 components and 2-dimensional data\nclass_probabilities = fit!(gmm, data, maxiter=100, tol=1e-4, initialize_kmeans=true)\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.log_likelihood-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.log_likelihood","text":"log_likelihood(gmm::GaussianMixtureModel, data::Matrix{<:Real})\n\nCompute the log-likelihood of the data given the Gaussian Mixture Model (GMM). The data matrix should be of shape (# observations, # features).\n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#Poisson-Mixture-Model","page":"Mixture Models","title":"Poisson Mixture Model","text":"","category":"section"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel\n\nA Poisson Mixture Model for clustering and density estimation.\n\nFields\n\nk::Int: Number of poisson-distributed clusters.\nλₖ::Vector{Float64}: Means of each cluster.\nπₖ::Vector{Float64}: Mixing coefficients for each cluster.\n\nExamples\n\njulia pmm = PoissonMixtureModel(3) # 3 clusters, 2-dimensional data fit!(pmm, data)\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel-Tuple{Int64}","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel(k::Int)\n\nConstructor for PoissonMixtureModel. Initializes λₖ's means to  ones and πₖ to a uniform distribution.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(pmm::PoissonMixtureModel, data::Matrix{Int}; <keyword arguments>)\n\nFits a Poisson Mixture Model (PMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\npmm::PoissonMixtureModel: The Poisson Mixture Model to be fitted.\ndata::Matrix{Int}: The dataset on which the model will be fitted, where each row represents a data point.\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the PMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\nExample\n\ndata = rand(1:10, 100, 1)  # Generate some random integer data\npmm = PoissonMixtureModel(k=3)  # Initialize a PMM with 3 components\nclass_probabilities = fit!(pmm, data, maxiter=100, tol=1e-4, initialize_kmeans=true)\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.log_likelihood-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.log_likelihood","text":"log_likelihood(pmm::PoissonMixtureModel, data::Matrix{Int})\n\nCompute the log-likelihood of the data given the Poisson Mixture Model (PMM). The data matrix should be of shape (# observations, # features).\n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#Misc","page":"Miscellaneous","title":"Misc","text":"","category":"section"},{"location":"Misc/#StateSpaceDynamics.FilterSmooth","page":"Miscellaneous","title":"StateSpaceDynamics.FilterSmooth","text":"\"     FilterSmooth{T<:Real}\n\nA mutable structure for storing smoothed estimates and associated covariance matrices in a filtering or smoothing algorithm.\n\nType Parameters\n\nT<:Real: The numerical type used for all fields (e.g., Float64, Float32).\n\nFields\n\nx_smooth::Matrix{T}   The matrix containing smoothed state estimates over time. Each column typically represents the state vector at a given time step.\np_smooth::Array{T, 3}   The posterior covariance matrices with dimensions (latentdim, latentdim, time_steps)\nE_z::Array{T, 3}   The expected latent states, size (statedim, T, ntrials).\nE_zz::Array{T, 4}   The expected value of zt * zt', size (statedim, statedim, T, n_trials).\nE_zz_prev::Array{T, 4}   The expected value of zt * z{t-1}', size (statedim, statedim, T, n_trials).\n\nExample\n\n```julia\n\nInitialize a FilterSmooth object with Float64 type\n\nfilter = FilterSmooth{Float64}(     xsmooth = zeros(10, 100),     psmooth = zeros(10, 10, 100),     Ez = zeros(10, 5, 100),     Ezz = zeros(10, 10, 5, 100),     Ezzprev = zeros(10, 10, 5, 100) )\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ForwardBackward","page":"Miscellaneous","title":"StateSpaceDynamics.ForwardBackward","text":"ForwardBackward{T<:Real}\n\nA mutable struct that encapsulates the forward–backward algorithm outputs for a hidden Markov model (HMM).\n\nFields\n\nloglikelihoods::Matrix{T}: Matrix of log-likelihoods for each observation and state.\nα::Matrix{T}: The forward probabilities (α) for each time step and state.\nβ::Matrix{T}: The backward probabilities (β) for each time step and state.\nγ::Matrix{T}: The state occupancy probabilities (γ) for each time step and state.\nξ::Array{T,3}: The pairwise state occupancy probabilities (ξ) for consecutive time steps and state pairs.\n\nTypically, α and β are computed by the forward–backward algorithm to find the likelihood of an observation sequence. γ and ξ are derived from these calculations to estimate how states transition over time.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ProbabilisticPCA","page":"Miscellaneous","title":"StateSpaceDynamics.ProbabilisticPCA","text":"mutable struct ProbabilisticPCA\n\nProbabilistic PCA model from Bishop's Pattern Recognition and Machine Learning.\n\nFields:\n\nW: Weight matrix that maps from latent space to data space.\nσ²: Noise variance\nμ: Mean of the data\nk: Number of latent dimensions\nD: Number of features\nz: Latent variables\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ProbabilisticPCA-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.ProbabilisticPCA","text":"ProbabilisticPCA(;W::Matrix{<:AbstractFloat}, σ²:: <: AbstractFloat, μ::Matrix{<:AbstractFloat}, k::Int, D::Int)\n\nConstructor for ProbabilisticPCA model.\n\n# Args:\n\n- W::Matrix{<:AbstractFloat}: Weight matrix that maps from latent space to data space.\n\n- σ²:: <: AbstractFloat: Noise variance\n\n- μ::Matrix{<:AbstractFloat}: Mean of the data\n\n- k::Int: Number of latent dimensions\n\n- D::Int: Number of features\n\n# Example:\n\n```julia\n\n# PPCA with unknown parameters\n\nppca = ProbabilisticPCA(k=1, D=2)\n\n# PPCA with known parameters\n\nppca = ProbabilisticPCA(W=rand(2, 1), σ²=0.1, μ=rand(2), k=1, D=2)\n\n```\n\n\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.E_Step-Tuple{ProbabilisticPCA, Matrix{<:Real}}","page":"Miscellaneous","title":"StateSpaceDynamics.E_Step","text":"E_Step(ppca::ProbabilisticPCA, X::Matrix{<:AbstractFloat})\n\nExpectation step of the EM algorithm for PPCA. See Bishop's Pattern Recognition and Machine Learning for more details.\n\nArgs:\n\nppca::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nE_Step(ppca, rand(10, 2))\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.M_Step!-Tuple{ProbabilisticPCA, Matrix{<:Real}, AbstractArray, AbstractArray}","page":"Miscellaneous","title":"StateSpaceDynamics.M_Step!","text":"M_Step!(model::ProbabilisticPCA, X::Matrix{<:AbstractFloat}, E_z::Matrix{<:AbstractFloat}, E_zz::Array{<:AbstractFloat, 3}\n\nMaximization step of the EM algorithm for PPCA. See Bishop's Pattern Recognition and Machine Learning for more details.\n\nArgs:\n\nmodel::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\nE_z::Matrix{<:AbstractFloat}: E[z]\nE_zz::Matrix{<:AbstractFloat}: E[zz']\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nE_z, E_zz = E_Step(ppca, rand(10, 2))\nM_Step!(ppca, rand(10, 2), E_z, E_zzᵀ)\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.fit!","page":"Miscellaneous","title":"StateSpaceDynamics.fit!","text":"fit!(model::ProbabilisticPCA, X::Matrix{<:AbstractFloat}, max_iter::Int=100, tol::AbstractFloat=1e-6)\n\nFit the PPCA model to the data using the EM algorithm.\n\nArgs:\n\nmodel::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\nmax_iter::Int: Maximum number of iterations\ntol::AbstractFloat: Tolerance for convergence\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nfit!(ppca, rand(10, 2))\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.loglikelihood-Tuple{ProbabilisticPCA, Matrix{<:Real}}","page":"Miscellaneous","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::ProbabilisticPCA, X::Matrix{<:AbstractFloat})\n\nCalculate the log-likelihood of the data given the PPCA model.\n\nArgs:\n\nmodel::ProbabilisticPCA: PPCA model\nX::Matrix{<:AbstractFloat}: Data matrix\n\nExamples:\n\nppca = ProbabilisticPCA(K=1, D=2)\nloglikelihood(ppca, rand(10, 2))\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.GaussianHMM-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.GaussianHMM","text":"GaussianHMM(; K::Int, output_dim::Int, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Hidden Markov Model with Gaussian Emissions\n\nArguments\n\nK::Int: The number of hidden states\noutput_dim::Int: The dimensionality of the observation\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization)\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization)\n\nReturns\n\n::HiddenMarkovModel: Hidden Markov Model Object with Gaussian Emissions\n\n```\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingBernoulliRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingBernoulliRegression","text":"SwitchingBernoulliRegression(; K::Int, input_dim::Int, include_intercept::Bool=true, β::Vector{<:Real}=if include_intercept zeros(input_dim + 1) else zeros(input_dim) end, λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching Bernoulli Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input data.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model (defaults to true).\nβ::Vector{<:Real}: The regression coefficients (defaults to zeros). \nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization).\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Bernoulli Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingGaussianRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingGaussianRegression","text":"SwitchingGaussianRegression(; \n    K::Int,\n    input_dim::Int,\n    output_dim::Int,\n    include_intercept::Bool = true,\n    β::Matrix{<:Real} = if include_intercept\n        zeros(input_dim + 1, output_dim)\n    else\n        zeros(input_dim, output_dim)\n    end,\n    Σ::Matrix{<:Real} = Matrix{Float64}(I, output_dim, output_dim),\n    λ::Float64 = 0.0,\n    A::Matrix{<:Real} = initialize_transition_matrix(K),\n    πₖ::Vector{Float64} = initialize_state_distribution(K)\n)\n\nCreate a Switching Gaussian Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input features.\noutput_dim::Int: The dimensionality of the output predictions.\ninclude_intercept::Bool: Whether to include an intercept in the regression model (default is true).\nβ::Matrix{<:Real}: The regression coefficients (defaults to zeros based on input_dim and output_dim).\nΣ::Matrix{<:Real}: The covariance matrix of the Gaussian emissions (defaults to an identity matrix).\nλ::Float64: The regularization parameter for the regression (default is 0.0).\nA::Matrix{<:Real}: The transition matrix of the Hidden Markov Model (defaults to random initialization).\nπₖ::Vector{Float64}: The initial state distribution of the Hidden Markov Model (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Gaussian Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridgm-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Array{Matrix{T}, 1}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridgm","text":"block_tridgm(main_diag::Vector{Matrix{T}}, upper_diag::Vector{Matrix{T}}, lower_diag::Vector{Matrix{T}}) where {T<:Real}\n\nConstruct a block tridiagonal matrix from three vectors of matrices.\n\nArguments\n\nmain_diag::Vector{Matrix{T}}: Vector of matrices for the main diagonal.\nupper_diag::Vector{Matrix{T}}: Vector of matrices for the upper diagonal.\nlower_diag::Vector{Matrix{T}}: Vector of matrices for the lower diagonal.\n\nReturns\n\nA sparse matrix representing the block tridiagonal matrix.\n\nThrows\n\nErrorException if the lengths of upper_diag and lower_diag are not one less than the length of main_diag.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Array{Matrix{T}, 1}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse","text":"block_tridiagonal_inverse(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix.\n\nArguments\n\nA: Lower diagonal blocks.\nB: Main diagonal blocks.\nC: Upper diagonal blocks.\n\nReturns\n\nλii: Diagonal blocks of the inverse.\nλij: Off-diagonal blocks of the inverse.\n\nNotes: This implementation is from the paper:\n\n\"An Accelerated Lambda Iteration Method for Multilevel Radiative Transfer” Rybicki, G.B., and Hummer, D.G., Astronomy and Astrophysics, 245, 171–181 (1991), Appendix B.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse_static-Union{Tuple{T}, Tuple{Array{Matrix{T}, 1}, Array{Matrix{T}, 1}, Array{Matrix{T}, 1}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse_static","text":"block_tridiagonal_inverse_static(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix using static matrices. See block_tridiagonal_inverse for details.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.euclidean_distance-Tuple{AbstractVector{Float64}, AbstractVector{Float64}}","page":"Miscellaneous","title":"StateSpaceDynamics.euclidean_distance","text":"euclidean_distance(a::AbstractVector{Float64}, b::AbstractVector{Float64})\n\nCalculate the Euclidean distance between two points.\n\nArguments\n\na::AbstractVector{Float64}: The first point.\nb::AbstractVector{Float64}: The second point.\n\nReturns\n\nThe Euclidean distance between a and b.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.gaussian_entropy-Union{Tuple{LinearAlgebra.Symmetric{T, S} where S<:(AbstractMatrix{<:T})}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.gaussian_entropy","text":"gaussian_entropy(H::Symmetric{T}) where T <: Real\n\nCalculate the entropy of a Gaussian distribution with Hessian (i.e. negative precision) matrix H.\n\nArguments\n\nH::Symmetric{T}: The Hessian matrix.\n\nReturns\n\nThe entropy of the Gaussian distribution.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::Matrix{<:Real}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6)\n\nPerform K-means clustering on the input data.\n\nArguments\n\ndata::Matrix{<:Real}: The input data matrix where each row is a data point.\nk_means::Int: The number of clusters.\nmax_iters::Int=100: Maximum number of iterations.\ntol::Float64=1e-6: Convergence tolerance.\n\nReturns\n\nA tuple containing the final centroids and cluster labels for each data point.\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering-2","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::Vector{Float64}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6)\n\nPerform K-means clustering on vector data.\n\nArguments\n\ndata::Vector{Float64}: The input data vector.\nk_means::Int: The number of clusters.\nmax_iters::Int=100: Maximum number of iterations.\ntol::Float64=1e-6: Convergence tolerance.\n\nReturns\n\nA tuple containing the final centroids and cluster labels for each data point.\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Tuple{Matrix{<:Real}, Int64}","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::Matrix{<:Real}, k_means::Int)\n\nPerform K-means++ initialization for cluster centroids.\n\nArguments\n\ndata::Matrix{<:Real}: The input data matrix where each row is a data point.\nk_means::Int: The number of clusters.\n\nReturns\n\nA matrix of initial centroids for K-means clustering.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Tuple{Vector{Float64}, Int64}","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::Vector{Float64}, k_means::Int)\n\nPerform K-means++ initialization for cluster centroids on vector data.\n\nArguments\n\ndata::Vector{Float64}: The input data vector.\nk_means::Int: The number of clusters.\n\nReturns\n\nA matrix of initial centroids for K-means clustering.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.logistic-Tuple{Real}","page":"Miscellaneous","title":"StateSpaceDynamics.logistic","text":"logistic(x::Real)\n\nCalculate the logistic function in a numerically stable way.\n\nArguments\n\nx::Real: The input value.\n\nReturns\n\nThe result of the logistic function applied to x.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.make_posdef!-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.make_posdef!","text":"make_posdef!(A::Matrix{T}) where {T}\n\nEnsure that a matrix is positive definite by adjusting its eigenvalues.\n\nArguments\n\nA::Matrix{T}: The input matrix.\n\nReturns\n\nA positive definite matrix derived from A.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.random_rotation_matrix-Tuple{Int64}","page":"Miscellaneous","title":"StateSpaceDynamics.random_rotation_matrix","text":"random_rotation_matrix(n::Int)\n\nGenerate a random rotation matrix of size n x n.\n\nArguments\n\nn::Int: The size of the rotation matrix.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.row_matrix-Tuple{AbstractVector}","page":"Miscellaneous","title":"StateSpaceDynamics.row_matrix","text":"row_matrix(x::AbstractVector)\n\nConvert a vector to a row matrix.\n\nArguments\n\nx::AbstractVector: The input vector.\n\nReturns\n\nA row matrix (1 × n) containing the elements of x.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.stabilize_covariance_matrix-Tuple{Matrix{<:Real}}","page":"Miscellaneous","title":"StateSpaceDynamics.stabilize_covariance_matrix","text":"stabilize_covariance_matrix(Σ::Matrix{<:Real})\n\nStabilize a covariance matrix by ensuring it is symmetric and positive definite.\n\nArguments\n\nΣ::Matrix{<:Real}: The input covariance matrix.\n\nReturns\n\nA stabilized version of the input covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#What-is-a-Hidden-Markov-Model?","page":"Hidden Markov Models","title":"What is a Hidden Markov Model?","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model (HMM) is a graphical model that describes how systems change over time. When modeling a time series with T observations using an HMM, we assume that the observed data y_1T depends on hidden states x_1T that are not observed. Specifically, an HMM is a type of state-space model in which the hidden states are discrete.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The three components of an HMM are as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"An initial state distribution (pi): which hidden states we are likely to start in.\nA transition matrix (A): how the hidden states evolve over time.\nAn emission model: how the hidden states generate the observed data.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is given by:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beginalign*\n    x_1 sim textCat(pi) \n    x_t mid x_t-1 sim textCat(A_x_t-1 ) \n    y_t mid x_t sim p(y_t mid theta_x_t)\nendalign*","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t is the hidden (discrete) state at time t\ny_t is the observed data at time t\npi is the initial state distribution\nmathbfA is the state transition matrix\ntheta_x_t are the parameters of the emission distribution for state x_t","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The emission model can take many forms: Gaussian, Poisson, Bernoulli, categorical, etc... In the case of a Gaussian emission distribution, this becomes:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) sim mathcalN(mu_k Sigma_k)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"mu_k is the mean of the emission distribution for state k\nSigma_k is the covariance of the emission distribution for state k","category":"page"},{"location":"HiddenMarkovModels/#What-is-a-Generalized-Linear-Model-Hidden-Markov-Model","page":"Hidden Markov Models","title":"What is a Generalized Linear Model - Hidden Markov Model","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model - Generalized Linear Model (GLM-HMM) - also known as Switching Regression Model - is an extension to classic HMMs where the emission models are state-dependent GLMs that link an observed input to an observed output. This formulation allows each hidden state to define its own regression relationship between inputs and outputs, enabling the model to capture complex, state-dependent dynamics in the data. Currently, StateSpaceDynamics.jl support Gaussian, Bernoulli, Poisson, and Autoregressive GLMs as emission models.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beginalign*\n    x_1 sim textCat(pi) \n    x_t mid x_t-1 sim textCat(A_x_t-1 ) \n    y_t mid x_t u_t sim p(y_t mid theta_x_t u_t)\nendalign*","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t is the hidden (discrete) state at time t\ny_t is the observed output at time t\nu_t is the observed input (covariate) at time t\ntheta_x_t are the parameters of the GLM emission model for state x_t","category":"page"},{"location":"HiddenMarkovModels/#Example-Emission-Models","page":"Hidden Markov Models","title":"Example Emission Models","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For example, if the emission is a Gaussian GLM:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) u_t sim mathcalN(mu_k + beta_k^top u_t sigma_k^2)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k are the regression weights for state k\nsigma_k^2 is the state-dependent variance\nmu_k is the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"If the emission is Bernoulli (for binary outputs):","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) u_t sim textBernoulli left( sigma left( mu_k + beta_k^top u_t right) right)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k are the regression weights for state k\nsigma(cdot) is the logistic sigmoid function for binary outputs\nmu_k is the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/#Learning-in-an-HMM","page":"Hidden Markov Models","title":"Learning in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"StateSpaceDynamics.jl implements Expectation-Maximization (EM) for parameter learning in both HMMs and GLM-HMMs. EM is an iterative method for finding maximum likelihood estimates of the parameters in graphical models with hidden variables. ","category":"page"},{"location":"HiddenMarkovModels/#Expectation-Step-(E-step)","page":"Hidden Markov Models","title":"Expectation Step (E-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the expectation step (E-step), we calculate the posterior distribution of the latent states given the current parameters of the model:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"p(X mid Y theta_textold)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"We use dynamic programming to efficiently calculate this posterior using the forward and backward recursions for HMMs. This posterior is then used to construct the expectation of the complete data log-likelihood, also known as the Q-function:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Q(theta theta_textold) = sum_X p(X mid Y theta_textold) ln p(Y X mid theta)","category":"page"},{"location":"HiddenMarkovModels/#Maximization-Step-(M-step)","page":"Hidden Markov Models","title":"Maximization Step (M-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the maximization step (M-step), we maximize this expectation with respect to the parameters theta. Specifically:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For the initial state distribution and the transition matrix, we use analytical updates for the parameters, derived using Lagrange multipliers.\nFor emission models in the case of HMMs, we also implement analytical updates.\nIf the emission model is a GLM, we use Optim.jl to numerically optimize the objective function.","category":"page"},{"location":"HiddenMarkovModels/#Inference-in-an-HMM","page":"Hidden Markov Models","title":"Inference in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For state inference in Hidden Markov Models (HMMs), we implement two common algorithms:","category":"page"},{"location":"HiddenMarkovModels/#Forward-Backward-Algorithm","page":"Hidden Markov Models","title":"Forward-Backward Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Forward-Backward algorithm is used to compute the posterior state probabilities at each time step. Given the observed data, it calculates the probability of being in each possible hidden state at each time step, marginalizing over all possible state sequences.","category":"page"},{"location":"HiddenMarkovModels/#Viterbi-Algorithm","page":"Hidden Markov Models","title":"Viterbi Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Viterbi algorithm is used for best state sequence labeling. It finds the most likely sequence of hidden states given the observed data. This is done by dynamically computing the highest probability path through the state space, which maximizes the likelihood of the observed sequence.","category":"page"},{"location":"HiddenMarkovModels/#Reference","page":"Hidden Markov Models","title":"Reference","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For a complete mathematical formulation of the relevant HMM and HMM-GLM learning and inference algorithms, we recommend Pattern Recognition and Machine Learning, Chapter 13 by Christopher Bishop.","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.HiddenMarkovModel","page":"Hidden Markov Models","title":"StateSpaceDynamics.HiddenMarkovModel","text":"HiddenMarkovModel\n\nA Hidden Markov Model (HMM) with custom emissions.\n\nFields\n\nK::Int: Number of states.\nB::Vector=Vector(): Vector of emission models.\nemission=nothing: If B is missing emissions, clones of this model will be used to fill in the rest.\nA::Matrix{<:Real}: Transition matrix.\nπₖ::Vector{Float64}: Initial state distribution.\n\n\n\n\n\n","category":"type"},{"location":"HiddenMarkovModels/#Base.rand","page":"Hidden Markov Models","title":"Base.rand","text":"sample(model::HiddenMarkovModel, data...; n::Int)\n\nGenerate n samples from a Hidden Markov Model. Returns a tuple of the state sequence and the observation sequence.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to sample from.\ndata...: The data to fit the Hidden Markov Model. Requires the same format as the emission model.\nn::Int: The number of samples to generate.\n\nReturns\n\nstate_sequence::Vector{Int}: The state sequence, where each element is an integer 1:K.\nobservation_sequence::Matrix{Float64}: The observation sequence. This takes the form of the emission model's output.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.class_probabilities","page":"Hidden Markov Models","title":"StateSpaceDynamics.class_probabilities","text":"function class_probabilities(model::HiddenMarkovModel, Y::Vector{<:Matrix{<:Real}}, X::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing;)\n\nCalculate the class probabilities at each time point using forward backward algorithm on multiple trials of data\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Vectpr{<:Matrix{<:Real}}: The trials of emission data\nX::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing: Optional trials of input data for fitting Switching Regression Models\n\nReturns\n\nclass_probabilities::Vector{<:Matrix{Float64}}: Each trial's class probabilities at each timepoint\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.class_probabilities-2","page":"Hidden Markov Models","title":"StateSpaceDynamics.class_probabilities","text":"function class_probabilities(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nCalculate the class probabilities at each time point using forward backward algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nclass_probabilities::Matrix{Float64}: The class probabilities at each timepoint\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.fit!","page":"Hidden Markov Models","title":"StateSpaceDynamics.fit!","text":"fit!(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real}, Nothing}=nothing; max_iters::Int=100, tol::Float64=1e-6)\n\nFit the Hidden Markov Model using the EM algorithm.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data.\nX::Union{Matrix{<:Real}, Nothing}=nothing: Optional input data for fitting Switching Regression Models\nmax_iters::Int=100: The maximum number of iterations to run the EM algorithm.\ntol::Float64=1e-6: When the log likelihood is improving by less than this value, the algorithm will stop.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.fit!-2","page":"Hidden Markov Models","title":"StateSpaceDynamics.fit!","text":"fit!(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real}, Nothing}=nothing; max_iters::Int=100, tol::Float64=1e-6)\n\nFit the Hidden Markov Model to multiple trials of data using the EM algorithm.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Vector{<:Matrix{<:Real}}: The trialized emission data.\nX::Union{Vector{<:Matrix{<:Real}}, Nothing}=nothing: Optional input data for fitting Switching Regression Models\nmax_iters::Int=100: The maximum number of iterations to run the EM algorithm.\ntol::Float64=1e-6: When the log likelihood is improving by less than this value, the algorithm will stop.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.viterbi","page":"Hidden Markov Models","title":"StateSpaceDynamics.viterbi","text":"viterbi(model::HiddenMarkovModel, Y::Vector{<:Matrix{<:Real}}, X::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing;)\n\nGet most likely class labels using the Viterbi algorithm for multiple trials of data\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Vectpr{<:Matrix{<:Real}}: The trials of emission data\nX::Union{Vector{<:Matrix{<:Real}},Nothing}=nothing: Optional trials of input data for fitting Switching Regression Models\n\nReturns\n\nbest_path::Vector{<:Vector{Float64}}: Each trial's best state path\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.viterbi-2","page":"Hidden Markov Models","title":"StateSpaceDynamics.viterbi","text":"viterbi(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nGet most likely class labels using the Viterbi algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nbest_path::Vector{Float64}: The most likely state label at each timepoint\n\n\n\n\n\n","category":"function"},{"location":"SLDS/#Swiching-Linear-Dynamical-Systems","page":"Switching Linear Dynamical Systems","title":"Swiching Linear Dynamical Systems","text":"","category":"section"},{"location":"SLDS/#StateSpaceDynamics.SwitchingLinearDynamicalSystem","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.SwitchingLinearDynamicalSystem","text":"Switching Linear Dynamical System\n\n\n\n\n\n","category":"type"},{"location":"SLDS/#Base.rand-Tuple{AbstractRNG, SwitchingLinearDynamicalSystem, Int64}","page":"Switching Linear Dynamical Systems","title":"Base.rand","text":"Generate synthetic data with switching LDS models\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{SwitchingLinearDynamicalSystem, Matrix{T}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(slds::SwitchingLinearDynamicalSystem, y::Matrix{T}; \n     max_iter::Int=1000, \n     tol::Real=1e-12, \n     ) where {T<:Real}\n\nFit a Switching Linear Dynamical System using the variational Expectation-Maximization (EM) algorithm with Kalman smoothing.\n\nArguments\n\nslds::SwitchingLinearDynamicalSystem: The Switching Linear Dynamical System to be fitted.\ny::Matrix{T}: Observed data, size (obsdim, Tsteps).\n\nKeyword Arguments\n\nmax_iter::Int=1000: Maximum number of EM iterations.\ntol::Real=1e-12: Convergence tolerance for log-likelihood change.\n\nReturns\n\nmls::Vector{T}: Vector of log-likelihood values for each iteration.\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.hmm_elbo-Tuple{SwitchingLinearDynamicalSystem, StateSpaceDynamics.ForwardBackward}","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.hmm_elbo","text":"\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.initialize_slds-Tuple{}","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.initialize_slds","text":"Initialize a Switching Linear Dynamical System with random parameters.\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.mstep!-Union{Tuple{T}, Tuple{SwitchingLinearDynamicalSystem, Array{StateSpaceDynamics.FilterSmooth{T}, 1}, Matrix{T}, StateSpaceDynamics.ForwardBackward}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.mstep!","text":"\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.variational_expectation!-Union{Tuple{T}, Tuple{SwitchingLinearDynamicalSystem, Any, StateSpaceDynamics.ForwardBackward, Array{StateSpaceDynamics.FilterSmooth{T}, 1}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.variational_expectation!","text":"variational_expectation!(model::SwitchingLinearDynamicalSystem, y, FB, FS) -> Float64\n\nCompute the variational expectation (Evidence Lower Bound, ELBO) for a Switching Linear Dynamical System.\n\nArguments\n\nmodel::SwitchingLinearDynamicalSystem:   The switching linear dynamical system model containing parameters such as the number of regimes (K), system matrices (B), and observation models.\ny:   The observation data, typically a matrix where each column represents an observation at a specific time step.\nFB:   The forward-backward object that holds variables related to the forward and backward passes, including responsibilities (γ).\nFS:   An array of FilterSmooth objects, one for each regime, storing smoothed state estimates and covariances.\n\nReturns\n\nFloat64:   The total Evidence Lower Bound (ELBO) computed over all regimes and observations.\n\nDescription\n\nThis function performs the variational expectation step for a Switching Linear Dynamical System by executing the following operations:\n\nExtract Responsibilities:   Retrieves the responsibilities (γ) from the forward-backward object and computes their exponentials (hs).\nParallel Smoothing and Sufficient Statistics Calculation:   For each regime k from 1 to model.K, the function:\nPerforms smoothing using the smooth function to obtain smoothed states (x_smooth), covariances (p_smooth), inverse off-diagonal terms, and total entropy.\nComputes sufficient statistics (E_z, E_zz, E_zz_prev) from the smoothed estimates.\nCalculates the ELBO contribution for the current regime and accumulates it into ml_total.\nUpdate Variational Distributions:  \nComputes the variational distributions (qs) from the smoothed states, which are stored as log-likelihoods in FB.\nExecutes the forward and backward passes to update the responsibilities (γ) based on the new qs.\nRecalculates the responsibilities (γ) to reflect the updated variational distributions.\nReturn ELBO:   Returns the accumulated ELBO (ml_total), which quantifies the quality of the variational approximation.\n\nExample\n\n```julia\n\nAssume model is an instance of SwitchingLinearDynamicalSystem with K regimes\n\ny is the observation matrix of size (numfeatures, numtime_steps)\n\nFB is a pre-initialized ForwardBackward object\n\nFS is an array of FilterSmooth objects, one for each regime\n\nelbo = variational_expectation!(model, y, FB, FS) println(\"Computed ELBO: \", elbo)\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.variational_qs!-Union{Tuple{T}, Tuple{Array{GaussianObservationModel{T}, 1}, StateSpaceDynamics.ForwardBackward, Any, Array{StateSpaceDynamics.FilterSmooth{T}, 1}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.variational_qs!","text":"variational_qs!(model::Vector{GaussianObservationModel{T}}, FB, y, FS) where {T<:Real}\n\nCompute the variational distributions (qs) and update the log-likelihoods for a set of Gaussian observation models within a Forward-Backward framework.\n\nArguments\n\nmodel::Vector{GaussianObservationModel{T}}   A vector of Gaussian observation models, where each model defines the parameters for a specific regime or state in a Switching Linear Dynamical System. Each GaussianObservationModel should contain fields such as the observation matrix C and the observation noise covariance R.\nFB   The Forward-Backward object that holds variables related to the forward and backward passes of the algorithm. It must contain a mutable field loglikelihoods, which is a matrix where each entry loglikelihoods[k, t] corresponds to the log-likelihood of the observation at time t under regime k.\ny   The observation data matrix, where each column represents an observation vector at a specific time step. The dimensions are typically (num_features, num_time_steps).\nFS   An array of FilterSmooth objects, one for each regime, that store smoothed state estimates (x_smooth) and their covariances (p_smooth). These are used to compute the expected sufficient statistics needed for updating the variational distributions.\n\nReturns\n\nNothing   The function performs in-place updates on the FB.loglikelihoods matrix. It does not return any value.\n\nDescription\n\nvariational_qs! updates the log-likelihoods for each Gaussian observation model across all time steps based on the current smoothed state estimates. This is a critical step in variational inference algorithms for Switching Linear Dynamical Systems, where the goal is to approximate the posterior distributions over latent variables.\n\nExample\n\n```julia\n\nDefine Gaussian observation models for each regime\n\nmodel = [     GaussianObservationModel(C = randn(5, 10), R = Matrix{Float64}(I, 5, 5)),     GaussianObservationModel(C = randn(5, 10), R = Matrix{Float64}(I, 5, 5)) ]\n\nInitialize ForwardBackward object with a preallocated loglikelihoods matrix\n\nFB = ForwardBackward(loglikelihoods = zeros(Float64, length(model), 100))\n\nGenerate synthetic observation data (5 features, 100 time steps)\n\ny = randn(5, 100)\n\nInitialize FilterSmooth objects for each regime\n\nFS = [     initialize_FilterSmooth(model[k], size(y, 2)) for k in 1:length(model) ]\n\nCompute variational distributions and update log-likelihoods\n\nvariational_qs!(model, FB, y, FS)\n\nAccess the updated log-likelihoods\n\nprintln(FB.loglikelihoods)\n\n\n\n\n\n","category":"method"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl is a comprehensive Julia package for state space modeling, designed specifically with neuroscientific applications in mind. The package provides efficient implementations of various state space models along with tools for parameter estimation, state inference, and model selection.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install StateSpaceDynamics.jl, start up Julia and type the following code-snipped into the REPL. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"StateSpaceDynamics\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"or alternatively, you can enter the package manager by typing ] and then run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add StateSpaceDynamics","category":"page"},{"location":"#What-are-State-Space-Models?","page":"Home","title":"What are State Space Models?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"State space models are a class of probabilistic models that describe the evolution of a system through two main components - a latent and observation process. The latent process is a stochastic process that is not directly observed, but is used to generate the observed data. The observation process is a conditional distribution that describes how the observed data is generated from the latent process.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In their most general form, state space models can be written as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim p(x_t+1  x_t) \n    y_t sim p(y_t  x_t)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x_t is the latent state at time t and y_t is the observed data at time t.","category":"page"},{"location":"#Example:-Linear-Dynamical-Systems","page":"Home","title":"Example: Linear Dynamical Systems","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A fundamental example is the Linear Dynamical System (LDS), which combines linear dynamics with Gaussian noise. The LDS can be expressed in two equivalent forms:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Equation form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 = A x_t + b + epsilon_t \n    y_t = C x_t + d + delta_t\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfA is the state transition matrix\nmathbfC is the observation matrix  \nmathbfb and mathbfd are bias terms\nboldsymbolepsilon_t and boldsymboldelta_t are Gaussian noise terms with covariances mathbfQ and mathbfR respectively","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distributional form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim mathcalN(A x_t + b Q) \n    y_t sim mathcalN(C x_t + d R)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where mathbfQ and mathbfR are the state and observation noise covariance matrices, respectively.","category":"page"},{"location":"#Models-Implemented","page":"Home","title":"Models Implemented","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl implements several types of state space models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Linear Dynamical Systems (LDS)\nGaussian LDS\nPoisson LDS\nHidden Markov Models (HMM)\nGaussian emissions\nRegression-based emissions\nGaussian regression\nBernoulli regression\nPoisson regression\nAutoregressive emissions","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a simple example using a Linear Dynamical System:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StateSpaceDynamics\n\n# Create a Gaussian LDS\nlds = GaussianLDS(\n    latent_dim=3,    # 3D latent state\n    obs_dim=10       # 10D observations\n)\n\n# Generate synthetic data\nx, y = sample(lds, 1000)  # 1000 timepoints\n\n# Fit the model\nfit!(lds, y)\n\n# Get smoothed state estimates\nx_smoothed = smooth(lds, y)","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you encounter a bug or would like to contribute to the package, please open an issue on our GitHub repository. Once the suggested change has received positive feedback feel free to submit a PR adhering to the blue style guide.","category":"page"},{"location":"#Citing-StateSpaceDynamics.jl","page":"Home","title":"Citing StateSpaceDynamics.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use StateSpaceDynamics.jl in your research, please cite the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"[Citation information to be added upon publication]","category":"page"},{"location":"EmissionModels/#Emission-Models","page":"EmissionModels","title":"Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The StateSpaceDynamics.jl package provides several emission models for state space modeling. These models define how observations are generated from latent states.","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.AutoRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.AutoRegressionEmission","text":"AutoRegressionEmission <: EmissionModel\n\nA mutable struct representing an autoregressive emission model, which wraps around an AutoRegression model.\n\nFields\n\ninner_model::AutoRegression: The underlying autoregressive model used for the emissions.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.BernoulliRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.BernoulliRegressionEmission","text":"BernoulliRegressionEmission\n\nA Bernoulli regression model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\ninclude_intercept::Bool = true: Whether to include an intercept term.\nβ::Vector{<:Real} = if include_intercept zeros(input_dim + 1) else zeros(input_dim) end: Coefficients of the model. The first element is the intercept term, if included.\nλ::Float64 = 0.0: Regularization parameter.\n\n```\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianEmission","text":"mutable struct GaussianEmission <: EmissionModel\n\nGaussianEmission model with mean and covariance.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianEmission-Tuple{}","page":"EmissionModels","title":"StateSpaceDynamics.GaussianEmission","text":"function GaussianEmission(; output_dim::Int, μ::Vector{<:Real}=zeros(output_dim), Σ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim))\n\nFuncton to create a GaussianEmission with given output dimension, mean, and covariance.\n\nArguments\n\noutput_dim::Int: The output dimension of the emission\nμ::Vector{<:Real}=zeros(output_dim): The mean of the Gaussian\nΣ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim)): The covariance matrix of the Gaussian\n\nReturns\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianRegressionEmission","text":"GaussianRegressionEmission\n\nA Gaussian regression Emission model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\noutput_dim::Int: Dimension of the output data.\ninclude_intercept::Bool = true: Whether to include an intercept term; if true, the first column of β is assumed to be the intercept/bias.\nβ::Matrix{<:Real} = if include_intercept zeros(input_dim + 1, output_dim) else zeros(input_dim, output_dim) end: Coefficient matrix of the model. Shape inputdim by outputdim. The first row are the intercept terms, if included.\nΣ::Matrix{<:Real} = Matrix{Float64}(I, output_dim, output_dim): Covariance matrix of the model.\nλ::Float64 = 0.0: Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.PoissonRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.PoissonRegressionEmission","text":"PoissonRegressionEmission\n\nA Poisson regression model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\ninclude_intercept::Bool = true: Whether to include an intercept term.\nβ::Vector{<:Real} = if include_intercept zeros(input_dim + 1) else zeros(input_dim) end: Coefficients of the model. The first element is the intercept term, if included.\nλ::Float64 = 0.0: Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.RegressionOptimization","page":"EmissionModels","title":"StateSpaceDynamics.RegressionOptimization","text":"RegressionOptimization{T<:RegressionEmission}\n\nHolds the optimization problem data for regression emissions.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{AbstractRNG, AutoRegressionEmission, Matrix{<:Real}}","page":"EmissionModels","title":"Base.rand","text":"sample(model::AutoRegressionEmission, Y_prev::Matrix{<:Real}; observation_sequence::Matrix{<:Real}=Matrix{Float64}(undef, 0, model.output_dim))\n\nGenerate a sample from the given autoregressive emission model using the previous observations Y_prev, and append it to the provided observation sequence.\n\nArguments\n\nmodel::AutoRegressionEmission: The autoregressive emission model to sample from.\nY_prev::Matrix{<:Real}: The matrix of previous observations, where each row represents an observation.\nobservation_sequence::Matrix{<:Real}: The sequence of observations to which the new sample will be appended (defaults to an empty matrix with appropriate dimensions).\n\nReturns\n\nMatrix{Float64}: The updated observation sequence with the new sample appended.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Base.rand-Tuple{AbstractRNG, BernoulliRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"sample(model::BernoulliRegressionEmission, Φ::AbstractMatrix{<:Real}; n::Int=size(Φ, 1))\n\nGenerate n samples from a Bernoulli regression model. Returns a matrix of size (n, 1).\n\nArguments\n\nmodel::BernoulliRegressionEmission: Bernoulli regression model.\nΦ::AbstractMatrix{<:Real}: Design matrix of shape (n, input_dim).\nn::Int=size(Φ, 1): Number of samples to generate.\n\nReturns\n\nY::AbstractMatrix{<:Real}: Matrix of samples of shape (n, 1).\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Base.rand-Tuple{AbstractRNG, GaussianEmission}","page":"EmissionModels","title":"Base.rand","text":"sample(model::Gaussian; n::Int=1)\n\nGenerate n samples from a Gaussian model. Returns a matrix of size (n, output_dim).\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Base.rand-Tuple{AbstractRNG, GaussianRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"sample(model::GaussianRegressionEmission, Φ::AbstractMatrix{<:Real}; n::Int=size(Φ, 1))\n\nGenerate n samples from a Gaussian regression model. Returns a matrix of size (n, output_dim).\n\nArguments\n\nmodel::GaussianRegressionEmission: Gaussian regression model.\nΦ::AbstractMatrix{<:Real}: Design matrix of shape (n, input_dim).\nn::Int=size(Φ, 1): Number of samples to generate.\n\nReturns\n\nY::AbstractMatrix{<:Real}: Matrix of samples of shape (n, output_dim).\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Base.rand-Tuple{AbstractRNG, PoissonRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"sample(model::PoissonRegressionEmission, Φ::AbstractMatrix{<:Real}; n::Int=size(Φ, 1))\n\nGenerate n samples from a Poisson regression model. Returns a matrix of size (n, 1).\n\nArguments\n\nmodel::PoissonRegressionEmission: Poisson regression model.\nΦ::AbstractMatrix{<:Real}: Design matrix of shape (n, input_dim).\nn::Int=size(Φ, 1): Number of samples to generate.\n\nReturns\n\nY::AbstractMatrix{<:Real}: Matrix of samples of shape (n, 1).\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.SwitchingAutoRegression-Tuple{}","page":"EmissionModels","title":"StateSpaceDynamics.SwitchingAutoRegression","text":"SwitchingAutoRegression(; K::Int, output_dim::Int, order::Int, include_intercept::Bool=true, β::Matrix{<:Real}=if include_intercept zeros(output_dim * order + 1, output_dim) else zeros(output_dim * order, output_dim) end, Σ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim), λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching AutoRegression Model\n\nArguments\n\nK::Int: The number of hidden states.\noutput_dim::Int: The dimensionality of the output data.\norder::Int: The order of the autoregressive model.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model.\nβ::Matrix{<:Real}: The autoregressive coefficients (defaults to zeros).\nΣ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim): The covariance matrix for the autoregressive model (defaults to an identity matrix).\nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (Defaults to a random initialization). \nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (Defaults to a random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching AutoRegression Model\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.calc_regularization","page":"EmissionModels","title":"StateSpaceDynamics.calc_regularization","text":"calc_regularization(β::Matrix{<:Real}, λ::Float64, include_intercept::Bool)\n\nCalculate L2 regularization term for regression coefficients.\n\nArguments\n\nβ::Matrix{<:Real}: Coefficient matrix\nλ::Float64: Regularization parameter\ninclude_intercept::Bool: Whether to exclude the intercept term from regularization\n\nReturns\n\nFloat64: The regularization term value\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.calc_regularization_gradient","page":"EmissionModels","title":"StateSpaceDynamics.calc_regularization_gradient","text":"calc_regularization_gradient(β::Matrix{<:Real}, λ::Float64, include_intercept::Bool)\n\nCalculate gradient of L2 regularization term for regression coefficients.\n\nArguments\n\nβ::Matrix{<:Real}: Coefficient matrix\nλ::Float64: Regularization parameter\ninclude_intercept::Bool: Whether to exclude the intercept term from regularization\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.construct_AR_feature_matrix","page":"EmissionModels","title":"StateSpaceDynamics.construct_AR_feature_matrix","text":"construct_AR_feature_matrix(data::Matrix{Float64}, order::Int) -> Matrix{Float64}\n\nConstruct an autoregressive (AR) feature matrix from input time series data.\n\nArguments\n\ndata::Matrix{Float64}: A matrix of size (num_feats, T), where num_feats is the number of features, and T is the number of timepoints.\norder::Int: The autoregressive order, determining how many past timepoints are included for each time step.\n\nReturns\n\nMatrix{Float64}: A transformed feature matrix of size (num_feats * (order + 1), T - order), where each column contains stacked feature vectors from the current and past order timepoints.\n\nExample\n\n```julia data = rand(3, 10)  # 3 features, 10 timepoints order = 2 ARfeats = constructARfeaturematrix(data, order) size(AR_feats)  # (3 * (2 + 1), 10 - 2) => (9, 8)\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.construct_AR_feature_matrix-2","page":"EmissionModels","title":"StateSpaceDynamics.construct_AR_feature_matrix","text":"construct_AR_feature_matrix(data::Vector{Matrix{Float64}}, order::Int) -> Vector{Matrix{Float64}}\n\nConstructs autoregressive (AR) feature matrices for multiple trials of time series data. Each trial is represented as a matrix, and the function applies the same AR transformation to each trial independently.\n\nArguments\n\ndata::Vector{Matrix{Float64}}: A vector of matrices, where each matrix represents a trial of time series data with dimensions (num_feats, T), where num_feats is the number of features and T is the number of timepoints.\norder::Int: The autoregressive order, determining how many past timepoints are included for each time step.\n\nReturns\n\nVector{Matrix{Float64}}: A vector of transformed feature matrices, where each matrix has dimensions (num_feats * (order + 1), T - order), containing stacked feature vectors from the current and past order timepoints.\n\nExample\n\n```julia data = [rand(3, 10) for _ in 1:5]  # 5 trials, each with 3 features and 10 timepoints order = 2 ARfeatstrials = constructARfeaturematrix(data, order) size(ARfeats_trials[1])  # (9, 8), same transformation applied per trial\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.fit!","page":"EmissionModels","title":"StateSpaceDynamics.fit!","text":"fit!(model::GaussianEmission, Y::AbstractMatrix{<:Real}, w::AbstractVector{Float64}=ones(size(Y, 1)))\n\nFit a GaussianEmission model to the data Y. \n\nArguments\n\nmodel::GaussianEmission: Gaussian model to fit.\nY::AbstractMatrix{<:Real}: Data to fit the model to. Should be a matrix of size (n, output_dim).\nw::AbstractVector{Float64}=ones(size(Y, 1)): Weights for the data. Should be a vector of size n.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::AutoRegressionEmission, Y_prev::Matrix{<:Real}, Y::AbstractMatrix{<:Real})\n\nCalculate the log likelihood of the data Y given the autoregressive emission model and the previous observations Y_prev.\n\nArguments\n\nmodel::AutoRegressionEmission: The autoregressive emission model for which to calculate the log likelihood.\nY_prev::Matrix{<:Real}: The matrix of previous observations, where each row represents an observation.\nY::AbstractMatrix{<:Real}: The data matrix, where each row represents an observation.\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-2","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::BernoulliRegressionEmission, Φ::AbstractMatrix{<:Real}, Y::AbstractMatrix{<:Real}, w::AbstractVector{Float64}=ones(size(Y, 1)))\n\nCalculate the log likelihood of the data Y given the Bernoulli regression emission model and the input features Φ. Optionally, a vector of weights w can be provided.\n\nArguments\n\nmodel::BernoulliRegressionEmission: The Bernoulli regression emission model for which to calculate the log likelihood.\nΦ::AbstractMatrix{<:Real}: The input features matrix (Observations x Features).\nY::AbstractMatrix{<:Real}: The data matrix (Observations x Features).\nw::AbstractVector{Float64}: A vector of weights corresponding to each observation (defaults to a vector of ones).\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-3","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianRegressionEmission, Φ::AbstractMatrix{<:Real}, Y::AbstractMatrix{<:Real})\n\nCalculate the log likelihood of the data Y given the Gaussian regression emission model and the input features Φ.\n\nArguments\n\nmodel::GaussianRegressionEmission: The Gaussian regression emission model for which to calculate the log likelihood.\nΦ::AbstractMatrix{<:Real}: The input features matrix (Observations x Features).\nY::AbstractMatrix{<:Real}: The data matrix (Observations x Features).\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-4","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::PoissonRegressionEmission, Φ::AbstractMatrix{<:Real}, Y::AbstractMatrix{<:Real}, w::AbstractVector{Float64}=ones(size(Y, 1)))\n\nCalculate the log-likelihood of a Poisson regression model.\n\nArguments\n\nmodel::PoissonRegressionEmission: Poisson regression model.\nΦ::AbstractMatrix{<:Real}: Design matrix of shape (n, input_dim).\nY::AbstractMatrix{<:Real}: Response matrix of shape (n, 1).\nw::AbstractVector{Float64}: Weights of the data points. Should be a vector of size n.\n\nReturns\n\nloglikelihood::Float64: Log-likelihood of the model.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Tuple{GaussianEmission, AbstractMatrix{<:Real}}","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianEmission, Y::AbstractMatrix{<:Real})\n\nCalculate the log likelihood of the data Y given the Gaussian emission model.\n\nArguments\n\nmodel::GaussianEmission: The Gaussian emission model for which to calculate the log likelihood.\nY::AbstractMatrix{<:Real}: The data matrix, where each row represents an observation.\n\nReturns\n\nVector{Float64}: A vector of log likelihoods, one for each observation in the data.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#What-is-a-Linear-Dynamical-System?","page":"Linear Dynamical Systems","title":"What is a Linear Dynamical System?","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A Linear Dynamical System (LDS) is a mathematical model used to describe how a system evolves over time. These systems are a subset of state-space models, where the hidden state dynamics are continuous. What makes these models linear is that the latent dynamics evolve according to a linear function of the previous state. The observations, however, can be related to the hidden state through a nonlinear link function.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"At its core, an LDS defines:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A state transition function: how the internal state evolves from one time step to the next.\nAn observation function: how the internal state generates the observed data.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.LinearDynamicalSystem","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.LinearDynamicalSystem","text":"LinearDynamicalSystem{S<:AbstractStateModel, O<:AbstractObservationModel}\n\nRepresents a unified Linear Dynamical System with customizable state and observation models.\n\nFields\n\nstate_model::S: The state model (e.g., GaussianStateModel)\nobs_model::O: The observation model (e.g., GaussianObservationModel or PoissonObservationModel)\nlatent_dim::Int: Dimension of the latent state\nobs_dim::Int: Dimension of the observations\nfit_bool::Vector{Bool}: Vector indicating which parameters to fit during optimization\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#The-Gaussian-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Gaussian Linear Dynamical System — typically just referred to as an LDS — is a specific type of linear dynamical system where both the state transition and observation functions are linear, and all noise is Gaussian.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The generative model is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t sim mathcalN(A x_t-1 Q) \n    y_t sim mathcalN(C x_t R)\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x_t is the hidden state at time t\ny_t is the observed data at time t  \nA is the state transition matrix\nC is the observation matrix\nQ is the process noise covariance\nR is the observation noise covariance","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This can equivalently be written in equation form:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t = A x_t-1 + epsilon_t \n    y_t = C x_t + eta_t\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"ε_t ~ N(0, Q) is the process noise\nη_t ~ N(0, R) is the observation noise","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianStateModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianStateModel","text":"GaussianStateModel{T<:Real} <: AbstractStateModel\n\nRepresents the state model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nA::Matrix{T}: Transition matrix\nQ::Matrix{T}: Process noise covariance\nx0::Vector{T}: Initial state\nP0::Matrix{T}: Initial state covariance\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianObservationModel","text":"GaussianObservationModel{T<:Real} <: AbstractObservationModel\n\nRepresents the observation model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nC::Matrix{T}: Observation matrix\nR::Matrix{T}: Observation noise covariance\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianLDS","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianLDS","text":"GaussianLDS(; A, C, Q, R, x0, P0, fit_bool, obs_dim, latent_dim)\n\nConstruct a Linear Dynamical System with Gaussian state and observation models.\n\nArguments\n\nA::Matrix{T}=Matrix{T}(undef, 0, 0): Transition matrix\nC::Matrix{T}=Matrix{T}(undef, 0, 0): Observation matrix\nQ::Matrix{T}=Matrix{T}(undef, 0, 0): Process noise covariance\nR::Matrix{T}=Matrix{T}(undef, 0, 0): Observation noise covariance\nx0::Vector{T}=Vector{T}(undef, 0): Initial state\nP0::Matrix{T}=Matrix{T}(undef, 0, 0): Initial state covariance\nfit_bool::Vector{Bool}=fill(true, 6): Vector indicating which parameters to fit during optimization\nobs_dim::Int: Dimension of the observations (required if C or R is not provided.)\nlatent_dim::Int: Dimension of the latent state (required if A, Q, x0, P0, or C is not provided.)\n\n\n\n\n\n","category":"function"},{"location":"LinearDynamicalSystems/#The-Poisson-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Poisson Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Poisson Linear Dynamical System is a variant of the LDS where the observations are modeled as counts. This is useful in fields like neuroscience where we are often interested in modeling spike count data. To relate the spiking data to the Gaussian latent variable, we use a nonlinear link function, specifically the exponential function. ","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The generative model is given by: ","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t sim mathcalN(A x_t-1 Q) \n    y_t sim textPoisson(exp(Cx_t + b))\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where b is a bias term.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.PoissonObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.PoissonObservationModel","text":"PoissonObservationModel{T<:Real} <: AbstractObservationModel\n\nRepresents the observation model of a Linear Dynamical System with Poisson observations.\n\nFields\n\nC::Matrix{T}: Observation matrix\nlog_d::Vector{T}: Mean firing rate vector (log space)\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.PoissonLDS","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.PoissonLDS","text":"PoissonLDS(; A, C, Q, log_d, x0, P0, refractory_period, fit_bool, obs_dim, latent_dim)\n\nConstruct a Linear Dynamical System with Gaussian state and Poisson observation models.\n\nArguments\n\nA::Matrix{T}=Matrix{T}(undef, 0, 0): Transition matrix\nC::Matrix{T}=Matrix{T}(undef, 0, 0): Observation matrix\nQ::Matrix{T}=Matrix{T}(undef, 0, 0): Process noise covariance\nlog_d::Vector{T}=Vector{T}(undef, 0): Mean firing rate vector (log space)\nx0::Vector{T}=Vector{T}(undef, 0): Initial state\nP0::Matrix{T}=Matrix{T}(undef, 0, 0): Initial state covariance\nrefractory_period::Int=1: Refractory period\nfit_bool::Vector{Bool}=fill(true, 7): Vector indicating which parameters to fit during optimization\nobs_dim::Int: Dimension of the observations (required if C, D, or log_d is not provided.)\nlatent_dim::Int: Dimension of the latent state (required if A, Q, x0, P0, or C is not provided.)\n\n\n\n\n\n","category":"function"},{"location":"LinearDynamicalSystems/#Sampling-from-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Sampling from Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"You can generate synthetic data from fitted LDS models:","category":"page"},{"location":"LinearDynamicalSystems/#Base.rand-Tuple{LinearDynamicalSystem}","page":"Linear Dynamical Systems","title":"Base.rand","text":"Random.rand(lds::LinearDynamicalSystem; tsteps::Int, ntrials::Int)\n\nSample latent states and observations from a linear dynamical system using the default random number generator.\n\nThis is a convenience wrapper around Random.rand(rng, lds; tsteps, ntrials).\n\nArguments\n\nlds::LinearDynamicalSystem: The system to simulate.\ntsteps::Int: Number of time steps to simulate.\nntrials::Int: Number of independent trials.\n\nReturns\n\n(x, y): A tuple containing:\nx::Array{T,3}: Simulated latent states, of shape (latent_dim, tsteps, ntrials)\ny::Array{T,3}: Simulated observations, of shape (obs_dim, tsteps, ntrials)\n\nExamples\n\n```julia lds = GaussianLDS(obsdim=3, latentdim=2) x, y = rand(lds; tsteps=100, ntrials=10)\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#Inference-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Inference in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In StateSpaceDynamics.jl, we directly maximize the complete-data log-likelihood function with respect to the latent states given the data and the parameters of the model. In other words, the maximum a priori (MAP) estimate of the latent state path is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"undersetxtextargmax  left log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t) right","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This MAP estimation approach has the same computational complexity as traditional Kalman filtering and smoothing — mathcalO(T) — but is significantly more flexible. Notably, it can handle nonlinear observations and non-Gaussian noise while still yielding exact MAP estimates, unlike approximate techniques such as the Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF).","category":"page"},{"location":"LinearDynamicalSystems/#Newton's-Method-for-Latent-State-Optimization","page":"Linear Dynamical Systems","title":"Newton's Method for Latent State Optimization","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"To find the MAP trajectory, we iteratively optimize the latent states using Newton's method. The update equation at each iteration is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x^(i+1) = x^(i) - left nabla^2 mathcalL(x^(i)) right^-1 nabla mathcalL(x^(i))","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x) is the complete-data log-likelihood:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x) = log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t)","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"nabla mathcalL(x) is the gradient of the full log-likelihood with respect to all latent states\nnabla^2 mathcalL(x) is the Hessian of the full log-likelihood","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This update is performed over the entire latent state sequence x_1T, and repeated until convergence.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"For Gaussian models, mathcalL(x) is quadratic and Newton's method converges in a single step — recovering the exact Kalman smoother solution. For non-Gaussian models, the Hessian is not constant and the optimization is more complex. However, the MAP estimate can still be computed efficiently using the same approach as the optimization problem is still convex.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.smooth","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.smooth","text":"smooth(lds::LinearDynamicalSystem{S,O}, y::Matrix{T}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nThis function performs direct smoothing for a linear dynamical system (LDS) given the system parameters and the observed data.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The LDS object representing the system parameters.\ny::Matrix{T}: The observed data matrix.\nw::Vector{Float64}: coeffcients to weight the data.\n\nReturns\n\nx::Matrix{T}: The optimal state estimate.\np_smooth::Array{T, 3}: The posterior covariance matrix.\ninverse_offdiag::Array{T, 3}: The inverse off-diagonal matrix.\nQ_val::T: The Q-function value.\n\nExample\n\nlds = GaussianLDS(obs_dim=4, latent_dim=3)\ny = randn(100, 4)  # 100 time steps, 4 observed dimensions\nx, p_smooth, inverse_offdiag, Q_val = DirectSmoother(lds, y)\n\n\n\n\n\nsmooth(lds::LinearDynamicalSystem{S,O}, y::Array{T,3}) where {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nThis function performs direct smoothing for a linear dynamical system (LDS) given the system parameters and the observed data for multiple trials.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The LDS object representing the system parameters.\ny::Array{T,3}: The observed data array with dimensions (obsdim, tiemsteps, ntrials).\n\nReturns\n\nx::Array{T,3}: The optimal state estimates with dimensions (ntrials, timesteps, latentdim).\np_smooth::Array{T,4}: The posterior covariance matrices with dimensions (latentdim, latentdim, time_steps, ntrials).\ninverse_offdiag::Array{T,4}: The inverse off-diagonal matrices with dimensions (latentdim, latentdim, time_steps, ntrials).\n\nExample\n\nlds = GaussianLDS(obs_dim=4, latent_dim=3)\ny = randn(5, 100, 4)  # 5 trials, 100 time steps, 4 observed dimension\nx, p_smooth, inverse_offdiag = smooth(lds, y)\n\n\n\n\n\n","category":"function"},{"location":"LinearDynamicalSystems/#Laplace-Approximation-of-Posterior-for-Non-Conjugate-Observation-Models","page":"Linear Dynamical Systems","title":"Laplace Approximation of Posterior for Non-Conjugate Observation Models","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In the case of non-Gaussian observations, we can use a Laplace approximation to compute the posterior distribution of the latent states. For Gaussian observations (which are conjugate with the Gaussian state model), the posterior is also Gaussian and is the exact posterior. However, for non-Gaussian observations, we can approximate the posterior using a Gaussian distribution centered at the MAP estimate of the latent states. This approximation is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"p(x mid y) approx mathcalN(x^* -left nabla^2 mathcalL(x^*) right^-1)","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x^* is the MAP estimate of the latent states\nnabla^2 mathcalL(x^*) is the Hessian of the log-likelihood at the MAP estimate","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Despite the requirement of inverting a Hessian of dimension (d times T) times (d times T), this is still computationally efficient, as the Markov structure of the model renders the Hessian block-tridiagonal, and thus the inversion is tractable.","category":"page"},{"location":"LinearDynamicalSystems/#Learning-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Learning in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Given the latent structure of state-space models, we must rely on either the Expectation-Maximization (EM) or Variational Inference (VI) approaches to learn the parameters of the model. StateSpaceDynamics.jl supports both EM and VI. For LDS models, we can use Laplace EM, where we approximate the posterior of the latent state path using the Laplace approximation as outlined above. Using these approximate posteriors (or exact ones in the Gaussian case), we can apply closed-form updates for the model parameters.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.fit!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{LinearDynamicalSystem{S, O}, Array{T, 3}}} where {T<:Real, S<:GaussianStateModel{T}, O<:AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(lds::LinearDynamicalSystem{S,O}, y::Array{T,3}; \n     max_iter::Int=1000, \n     tol::Real=1e-12\n) where {T<:Real, S<:GaussianStateModel{T}, O<:AbstractObservationModel{T}}\n\nFit a Linear Dynamical System to a collection of trials using the Expectation-Maximization (EM) algorithm with Kalman smoothing.\n\nArguments\n\nlds::LinearDynamicalSystem{S,O}: The Linear Dynamical System to be fitted.\ny::Array{T,3}: Observed data of shape (obs_dim, tsteps, n_trials).\n\nKeyword Arguments\n\nmax_iter: Maximum number of EM iterations (default: 1000).\ntol: Convergence tolerance for the change in log-likelihood (default: 1e-12).\n\nReturns\n\nmls::Vector{T}: Log-likelihood at each iteration of the EM algorithm.\n\n\n\n\n\n","category":"method"}]
}
