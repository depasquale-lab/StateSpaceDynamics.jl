var documenterSearchIndex = {"docs":
[{"location":"tutorials/poisson_mixture_model_example/#Simulating-and-Fitting-a-Poisson-Mixture-Model","page":"Poisson Mixture Model Example","title":"Simulating and Fitting a Poisson Mixture Model","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create a Poisson Mixture Model and fit it using the EM algorithm.","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing StableRNGs\nusing StatsPlots\nusing Distributions\n\nrng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Create-a-true-PoissonMixtureModel-to-simulate-from","page":"Poisson Mixture Model Example","title":"Create a true PoissonMixtureModel to simulate from","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"k = 3\ntrue_λs = [5.0, 10.0, 25.0]  # Poisson means\ntrue_πs = [0.25, 0.45, 0.3]   # Mixing weights\n\ntrue_pmm = PoissonMixtureModel(k, true_λs, true_πs)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Generate-data-from-the-true-model","page":"Poisson Mixture Model Example","title":"Generate data from the true model","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"n = 500\nlabels = rand(rng, Categorical(true_πs), n)\ndata = [rand(rng, Poisson(true_λs[labels[i]])) for i in 1:n]  # Vector{Int}","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Plot-histogram-of-Poisson-samples-by-component","page":"Poisson Mixture Model Example","title":"Plot histogram of Poisson samples by component","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"p1 = histogram(\n    data;\n    group=labels,\n    bins=0:1:maximum(data),\n    bar_position=:dodge,\n    xlabel=\"Count\",\n    ylabel=\"Frequency\",\n    title=\"Poisson Mixture Samples by Component\",\n    alpha=0.7,\n    legend=:topright,\n)\np1","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Fit-a-new-PoissonMixtureModel-to-the-data","page":"Poisson Mixture Model Example","title":"Fit a new PoissonMixtureModel to the data","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"fit_pmm = PoissonMixtureModel(k)\n_, lls = fit!(fit_pmm, data; maxiter=100, tol=1e-6, initialize_kmeans=true)","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Plot-log-likelihoods-to-visualize-EM-convergence","page":"Poisson Mixture Model Example","title":"Plot log-likelihoods to visualize EM convergence","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"p2 = plot(\n    lls;\n    xlabel=\"Iteration\",\n    ylabel=\"Log-Likelihood\",\n    title=\"EM Convergence (Poisson Mixture)\",\n    marker=:circle,\n    label=\"log_likelihood\",\n)\np2","category":"page"},{"location":"tutorials/poisson_mixture_model_example/#Plot-model-PMFs-over-the-data-histogram","page":"Poisson Mixture Model Example","title":"Plot model PMFs over the data histogram","text":"","category":"section"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"p3 = histogram(\n    data;\n    bins=0:1:maximum(data),\n    normalize=true,\n    alpha=0.3,\n    label=\"Data\",\n    xlabel=\"Count\",\n    ylabel=\"Density\",\n    title=\"Poisson Mixtures: Data and PMFs\",\n)\n\nx = collect(0:maximum(data))\ncolors = [:red, :green, :blue]\n\nfor i in 1:k\n    λi = fit_pmm.λₖ[i]\n    πi = fit_pmm.πₖ[i]\n    pmf_i = πi .* pdf.(Poisson(λi), x)\n    plot!(\n        p3, x, pmf_i;\n        lw=2,\n        c=colors[i],\n        label=\"Comp $i (λ=$(round(λi, sigdigits=3)))\",\n    )\nend\n\nmix_pmf = reduce(+, (πi .* pdf.(Poisson(λi), x) for (λi, πi) in zip(fit_pmm.λₖ, fit_pmm.πₖ)))\nplot!(\n    p3, x, mix_pmf;\n    lw=3, ls=:dash, c=:black,\n    label=\"Mixture\",\n)\n\np3","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"","category":"page"},{"location":"tutorials/poisson_mixture_model_example/","page":"Poisson Mixture Model Example","title":"Poisson Mixture Model Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"Misc/#Misc","page":"Miscellaneous","title":"Misc","text":"","category":"section"},{"location":"Misc/#StateSpaceDynamics.AbstractHMM","page":"Miscellaneous","title":"StateSpaceDynamics.AbstractHMM","text":"Abstract type for HMMs \n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.AutoRegressiveEmission","page":"Miscellaneous","title":"StateSpaceDynamics.AutoRegressiveEmission","text":"Special case of regression emission models that are autoregressive.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.FilterSmooth","page":"Miscellaneous","title":"StateSpaceDynamics.FilterSmooth","text":"\"     FilterSmooth{T<:Real}\n\nA mutable structure for storing smoothed estimates and associated covariance matrices in a filtering or smoothing algorithm.\n\nType Parameters\n\nT<:Real: The numerical type used for all fields (e.g., Float64, Float32).\n\nFields\n\nx_smooth::Matrix{T}   The matrix containing smoothed state estimates over time. Each column typically represents the state vector at a given time step.\np_smooth::Array{T, 3}   The posterior covariance matrices with dimensions (latentdim, latentdim, time_steps)\nE_z::Array{T, 3}   The expected latent states, size (statedim, T, ntrials).\nE_zz::Array{T, 4}   The expected value of zt * zt', size (statedim, statedim, T, n_trials).\nE_zz_prev::Array{T, 4}   The expected value of zt * z{t-1}', size (statedim, statedim, T, n_trials).\n\nExample\n\n```julia\n\nInitialize a FilterSmooth object with Float64 type\n\nfilter = FilterSmooth{Float64}(     xsmooth = zeros(10, 100),     psmooth = zeros(10, 10, 100),     Ez = zeros(10, 5, 100),     Ezz = zeros(10, 10, 5, 100),     Ezzprev = zeros(10, 10, 5, 100) )\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.ForwardBackward","page":"Miscellaneous","title":"StateSpaceDynamics.ForwardBackward","text":"ForwardBackward{T<:Real}\n\nA mutable struct that encapsulates the forward–backward algorithm outputs for a hidden Markov model (HMM).\n\nFields\n\nloglikelihoods::Matrix{T}: Matrix of log-likelihoods for each observation and state.\nα::Matrix{T}: The forward probabilities (α) for each time step and state.\nβ::Matrix{T}: The backward probabilities (β) for each time step and state.\nγ::Matrix{T}: The state occupancy probabilities (γ) for each time step and state.\nξ::Array{T,3}: The pairwise state occupancy probabilities (ξ) for consecutive time steps and state pairs.\n\nTypically, α and β are computed by the forward–backward algorithm to find the likelihood of an observation sequence. γ and ξ are derived from these calculations to estimate how states transition over time.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.MixtureModel","page":"Miscellaneous","title":"StateSpaceDynamics.MixtureModel","text":"Abstract type for Mixture Models. I.e. GMM's, etc.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.RegressionModel","page":"Miscellaneous","title":"StateSpaceDynamics.RegressionModel","text":"Abstract type for Regression Models. I.e. GaussianRegression, BernoulliRegression, etc.\n\n\n\n\n\n","category":"type"},{"location":"Misc/#StateSpaceDynamics.GaussianHMM-Union{Tuple{}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.GaussianHMM","text":"GaussianHMM(; K::Int, output_dim::Int, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Hidden Markov Model with Gaussian Emissions\n\nArguments\n\nK::Int: The number of hidden states\noutput_dim::Int: The dimensionality of the observation\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization)\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization)\n\nReturns\n\n::HiddenMarkovModel: Hidden Markov Model Object with Gaussian Emissions\n\n```\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingAutoRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingAutoRegression","text":"SwitchingAutoRegression(; K::Int, output_dim::Int, order::Int, include_intercept::Bool=true, β::Matrix{<:Real}=if include_intercept zeros(output_dim * order + 1, output_dim) else zeros(output_dim * order, output_dim) end, Σ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim), λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching AutoRegression Model\n\nArguments\n\nK::Int: The number of hidden states.\noutput_dim::Int: The dimensionality of the output data.\norder::Int: The order of the autoregressive model.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model.\nβ::Matrix{<:Real}: The autoregressive coefficients (defaults to zeros).\nΣ::Matrix{<:Real}=Matrix{Float64}(I, output_dim, output_dim): The covariance matrix for the autoregressive model (defaults to an identity matrix).\nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (Defaults to a random initialization). \nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (Defaults to a random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching AutoRegression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingBernoulliRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingBernoulliRegression","text":"SwitchingBernoulliRegression(; K::Int, input_dim::Int, include_intercept::Bool=true, β::Vector{<:Real}=if include_intercept zeros(input_dim + 1) else zeros(input_dim) end, λ::Float64=0.0, A::Matrix{<:Real}=initialize_transition_matrix(K), πₖ::Vector{Float64}=initialize_state_distribution(K))\n\nCreate a Switching Bernoulli Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input data.\ninclude_intercept::Bool=true: Whether to include an intercept in the regression model (defaults to true).\nβ::Vector{<:Real}: The regression coefficients (defaults to zeros). \nλ::Float64=0.0: Regularization parameter for the regression (defaults to zero).\nA::Matrix{<:Real}=initialize_transition_matrix(K): The transition matrix of the HMM (defaults to random initialization).\nπₖ::Vector{Float64}=initialize_state_distribution(K): The initial state distribution of the HMM (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Bernoulli Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.SwitchingGaussianRegression-Tuple{}","page":"Miscellaneous","title":"StateSpaceDynamics.SwitchingGaussianRegression","text":"SwitchingGaussianRegression(; \n    K::Int,\n    input_dim::Int,\n    output_dim::Int,\n    include_intercept::Bool = true,\n    β::Matrix{<:Real} = if include_intercept\n        zeros(input_dim + 1, output_dim)\n    else\n        zeros(input_dim, output_dim)\n    end,\n    Σ::Matrix{<:Real} = Matrix{Float64}(I, output_dim, output_dim),\n    λ::Float64 = 0.0,\n    A::Matrix{<:Real} = initialize_transition_matrix(K),\n    πₖ::Vector{Float64} = initialize_state_distribution(K)\n)\n\nCreate a Switching Gaussian Regression Model\n\nArguments\n\nK::Int: The number of hidden states.\ninput_dim::Int: The dimensionality of the input features.\noutput_dim::Int: The dimensionality of the output predictions.\ninclude_intercept::Bool: Whether to include an intercept in the regression model (default is true).\nβ::Matrix{<:Real}: The regression coefficients (defaults to zeros based on input_dim and output_dim).\nΣ::Matrix{<:Real}: The covariance matrix of the Gaussian emissions (defaults to an identity matrix).\nλ::Float64: The regularization parameter for the regression (default is 0.0).\nA::Matrix{<:Real}: The transition matrix of the Hidden Markov Model (defaults to random initialization).\nπₖ::Vector{Float64}: The initial state distribution of the Hidden Markov Model (defaults to random initialization).\n\nReturns\n\n::HiddenMarkovModel: A Switching Gaussian Regression Model\n\n\n\n\n\n","category":"method"},{"location":"Misc/#Base.getproperty-Tuple{StateSpaceDynamics.AutoRegressiveEmission, Symbol}","page":"Miscellaneous","title":"Base.getproperty","text":"getproperty(model::AutoRegressiveEmission, sym::Symbol)\n\nGet various properties of 'innerGaussianRegression`. \n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridgm-Union{Tuple{T}, Tuple{Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridgm","text":"block_tridgm(main_diag::Vector{Matrix{T}}, upper_diag::Vector{Matrix{T}}, lower_diag::Vector{Matrix{T}}) where {T<:Real}\n\nConstruct a block tridiagonal matrix from three vectors of matrices.\n\nThrows\n\nErrorException if the lengths of upper_diag and lower_diag are not one less than the length of main_diag.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse-Union{Tuple{T}, Tuple{Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse","text":"block_tridiagonal_inverse(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix.\n\nNotes: This implementation is from the paper:\n\n\"An Accelerated Lambda Iteration Method for Multilevel Radiative Transfer” Rybicki, G.B., and Hummer, D.G., Astronomy and Astrophysics, 245, 171–181 (1991), Appendix B.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.block_tridiagonal_inverse_static-Union{Tuple{T}, Tuple{Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}, Vector{<:AbstractMatrix{T}}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.block_tridiagonal_inverse_static","text":"block_tridiagonal_inverse_static(A, B, C)\n\nCompute the inverse of a block tridiagonal matrix using static matrices. See block_tridiagonal_inverse for details.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.check_same_type-Tuple","page":"Miscellaneous","title":"StateSpaceDynamics.check_same_type","text":"check_same_type(args...)\n\nUtility function to check if n arguments share the same types. \n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.euclidean_distance-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.euclidean_distance","text":"euclidean_distance(a::AbstractVector{Float64}, b::AbstractVector{Float64})\n\nCalculate the Euclidean distance between two points.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.gaussian_entropy-Tuple{LinearAlgebra.Symmetric{BigFloat, <:SparseArrays.AbstractSparseMatrix}}","page":"Miscellaneous","title":"StateSpaceDynamics.gaussian_entropy","text":"gaussian_entropy(H::Symmetric{BigFloat, <:SparseMatrix})\n\nSpecialized method for BigFloat sparse matrices using logdet.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.gaussian_entropy-Union{Tuple{LinearAlgebra.Symmetric{T, S} where S<:(AbstractMatrix{<:T})}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.gaussian_entropy","text":"gaussian_entropy(H::Symmetric{T}) where {T<:Real}\n\nCalculate the entropy of a Gaussian distribution with Hessian (i.e. negative precision) matrix H.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}, Tuple{AbstractMatrix{T}, Int64, Int64}, Tuple{AbstractMatrix{T}, Int64, Int64, Float64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::AbstractMatrix{T}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6) where {T<:Real}\n\nPerform K-means clustering on column-major data.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeans_clustering-Union{Tuple{T}, Tuple{AbstractVector{T}, Int64}, Tuple{AbstractVector{T}, Int64, Int64}, Tuple{AbstractVector{T}, Int64, Int64, Float64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeans_clustering","text":"kmeans_clustering(data::AbstractVector{T}, k_means::Int, max_iters::Int=100, tol::Float64=1e-6)\n\nPerform K-means clustering on vector data.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Union{Tuple{T}, Tuple{AbstractMatrix{T}, Int64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::AbstractMatrix{T}, k_means::Int) where {T<:Real}\n\nPerform K-means++ initialization for cluster centroids (column-major input).\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.kmeanspp_initialization-Union{Tuple{T}, Tuple{AbstractVector{T}, Int64}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.kmeanspp_initialization","text":"kmeanspp_initialization(data::AbstractVector{T}, k_means::Int)\n\nK-means++ initialization for vector data.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.logistic-Tuple{Real}","page":"Miscellaneous","title":"StateSpaceDynamics.logistic","text":"logistic(x::Real)\n\nCalculate the logistic function in a numerically stable way.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.make_posdef!-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.make_posdef!","text":"make_posdef!(A::AbstractMatrix{T}) where {T<:Real}\n\nEnsure that a matrix is positive definite by adjusting its eigenvalues.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.random_rotation_matrix","page":"Miscellaneous","title":"StateSpaceDynamics.random_rotation_matrix","text":"random_rotation_matrix(n)\n\nGenerate a random rotation matrix of size n x n.\n\n\n\n\n\n","category":"function"},{"location":"Misc/#StateSpaceDynamics.row_matrix-Tuple{AbstractVector}","page":"Miscellaneous","title":"StateSpaceDynamics.row_matrix","text":"row_matrix(x::AbstractVector)\n\nConvert a vector to a row matrix.\n\n\n\n\n\n","category":"method"},{"location":"Misc/#StateSpaceDynamics.stabilize_covariance_matrix-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"Miscellaneous","title":"StateSpaceDynamics.stabilize_covariance_matrix","text":"stabilize_covariance_matrix(Σ::Matrix{<:Real})\n\nStabilize a covariance matrix by ensuring it is symmetric and positive definite.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#What-are-Emission-Models?","page":"EmissionModels","title":"What are Emission Models?","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Emission models describe how observations are generated from latent states in a state space model. These models define the conditional distribution of the observed data given the hidden state or input features. In StateSpaceDynamics.jl, a flexible suite of emission models is supported, including both simple parametric distributions and regression-based models.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"At a high level, emission models encode:","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The distribution of observations (e.g., Gaussian, Poisson, Bernoulli)\nHow observations relate to inputs or latent states, either directly or via regression","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.EmissionModel","page":"EmissionModels","title":"StateSpaceDynamics.EmissionModel","text":"Base type hierarchy for emission models. Each emission model must implement:\n\nsample()\nloglikelihood()\nfit!()\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.RegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.RegressionEmission","text":"Base type hierarchy for regression emission models.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Gaussian-Emission-Model","page":"EmissionModels","title":"Gaussian Emission Model","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The GaussianEmission is a basic model where the observations are drawn from a multivariate normal distribution with a fixed mean and covariance.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim mathcalN(mu Sigma)","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"This emission model is often used when the observed data is real-valued and homoscedastic.","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianEmission","text":"mutable struct GaussianEmission <: EmissionModel\n\nGaussianEmission model with mean and covariance.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Union{Tuple{T}, Tuple{GaussianEmission, AbstractMatrix{T}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianEmission, Y::AbstractMatrix{T}) where {T<:Real}\n\nCalculate the log likelihood of the data Y given the Gaussian emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Base.rand-Tuple{GaussianEmission}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::GaussianEmission; kwargs...)\nRandom.rand(rng::AbstractRNG, model::GaussianEmission; n::Int=1)\n\nGenerate random samples from  a Gaussian emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{GaussianEmission, AbstractMatrix{T}}, Tuple{GaussianEmission, AbstractMatrix{T}, AbstractVector{T}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.fit!","text":"function fit!(model::GaussianEmission, \n        Y::AbstractMatrix{T}, \n        w::AbstractVector{T}=ones(size(Y, 1))) where {T<:Real}\n\nFit a GaussianEmission model to the data Y weighted by weights w.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Regression-Based-Emission-Models","page":"EmissionModels","title":"Regression-Based Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Regression-based emissions allow the output to depend on an input matrix Phi. The regression relationship is defined by a coefficient matrix beta, optionally with an intercept and regularization.","category":"page"},{"location":"EmissionModels/#Gaussian-Regression-Emission","page":"EmissionModels","title":"Gaussian Regression Emission","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"In the GaussianRegressionEmission, the outputs are real-valued and modeled via linear regression with additive Gaussian noise.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim mathcalN(Phi_t beta Sigma)","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.GaussianRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.GaussianRegressionEmission","text":"GaussianRegressionEmission\n\nStore a Gaussian regression Emission model.\n\nFields\n\ninput_dim::Int: Dimension of the input data.\noutput_dim::Int: Dimension of the output data.\ninclude_intercept::Bool: Whether to include an intercept term; if true, the first column of β is assumed to be the intercept/bias.\nβ::AbstractMatrix{<:Real} = if include_intercept zeros(input_dim + 1, output_dim) else zeros(input_dim, output_dim) end: Coefficient matrix of the model. Shape inputdim by outputdim. The first row are the intercept terms, if included.\nΣ::AbstractMatrix{<:Real}: Covariance matrix of the model.\nλ:<Real: Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{GaussianRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::GaussianRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\nRandom.rand(rng::AbstractRNG, model::GaussianRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\n\nGenerate samples from a Gaussian regression model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Union{Tuple{T}, Tuple{GaussianRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}}, Tuple{GaussianRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}, Union{Nothing, AbstractVector{T}}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(model::GaussianRegressionEmission,\n    Φ::AbstractMatrix{T},\n    Y::AbstractMatrix{T},\n    w::AbstractVector{T}=ones(size(Y, 1))) where {T<:Real}\n\nCalculate the log likelihood of the data Y given the Gaussian regression emission model and the input features Φ.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Bernoulli-Regression-Emission","page":"EmissionModels","title":"Bernoulli Regression Emission","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The BernoulliRegressionEmission is appropriate for binary data. The probability of success is modeled via a logistic function.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"p(y_t = 1 mid Phi_t) = sigma(Phi_t beta)","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Where sigma(z) = 1  (1 + e^-z) is the logistic function.","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.BernoulliRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.BernoulliRegressionEmission","text":"BernoulliRegressionEmission\n\nStore a Bernoulli regression model.\n\nFields\n\ninput_dim::Int: Dimensionality of the input data.\noutput_dim::Int: Dimensionality of the outputd data.\ninclude_intercept::Bool: Whether to include an intercept term.\nβ::AbstractMatrix{<:Real}: Bernoulli regression coefficients.\nλ<:Real: L2 Regularization parameter.\n\n```\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{BernoulliRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::BernoulliRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\nRandom.rand(rng::AbstractRNG, model::BernoulliRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\n\nGenerate samples from a Bernoulli regression emission.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"function loglikelihood(\n    model::BernoulliRegressionEmission,\n    Φ::AbstractMatrix{T1},\n    Y::AbstractMatrix{T2},\n    w::AbstractVector{T3}=ones(size(Y, 1))) where {T1<:Real, T2<:Real, T3<:Real}\n\nCalculate the log likelihood of the data Y given the Bernoulli regression emission model and the input features Φ. Optionally, a vector of weights w can be provided.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#Poisson-Regression-Emission","page":"EmissionModels","title":"Poisson Regression Emission","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The PoissonRegressionEmission is ideal for count data, such as spike counts in neuroscience. It models the intensity of the Poisson distribution as an exponential function of the linear predictors.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim textPoisson(lambda_t) quad lambda_t = exp(Phi_t beta)","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.PoissonRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.PoissonRegressionEmission","text":"PoissonRegressionEmission\n\nA Poisson regression model.\n\nFields\n\ninput_dim::Int: Dimensionality of the input data.\noutput_dim::Int: Dimensionality of the output data.\ninclude_intercept::Bool: Whether to include a regression intercept.\nβ::AbstractMatrix{<:Real}: The regression coefficients matrix.\nλ::Real;: L2 Regularization parameter.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{PoissonRegressionEmission, Union{Matrix{<:Real}, Vector{<:Real}}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::PoissonRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\nRandom.rand(rng::AbstractRNG, model::PoissonRegressionEmission, Φ::Union{Matrix{<:Real},Vector{<:Real}})\n\nGenerate samples from a Poisson regression emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-2","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(\n    model::PoissonRegressionEmission,\n    Φ::AbstractMatrix{T1},\n    Y::AbstractMatrix{T2},\n    w::AbstractVector{T3}=ones(size(Y, 1))) where {T1<:Real, T2<:Real, T3<:Real}\n\nCalculate the log-likelihood of a Poisson regression model.\n\n\n\n\n\n","category":"function"},{"location":"EmissionModels/#Autoregressive-Emission-Models","page":"EmissionModels","title":"Autoregressive Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"The AutoRegressionEmission models the observation at time t as depending on previous observations (i.e., an autoregressive structure), using a wrapped GaussianRegressionEmission.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"y_t sim mathcalN(sum_i=1^p A_i y_t-i Sigma)","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"Where p is the autoregressive order and A_i are regression weights.","category":"page"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"This model is useful when modeling temporal dependencies in the emission process, independent of latent dynamics.","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.AutoRegressionEmission","page":"EmissionModels","title":"StateSpaceDynamics.AutoRegressionEmission","text":"AutoRegressionEmission <: EmissionModel\n\nStore an autoregressive emission model, which wraps around a GaussianRegressionEmission.\n\nFields\n\noutput_dim::Int: The dimensionality of the output data\norder::Int: The order of the Autoregressive process\ninnerGaussianRegression::GaussianRegressionEmission: The underlying Gaussian regression model used for the emissions.\n\n\n\n\n\n","category":"type"},{"location":"EmissionModels/#Base.rand-Tuple{AutoRegressionEmission, Matrix{<:Real}}","page":"EmissionModels","title":"Base.rand","text":"Random.rand(model::AutoRegressionEmission, X::Matrix{<:Real})\nRandom.rand(rng::AbstractRNG, model::AutoRegressionEmission, X::Matrix{<:Real})\n\nGenerate samples from an autoregressive emission model.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#StateSpaceDynamics.loglikelihood-Union{Tuple{T}, Tuple{AutoRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}}, Tuple{AutoRegressionEmission, AbstractMatrix{T}, AbstractMatrix{T}, Union{Nothing, AbstractVector{T}}}} where T<:Real","page":"EmissionModels","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(\n    model::AutoRegressionEmission,\n    X::AbstractMatrix{T},\n    Y::AbstractMatrix{T},\n    w::Vector{T}=ones(size(Y, 1))) where {T<:Real}\n\nCalculate the log likelihood of the data Y given the autoregressive emission model and the previous observations X.\n\n\n\n\n\n","category":"method"},{"location":"EmissionModels/#Fitting-Regression-Emission-Models","page":"EmissionModels","title":"Fitting Regression Emission Models","text":"","category":"section"},{"location":"EmissionModels/","page":"EmissionModels","title":"EmissionModels","text":"All regression-based emissions can be fitted using maximum likelihood with optional weights and L2 regularization. Internally, StateSpaceDynamics.jl formulates this as an optimization problem, solved using gradient-based methods (e.g., LBFGS).","category":"page"},{"location":"EmissionModels/#StateSpaceDynamics.fit!-Union{Tuple{T3}, Tuple{T2}, Tuple{T1}, Tuple{RegressionEmission, AbstractMatrix{T1}, AbstractMatrix{T2}}, Tuple{RegressionEmission, AbstractMatrix{T1}, AbstractMatrix{T2}, AbstractVector{T3}}} where {T1<:Real, T2<:Real, T3<:Real}","page":"EmissionModels","title":"StateSpaceDynamics.fit!","text":"fit!(\n    model::RegressionEmission,\n    X::AbstractMatrix{T1},\n    y::AbstractMatrix{T2},\n    w::AbstractVector{T3}=ones(size(y, 1))) where {T1<:Real, T2<:Real, T3<:Real}\n\nFit a regression emission model give input data X, output data y, and weights w.\n\nArguments\n\n- `model::RegressionEmission`: A regression emission model.\n- `X::AbstractMatrix{<:Real}:`: Input data.\n- `y::AbstractMatrix{<:Real}`: Output data.\n- `w::AbstractVector{<:Real}`: Weights to define each point's contribution to the fit.\n\nReturns\n\n- `model::RegressionEmission`: The regression model with the newly updated parameters.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#What-is-a-Linear-Dynamical-System?","page":"Linear Dynamical Systems","title":"What is a Linear Dynamical System?","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A Linear Dynamical System (LDS) is a mathematical model used to describe how a system evolves over time. These systems are a subset of state-space models, where the hidden state dynamics are continuous. What makes these models linear is that the latent dynamics evolve according to a linear function of the previous state. The observations, however, can be related to the hidden state through a nonlinear link function.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"At its core, an LDS defines:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"A state transition function: how the internal state evolves from one time step to the next.\nAn observation function: how the internal state generates the observed data.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.LinearDynamicalSystem","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.LinearDynamicalSystem","text":"LinearDynamicalSystem{T<:Real, S<:AbstractStateModel{T}, O<:AbstractObservationModel{T}}\n\nRepresents a unified Linear Dynamical System with customizable state and observation models.\n\nFields\n\nstate_model::S: The state model (e.g., GaussianStateModel)\nobs_model::O: The observation model (e.g., GaussianObservationModel or PoissonObservationModel)\nlatent_dim::Int: Dimension of the latent state\nobs_dim::Int: Dimension of the observations\nfit_bool::Vector{Bool}: Vector indicating which parameters to fit during optimization\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#The-Gaussian-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Gaussian Linear Dynamical System — typically just referred to as an LDS — is a specific type of linear dynamical system where both the state transition and observation functions are linear, and all noise is Gaussian.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The generative model is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t sim mathcalN(A x_t-1 Q) \n    y_t sim mathcalN(C x_t R)\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x_t is the hidden state at time t\ny_t is the observed data at time t  \nA is the state transition matrix\nC is the observation matrix\nQ is the process noise covariance\nR is the observation noise covariance","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This can equivalently be written in equation form:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t = A x_t-1 + epsilon_t \n    y_t = C x_t + eta_t\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"ε_t ~ N(0, Q) is the process noise\nη_t ~ N(0, R) is the observation noise","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianStateModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianStateModel","text":"GaussianStateModel{T<:Real. M<:AbstractMatrix{T}, V<:AbstractVector{T}}}\n\nRepresents the state model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nA::M: Transition matrix (size latent_dim×latent_dim). \nQ::M: Process noise covariance matrix \nx0::V: Initial state vector (length latent_dim).\nP0::M: Initial state covariance matrix (size `latentdim×latentdim\n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.GaussianObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.GaussianObservationModel","text":"GaussianObservationModel{T<:Real, M<:AbstractMatrix{T}}\n\nRepresents the observation model of a Linear Dynamical System with Gaussian noise.\n\nFields\n\nC::M: Observation matrix of size (obs_dim × latent_dim). Maps latent states into observation space. \nR::M: Observation noise covariance of size (obs_dim × obs_dim). \n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#The-Poisson-Linear-Dynamical-System","page":"Linear Dynamical Systems","title":"The Poisson Linear Dynamical System","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The Poisson Linear Dynamical System is a variant of the LDS where the observations are modeled as counts. This is useful in fields like neuroscience where we are often interested in modeling spike count data. To relate the spiking data to the Gaussian latent variable, we use a nonlinear link function, specifically the exponential function. ","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"The generative model is given by: ","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"beginaligned\n    x_t sim mathcalN(A x_t-1 Q) \n    y_t sim textPoisson(exp(Cx_t + b))\nendaligned","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where b is a bias term.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.PoissonObservationModel","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.PoissonObservationModel","text":"PoissonObservationModel{T<:Real, M<:AbstractMatrix{T}, V<:AbstractVector{T}} <: AbstractObservationModel{T}\n\nRepresents the observation model of a Linear Dynamical System with Poisson observations.\n\nFields\n\nC::AbstractMatrix{T}: Observation matrix of size (obs_dim × latent_dim). Maps latent states into observation space.\nlog_d::AbstractVector{T}: Mean firing rate vector (log space) of size (obs_dim × obs_dim). \n\n\n\n\n\n","category":"type"},{"location":"LinearDynamicalSystems/#Sampling-from-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Sampling from Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"You can generate synthetic data from fitted LDS models:","category":"page"},{"location":"LinearDynamicalSystems/#Base.rand-Tuple{LinearDynamicalSystem}","page":"Linear Dynamical Systems","title":"Base.rand","text":"Random.rand(lds::LinearDynamicalSystem; tsteps::Int, ntrials::Int)\nRandom.rand(rng::AbstractRNG, lds::LinearDynamicalSystem; tsteps::Int, ntrials::Int)\n\nSample from a Linear Dynamical System.\n\n\n\n\n\n","category":"method"},{"location":"LinearDynamicalSystems/#Inference-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Inference in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In StateSpaceDynamics.jl, we directly maximize the complete-data log-likelihood function with respect to the latent states given the data and the parameters of the model. In other words, the maximum a priori (MAP) estimate of the latent state path is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"undersetxtextargmax  left log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t) right","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This MAP estimation approach has the same computational complexity as traditional Kalman filtering and smoothing — mathcalO(T) — but is significantly more flexible. Notably, it can handle nonlinear observations and non-Gaussian noise while still yielding exact MAP estimates, unlike approximate techniques such as the Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF).","category":"page"},{"location":"LinearDynamicalSystems/#Newton's-Method-for-Latent-State-Optimization","page":"Linear Dynamical Systems","title":"Newton's Method for Latent State Optimization","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"To find the MAP trajectory, we iteratively optimize the latent states using Newton's method. The update equation at each iteration is:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x^(i+1) = x^(i) - left nabla^2 mathcalL(x^(i)) right^-1 nabla mathcalL(x^(i))","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x) is the complete-data log-likelihood:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"mathcalL(x) = log p(x_0) + sum_t=2^T log p(x_t mid x_t-1) + sum_t=1^T log p(y_t mid x_t)","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"nabla mathcalL(x) is the gradient of the full log-likelihood with respect to all latent states\nnabla^2 mathcalL(x) is the Hessian of the full log-likelihood","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"This update is performed over the entire latent state sequence x_1T, and repeated until convergence.","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"For Gaussian models, mathcalL(x) is quadratic and Newton's method converges in a single step — recovering the exact Kalman smoother solution. For non-Gaussian models, the Hessian is not constant and the optimization is more complex. However, the MAP estimate can still be computed efficiently using the same approach as the optimization problem is still convex.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.smooth","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.smooth","text":"smooth(lds, y)\n\nThis function performs direct smoothing for a linear dynamical system (LDS) given the system parameters and the observed data for a single trial\n\nArguments\n\nlds::LinearDynamicalSystem{T,S,O}: The LDS object representing the system parameters.\ny::AbstractMatrix{T}: The observed data matrix.\nw::Union{Nothing,AbstractVector{T}}: coeffcients to weight the data.\n\nReturns\n\nx::AbstractMatrix{T}: The optimal state estimate.\np_smooth::Array{T, 3}: The posterior covariance matrix.\ninverse_offdiag::Array{T, 3}: The inverse off-diagonal matrix.\nQ_val::T: The Q-function value.\n\n\n\n\n\nsmooth(lds, y)\n\nThis function performs direct smoothing for a linear dynamical system (LDS) given the system parameters and the observed data for multiple trials.\n\nArguments\n\nlds::LinearDynamicalSystem{T,S,O}: The LDS object representing the system parameters.\ny::AbstractArray{T,3}: The observed data array with dimensions (obs_dim, tsteps, ntrials).\n\nReturns\n\nx::AbstractArray{T,3}: The optimal state estimates with dimensions (ntrials, tsteps, latent_dim).\np_smooth::AbstractArray{T,4}: The posterior covariance matrices with dimensions (latentdim, latentdim, tsteps, ntrials).\ninverse_offdiag::AbstractArray{T,4}: The inverse off-diagonal matrices with dimensions (latentdim, latentdim, tsteps, ntrials).\n\n\n\n\n\n","category":"function"},{"location":"LinearDynamicalSystems/#Laplace-Approximation-of-Posterior-for-Non-Conjugate-Observation-Models","page":"Linear Dynamical Systems","title":"Laplace Approximation of Posterior for Non-Conjugate Observation Models","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"In the case of non-Gaussian observations, we can use a Laplace approximation to compute the posterior distribution of the latent states. For Gaussian observations (which are conjugate with the Gaussian state model), the posterior is also Gaussian and is the exact posterior. However, for non-Gaussian observations, we can approximate the posterior using a Gaussian distribution centered at the MAP estimate of the latent states. This approximation is given by:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"p(x mid y) approx mathcalN(x^* -left nabla^2 mathcalL(x^*) right^-1)","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Where:","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"x^* is the MAP estimate of the latent states\nnabla^2 mathcalL(x^*) is the Hessian of the log-likelihood at the MAP estimate","category":"page"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Despite the requirement of inverting a Hessian of dimension (d times T) times (d times T), this is still computationally efficient, as the Markov structure of the model renders the Hessian block-tridiagonal, and thus the inversion is tractable.","category":"page"},{"location":"LinearDynamicalSystems/#Learning-in-Linear-Dynamical-Systems","page":"Linear Dynamical Systems","title":"Learning in Linear Dynamical Systems","text":"","category":"section"},{"location":"LinearDynamicalSystems/","page":"Linear Dynamical Systems","title":"Linear Dynamical Systems","text":"Given the latent structure of state-space models, we must rely on either the Expectation-Maximization (EM) or Variational Inference (VI) approaches to learn the parameters of the model. StateSpaceDynamics.jl supports both EM and VI. For LDS models, we can use Laplace EM, where we approximate the posterior of the latent state path using the Laplace approximation as outlined above. Using these approximate posteriors (or exact ones in the Gaussian case), we can apply closed-form updates for the model parameters.","category":"page"},{"location":"LinearDynamicalSystems/#StateSpaceDynamics.fit!-Union{Tuple{O}, Tuple{S}, Tuple{T}, Tuple{LinearDynamicalSystem{T, S, O}, AbstractArray{T, 3}}} where {T<:Real, S<:(GaussianStateModel{T, M, V} where {M<:AbstractMatrix{T}, V<:AbstractVector{T}}), O<:AbstractObservationModel{T}}","page":"Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(lds, y; max_iter::Int=1000, tol::Real=1e-12) \nwhere {T<:Real, S<:GaussianStateModel{T}, O<:GaussianObservationModel{T}}\n\nFit a Linear Dynamical System using the Expectation-Maximization (EM) algorithm with Kalman smoothing over multiple trials\n\nArguments\n\nlds::LinearDynamicalSystem{T,S,O}: The Linear Dynamical System to be fitted.\ny::AbstractArray{T,3}: Observed data, size(obsdim, Tsteps, n_trials)\n\nKeyword Arguments\n\nmax_iter::Int=1000: Maximum number of EM iterations.\ntol::T=1e-12: Convergence tolerance for log-likelihood change.\n\nReturns\n\nmls::Vector{T}: Vector of log-likelihood values for each iteration.\nparam_diff::Vector{T}: Vector of parameter deltas for each EM iteration. \n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#Mixture-Models","page":"Mixture Models","title":"Mixture Models","text":"","category":"section"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"A mixture model is a probability distribution which, given a finite k  0, samples from k different distributions f_i(x)  i in 1k randomly, where the probability of sampling from f_i(x) is pi_i. Generally, a mixture model is written in the form of:","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"f_mix(x Theta pi) = sum_k=1^K pi_k f_k(x)","category":"page"},{"location":"MixtureModels/","page":"Mixture Models","title":"Mixture Models","text":"Where f_i(x) is called the ith component and pi_i is called the ith mixing coeffiecent.","category":"page"},{"location":"MixtureModels/#Gaussian-Mixture-Model","page":"Mixture Models","title":"Gaussian Mixture Model","text":"","category":"section"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel\n\nA Gaussian Mixture Model for clustering and density estimation.\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.GaussianMixtureModel-Tuple{Int64, Int64}","page":"Mixture Models","title":"StateSpaceDynamics.GaussianMixtureModel","text":"GaussianMixtureModel(k::Int, data_dim::Int)\n\nConstructor for GaussianMixtureModel. Initializes Σₖ's covariance matrices to the  identity, πₖ to a uniform distribution, and μₖ's means to zeros.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(gmm::GaussianMixtureModel, data::Matrix{<:Real}; <keyword arguments>)\n\nFits a Gaussian Mixture Model (GMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\ngmm::GaussianMixtureModel: The Gaussian Mixture Model to be fitted.\ndata::Matrix{<:Real}: The dataset on which the model will be fitted, where each row represents a data point.\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the GMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.loglikelihood-Tuple{GaussianMixtureModel, Matrix{<:Real}}","page":"Mixture Models","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(gmm::GaussianMixtureModel, data::Matrix{<:Real})\n\nCompute the log-likelihood of the data given the Gaussian Mixture Model (GMM). The data matrix should be of shape (# observations, # features).\n\nArguments\n\ngmm::GaussianMixtureModel: The Gaussian Mixture Model instance \ndata::Matrix{<:Real}: data matrix to calculate the Log-Likelihood \n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#Poisson-Mixture-Model","page":"Mixture Models","title":"Poisson Mixture Model","text":"","category":"section"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel\n\nA Poisson Mixture Model for clustering and density estimation.\n\nFields\n\nk::Int: Number of poisson-distributed clusters.\nλₖ::Vector{Float64}: Means of each cluster.\nπₖ::Vector{Float64}: Mixing coefficients for each cluster.\n\n\n\n\n\n","category":"type"},{"location":"MixtureModels/#StateSpaceDynamics.PoissonMixtureModel-Tuple{Int64}","page":"Mixture Models","title":"StateSpaceDynamics.PoissonMixtureModel","text":"PoissonMixtureModel(k::Int)\n\nConstructor for PoissonMixtureModel. Initializes λₖ's means to  ones and πₖ to a uniform distribution.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.fit!-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.fit!","text":"fit!(pmm::PoissonMixtureModel, data::Matrix{Int}; maxiter::Int=50, tol::Float64=1e-3, initialize_kmeans::Bool=false)\n\nFits a Poisson Mixture Model (PMM) to the given data using the Expectation-Maximization (EM) algorithm.\n\nArguments\n\npmm::PoissonMixtureModel: The Poisson Mixture Model to be fitted.\ndata::Matrix{Int}: The dataset on which the model will be fitted, where each row represents a data point.\n\nKeyword Arguments\n\nmaxiter::Int=50: The maximum number of iterations for the EM algorithm (default: 50).\ntol::Float64=1e-3: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).\ninitialize_kmeans::Bool=false: If true, initializes the means of the PMM using K-means++ initialization (default: false).\n\nReturns\n\nclass_probabilities: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.\n\n\n\n\n\n","category":"method"},{"location":"MixtureModels/#StateSpaceDynamics.loglikelihood-Tuple{PoissonMixtureModel, Matrix{Int64}}","page":"Mixture Models","title":"StateSpaceDynamics.loglikelihood","text":"loglikelihood(pmm::PoissonMixtureModel, data::Matrix{Int})\n\nCompute the log-likelihood of the data given the Poisson Mixture Model (PMM). The data matrix should be of shape (# features, # obs).\n\nReturns\n\nFloat64: The log-likelihood of the data given the model.\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#Swiching-Linear-Dynamical-Systems","page":"Switching Linear Dynamical Systems","title":"Swiching Linear Dynamical Systems","text":"","category":"section"},{"location":"SLDS/#StateSpaceDynamics.SwitchingLinearDynamicalSystem","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.SwitchingLinearDynamicalSystem","text":"Switching Linear Dynamical System\n\nStruct to Encode a Hidden Markov model that switches among K distinct LinearDyanmicalSystems\n\nFields\n\nA::V: Transition matrix for mode switching. \nB::VL: Vector of Linear Dynamical System models. \nπₖ::V: Initial state distribution.\nK::Int: Number of modes. \n\n\n\n\n\n","category":"type"},{"location":"SLDS/#Base.rand-Tuple{AbstractRNG, SwitchingLinearDynamicalSystem, Int64}","page":"Switching Linear Dynamical Systems","title":"Base.rand","text":"Random.rand(rng, slds, T)\n\nGenerate synthetic data with switching LDS models\n\n#Arguments \n\nrng:AbstractRNG: Random number generator\nslds::SwitchingLinearDynamicalSystem: The switching LDS model \nT::Int: Number of time steps to sample\n\nReturns\n\nTuple{Array,Array, Array}: Latent states (x), observations (y), and mode sequences (z). \n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{AbstractHMM, AbstractMatrix{T}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.fit!","text":"fit!(slds::SwitchingLinearDynamicalSystem, y::Matrix{T}; \n     max_iter::Int=1000, \n     tol::Real=1e-12, \n     ) where {T<:Real}\n\nFit a Switching Linear Dynamical System using the variational Expectation-Maximization (EM) algorithm with Kalman smoothing.\n\nArguments\n\nslds::SwitchingLinearDynamicalSystem: The Switching Linear Dynamical System to be fitted.\ny::Matrix{T}: Observed data, size (obsdim, Tsteps).\n\nKeyword Arguments\n\nmax_iter::Int=1000: Maximum number of EM iterations.\ntol::Real=1e-12: Convergence tolerance for log-likelihood change.\n\nReturns\n\nmls::Vector{T}: Vector of log-likelihood values for each iteration.\nparam_diff::Vector{T}: Vector of parameter differences over each iteration. \nFB::ForwardBackward: ForwardBackward struct \nFS::FilterSmooth: FilterSmooth struct \n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.hmm_elbo-Tuple{AbstractHMM, StateSpaceDynamics.ForwardBackward}","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.hmm_elbo","text":"hmm_elbo(model::AbstractHMM, FB::ForwardBackward; ϵ::Float64=1e-10)\n\nCompute the evidence based lower bound (ELBO) from the discrete state model. \n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.initialize_slds-Tuple{}","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.initialize_slds","text":"initialize_slds(;K::Int=2, d::Int=2, p::Int=10, self_bias::Float64=5.0, seed::Int=42)\n\nInitialize a Switching Linear Dynamical System with random parameters.\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.mstep!-Union{Tuple{T}, Tuple{AbstractHMM, Array{StateSpaceDynamics.FilterSmooth{T}, 1}, AbstractMatrix{T}, StateSpaceDynamics.ForwardBackward}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.mstep!","text":"mstep!(slds::AbstractHMM, FS::Vector{FilterSmooth{T}}, y::AbstractMatrix{T}, FB::ForwardBackward) where {T<:Real}\n\nFunction to carry out the M-step in Expectation-Maximization algorithm for SLDS \n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.variational_expectation!-Union{Tuple{T}, Tuple{SwitchingLinearDynamicalSystem, AbstractMatrix{T}, StateSpaceDynamics.ForwardBackward, Array{StateSpaceDynamics.FilterSmooth{T}, 1}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.variational_expectation!","text":"variational_expectation!(model::SwitchingLinearDynamicalSystem, y, FB, FS) -> Float64\n\nCompute the variational expectation (Evidence Lower Bound, ELBO) for a Switching Linear Dynamical System by executing the following operations: \n\nExtract Responsibilities:   Retrieves the responsibilities (γ) from the forward-backward object and computes their exponentials (hs).\nParallel Smoothing and Sufficient Statistics Calculation:   For each regime k from 1 to model.K, the function:\nPerforms smoothing using the smooth function to obtain smoothed states (x_smooth), covariances (p_smooth), inverse off-diagonal terms, and total entropy.\nComputes sufficient statistics (E_z, E_zz, E_zz_prev) from the smoothed estimates.\nCalculates the ELBO contribution for the current regime and accumulates it into ml_total.\nUpdate Variational Distributions:  \nComputes the variational distributions (qs) from the smoothed states, which are stored as log-likelihoods in FB.\nExecutes the forward and backward passes to update the responsibilities (γ) based on the new qs.\nRecalculates the responsibilities (γ) to reflect the updated variational distributions.\nReturn ELBO:   Returns the accumulated ELBO (ml_total), which quantifies the quality of the variational approximation.\n\n\n\n\n\n","category":"method"},{"location":"SLDS/#StateSpaceDynamics.variational_qs!-Union{Tuple{T}, Tuple{AbstractVector{<:GaussianObservationModel{T, <:AbstractMatrix{T}}}, StateSpaceDynamics.ForwardBackward, AbstractMatrix{T}, Array{StateSpaceDynamics.FilterSmooth{T}, 1}}} where T<:Real","page":"Switching Linear Dynamical Systems","title":"StateSpaceDynamics.variational_qs!","text":"variational_qs!(model::AbstractVector{<:GaussianObservationModel{T, <:AbstractMatrix{T}}},    FB::ForwardBackward, y::AbstractMatrix{T}, FS::Vector{FilterSmooth{T}}) where {T<:Real}\n\nCompute the variational distributions (qs) and update the log-likelihoods for a set of Gaussian observation models within a Forward-Backward framework.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/gaussian_latent_dynamics_example/#Simulating-and-Fitting-a-Linear-Dynamical-System","page":"Gaussian LDS Example","title":"Simulating and Fitting a Linear Dynamical System","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to simulate a latent linear dynamical system and fit it using the EM algorithm.","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Load-Packages","page":"Gaussian LDS Example","title":"Load Packages","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing LaTeXStrings\nusing StableRNGs","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"rng = StableRNG(123);\nnothing #hide","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Create-a-State-Space-Model","page":"Gaussian LDS Example","title":"Create a State-Space Model","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"obs_dim = 10\nlatent_dim = 2\n\nA = 0.95 * [cos(0.25) -sin(0.25); sin(0.25) cos(0.25)]\nQ = Matrix(0.1 * I(2))\n\nx0 = [0.0; 0.0]\nP0 = Matrix(0.1 * I(2))\n\nC = randn(rng, obs_dim, latent_dim)\nR = Matrix(0.5 * I(obs_dim))\n\ntrue_gaussian_sm = GaussianStateModel(;A=A, Q=Q, x0=x0, P0=P0)\ntrue_gaussian_om = GaussianObservationModel(;C=C, R=R)\ntrue_lds = LinearDynamicalSystem(;\n    state_model=true_gaussian_sm,\n    obs_model=true_gaussian_om,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)\n)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Simulate-Latent-and-Observed-Data","page":"Gaussian LDS Example","title":"Simulate Latent and Observed Data","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"tSteps = 500\nlatents, observations = rand(rng, true_lds; tsteps=tSteps, ntrials=1)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Plot-Vector-Field-of-Latent-Dynamics","page":"Gaussian LDS Example","title":"Plot Vector Field of Latent Dynamics","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"x = y = -3:0.5:3\nX = repeat(x', length(y), 1)\nY = repeat(y, 1, length(x))\n\nU = zeros(size(X))\nV = zeros(size(Y))\n\nfor i in 1:size(X, 1)\n    for j in 1:size(X, 2)\n        v = A * [X[i,j], Y[i,j]]\n        U[i,j] = v[1] - X[i,j]\n        V[i,j] = v[2] - Y[i,j]\n    end\nend\n\nmagnitude = @. sqrt(U^2 + V^2)\nU_norm = U ./ magnitude\nV_norm = V ./ magnitude\n\np = quiver(X, Y, quiver=(U_norm, V_norm), color=:blue, alpha=0.3,\n           linewidth=1, arrow=arrow(:closed, :head, 0.1, 0.1))\nplot!(latents[1, :, 1], latents[2, :, 1], xlabel=\"x₁\", ylabel=\"x₂\",\n      color=:black, linewidth=1.5, title=\"Latent Dynamics\", legend=false)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Plot-Latent-States-and-Observations","page":"Gaussian LDS Example","title":"Plot Latent States and Observations","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"states = latents[:, :, 1]\nemissions = observations[:, :, 1]\n\nplot(size=(800, 600), layout=@layout[a{0.3h}; b])\n\nlim_states = maximum(abs.(states))\nfor d in 1:latent_dim\n    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black,\n          linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, tSteps), title=\"Simulated Latent States\",\n      yformatter=y->\"\", tickfontsize=12)\n\nlim_emissions = maximum(abs.(emissions))\nfor n in 1:obs_dim\n    plot!(1:tSteps, emissions[n, :] .- lim_emissions * (n-1), color=:black,\n          linewidth=2, label=\"\", subplot=2)\nend\n\nplot!(subplot=2, yticks=(-lim_emissions .* (obs_dim-1:-1:0), [L\"y_{%$n}\" for n in 1:obs_dim]),\n      xlabel=\"time\", xlims=(0, tSteps), title=\"Simulated Emissions\",\n      yformatter=y->\"\", tickfontsize=12)\n\nplot!(link=:x, size=(800, 600), left_margin=10Plots.mm)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Initialize-a-Model-and-Perform-Smoothing","page":"Gaussian LDS Example","title":"Initialize a Model and Perform Smoothing","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"A_init = random_rotation_matrix(2, rng)\nQ_init = Matrix(0.1 * I(2))\nC_init = randn(rng, obs_dim, latent_dim)\nR_init = Matrix(0.5 * I(obs_dim))\nx0_init = zeros(latent_dim)\nP0_init = Matrix(0.1 * I(latent_dim))\n\ngaussian_sm_init = GaussianStateModel(;A=A_init, Q=Q_init, x0=x0_init, P0=P0_init)\ngaussian_om_init = GaussianObservationModel(;C=C_init, R=R_init)\n\nnaive_ssm = LinearDynamicalSystem(;\n    state_model=gaussian_sm_init,\n    obs_model=gaussian_om_init,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)\n)\n\nx_smooth, _, _ = StateSpaceDynamics.smooth(naive_ssm, observations)\n\nplot()\nfor d in 1:latent_dim\n    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:tSteps, x_smooth[d, :, 1] .+ lim_states * (d-1), color=:firebrick, linewidth=2, label=\"\", subplot=1)\nend\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, tSteps), yformatter=y->\"\", tickfontsize=12,\n      title=\"True vs. Predicted Latent States (Pre-EM)\")","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Fit-Model-Using-EM-Algorithm","page":"Gaussian LDS Example","title":"Fit Model Using EM Algorithm","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"elbo, _ = fit!(naive_ssm, observations; max_iter=100, tol=1e-6)\n\nx_smooth, _, _ = StateSpaceDynamics.smooth(naive_ssm, observations)\n\nplot()\nfor d in 1:latent_dim\n    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:tSteps, x_smooth[d, :, 1] .+ lim_states * (d-1), color=:firebrick, linewidth=2, label=\"\", subplot=1)\nend\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, tSteps), yformatter=y->\"\", tickfontsize=12,\n      title=\"True vs. Predicted Latent States (Post-EM)\")","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/#Confirm-the-model-converges","page":"Gaussian LDS Example","title":"Confirm the model converges","text":"","category":"section"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"plot(elbo, xlabel=\"iteration\", ylabel=\"ELBO\", title=\"ELBO (Marginal Loglikelihood)\", legend=false)","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"","category":"page"},{"location":"tutorials/gaussian_latent_dynamics_example/","page":"Gaussian LDS Example","title":"Gaussian LDS Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Simulating-and-Fitting-a-Hidden-Markov-Model","page":"Gaussian GLM-GMM Example","title":"Simulating and Fitting a Hidden Markov Model","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create, sample from, and fit Hidden Markov Models (HMMs).","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Load-Packages","page":"Gaussian GLM-GMM Example","title":"Load Packages","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"using LinearAlgebra\nusing Plots\nusing Random\nusing StateSpaceDynamics\nusing StableRNGs","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Create-a-Gaussian-generalized-linear-model-hidden-Markov-model-(GLM-HMM)","page":"Gaussian GLM-GMM Example","title":"Create a Gaussian generalized linear model-hidden Markov model (GLM-HMM)","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"emission_1 = GaussianRegressionEmission(input_dim=3, output_dim=1, include_intercept=true, β=reshape([3.0, 2.0, 2.0, 3.0], :, 1), Σ=[1.0;;], λ=0.0)\nemission_2 = GaussianRegressionEmission(input_dim=3, output_dim=1, include_intercept=true, β=reshape([-4.0, -2.0, 3.0, 2.0], :, 1), Σ=[1.0;;], λ=0.0)\n\nA = [0.99 0.01; 0.05 0.95]\nπₖ = [0.8; 0.2]\n\ntrue_model = HiddenMarkovModel(K=2, A=A, πₖ=πₖ, B=[emission_1, emission_2])","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Sample-from-the-GLM-HMM","page":"Gaussian GLM-GMM Example","title":"Sample from the GLM-HMM","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"n = 20000\nΦ = randn(rng, 3, n)\ntrue_labels, data = rand(rng, true_model, Φ, n=n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-the-sampled-dataset","page":"Gaussian GLM-GMM Example","title":"Visualize the sampled dataset","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"colors = [:dodgerblue, :crimson]\n\nscatter(Φ[1, :], vec(data);\n    color = colors[true_labels],\n    ms = 3,\n    label = \"\",\n    xlabel = \"Input Feature 1\",\n    ylabel = \"Output\",\n    title = \"GLM-HMM Sampled Data\"\n)\n\nxvals = range(minimum(Φ[1, :]), stop=maximum(Φ[1, :]), length=100)\n\nβ1 = emission_1.β[:, 1]\ny_pred_1 = β1[1] .+ β1[2] .* xvals\nplot!(xvals, y_pred_1;\n    color = :dodgerblue,\n    lw = 3,\n    label = \"State 1 regression\",\n    legend = :topright,\n)\n\nβ2 = emission_2.β[:, 1]\ny_pred_2 = β2[1] .+ β2[2] .* xvals\nplot!(xvals, y_pred_2;\n    color = :crimson,\n    lw = 3,\n    label = \"State 2 regression\",\n    legend = :topright,\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Initialize-and-fit-a-new-HMM-to-the-sampled-data","page":"Gaussian GLM-GMM Example","title":"Initialize and fit a new HMM to the sampled data","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"A = [0.8 0.2; 0.1 0.9]\nπₖ = [0.6; 0.4]\nemission_1 = GaussianRegressionEmission(input_dim=3, output_dim=1, include_intercept=true, β=reshape([2.0, -1.0, 1.0, 2.0], :, 1), Σ=[2.0;;], λ=0.0)\nemission_2 = GaussianRegressionEmission(input_dim=3, output_dim=1, include_intercept=true, β=reshape([-2.5, -1.0, 3.5, 3.0], :, 1), Σ=[0.5;;], λ=0.0)\n\ntest_model = HiddenMarkovModel(K=2, A=A, πₖ=πₖ, B=[emission_1, emission_2])\nlls = fit!(test_model, data, Φ)\n\nplot(lls)\ntitle!(\"Log-likelihood over EM Iterations\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-the-emission-model-predictions","page":"Gaussian GLM-GMM Example","title":"Visualize the emission model predictions","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"state_colors = [:dodgerblue, :crimson]\ntrue_colors = [:green, :orange]\npred_colors = [:teal, :yellow]\n\nscatter(Φ[1, :], vec(data);\n    color = state_colors[true_labels],\n    ms = 3,\n    alpha = 1.0,\n    label = \"\",\n    xlabel = \"Input Feature 1\",\n    ylabel = \"Output\",\n    title = \"True vs. Predicted Regressions\"\n)\n\nxvals = range(minimum(Φ[1, :]), stop=maximum(Φ[1, :]), length=100)\n\nβ1_true = emission_1.β[:, 1]\ny_true_1 = β1_true[1] .+ β1_true[2] .* xvals\nplot!(xvals, y_true_1;\n    color = true_colors[1],\n    lw = 3,\n    linestyle = :solid,\n    label = \"State 1 (true)\"\n)\n\nβ2_true = emission_2.β[:, 1]\ny_true_2 = β2_true[1] .+ β2_true[2] .* xvals\nplot!(xvals, y_true_2;\n    color = true_colors[2],\n    lw = 3,\n    linestyle = :solid,\n    label = \"State 2 (true)\"\n)\n\nβ1_pred = test_model.B[1].β[:, 1]\ny_pred_1 = β1_pred[1] .+ β1_pred[2] .* xvals\nplot!(xvals, y_pred_1;\n    color = pred_colors[1],\n    lw = 3,\n    linestyle = :dash,\n    label = \"State 1 (pred)\"\n)\n\nβ2_pred = test_model.B[2].β[:, 1]\ny_pred_2 = β2_pred[1] .+ β2_pred[2] .* xvals\nplot!(xvals, y_pred_2;\n    color = pred_colors[2],\n    lw = 3,\n    linestyle = :dash,\n    label = \"State 2 (pred)\"\n)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-the-latent-state-predictions-using-Viterbi","page":"Gaussian GLM-GMM Example","title":"Visualize the latent state predictions using Viterbi","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"pred_labels= viterbi(test_model, data, Φ);\n\ntrue_mat = reshape(true_labels[1:1000], 1, :)\npred_mat = reshape(pred_labels[1:1000], 1, :)\n\np1 = heatmap(true_mat;\n    colormap = :roma50,\n    title = \"True State Labels\",\n    xlabel = \"\",\n    ylabel = \"\",\n    xticks = false,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\np2 = heatmap(pred_mat;\n    colormap = :roma50,\n    title = \"Predicted State Labels\",\n    xlabel = \"Timepoints\",\n    ylabel = \"\",\n    xticks = 0:200:1000,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\nplot(p1, p2;\n    layout = (2, 1),\n    size = (700, 500),\n    margin = 5Plots.mm)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Sampling-multiple,-independent-trials-of-data-from-an-HMM","page":"Gaussian GLM-GMM Example","title":"Sampling multiple, independent trials of data from an HMM","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"all_data = Vector{Matrix{Float64}}()\nΦ_total = Vector{Matrix{Float64}}()\n\nnum_trials = 100\nn=1000\nall_true_labels = []\n\nfor i in 1:num_trials\n    Φ = randn(rng, 3, n)\n    true_labels, data = rand(rng, true_model, Φ, n=n)\n    push!(all_true_labels, true_labels)\n    push!(all_data, data)\n    push!(Φ_total, Φ)\nend","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Fitting-an-HMM-to-multiple,-independent-trials-of-data","page":"Gaussian GLM-GMM Example","title":"Fitting an HMM to multiple, independent trials of data","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"A = [0.8 0.2; 0.1 0.9]\nπₖ = [0.6; 0.4]\nemission_1 = GaussianRegressionEmission(input_dim=3, output_dim=1, include_intercept=true, β=reshape([2.0, -1.0, 1.0, 2.0], :, 1), Σ=[2.0;;], λ=0.0)\nemission_2 = GaussianRegressionEmission(input_dim=3, output_dim=1, include_intercept=true, β=reshape([-2.5, -1.0, 3.5, 3.0], :, 1), Σ=[0.5;;], λ=0.0)\n\ntest_model = HiddenMarkovModel(K=2, A=A, πₖ=πₖ, B=[emission_1, emission_2])\n\nlls = fit!(test_model, all_data, Φ_total)\n\nplot(lls)\ntitle!(\"Log-likelihood over EM Iterations\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/#Visualize-latent-state-predictions-for-multiple-trials-of-data-using-Viterbi","page":"Gaussian GLM-GMM Example","title":"Visualize latent state predictions for multiple trials of data using Viterbi","text":"","category":"section"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"all_pred_labels_vec = viterbi(test_model, all_data, Φ_total)\nall_pred_labels = hcat(all_pred_labels_vec...)'\nall_true_labels_matrix = hcat(all_true_labels...)'\n\nstate_colors = [:dodgerblue, :crimson]\ntrue_subset = all_true_labels_matrix[1:10, 1:500]\npred_subset = all_pred_labels[1:10, 1:500]\n\np1 = heatmap(\n    true_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"True State Labels\",\n    xlabel = \"\",\n    ylabel = \"Trials\",\n    xticks = false,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\np2 = heatmap(\n    pred_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"Predicted State Labels\",\n    xlabel = \"Timepoints\",\n    ylabel = \"Trials\",\n    xticks = true,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\nfinal_plot = plot(\n    p1, p2,\n    layout = (2, 1),\n    size = (850, 550),\n    margin = 5Plots.mm,\n    legend = false,\n)\n\ndisplay(final_plot)","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"","category":"page"},{"location":"tutorials/gaussian_glm_hmm_example/","page":"Gaussian GLM-GMM Example","title":"Gaussian GLM-GMM Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"HiddenMarkovModels/#What-is-a-Hidden-Markov-Model?","page":"Hidden Markov Models","title":"What is a Hidden Markov Model?","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model (HMM) is a graphical model that describes how systems change over time. When modeling a time series with T observations using an HMM, we assume that the observed data y_1T depends on hidden states x_1T that are not observed. Specifically, an HMM is a type of state-space model in which the hidden states are discrete.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The three components of an HMM are as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"An initial state distribution (pi): which hidden states we are likely to start in.\nA transition matrix (A): how the hidden states evolve over time.\nAn emission model: how the hidden states generate the observed data.","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.HiddenMarkovModel","page":"Hidden Markov Models","title":"StateSpaceDynamics.HiddenMarkovModel","text":"HiddenMarkovModel\n\nStore a Hidden Markov Model (HMM) with custom emissions.\n\nFields\n\nA::AbstractMatrix{<:Real}: Transition matrix.\nB::AbstractVector{<:EmissionModel}: State-dependent emission models.\nπₖ::AbstractVector{<:Real}: Initial state distribution.\nK::Int: Number of states.\n\n\n\n\n\n","category":"type"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is given by:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beginalign*\n    x_1 sim textCat(pi) \n    x_t mid x_t-1 sim textCat(A_x_t-1 ) \n    y_t mid x_t sim p(y_t mid theta_x_t)\nendalign*","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t is the hidden (discrete) state at time t\ny_t is the observed data at time t\npi is the initial state distribution\nmathbfA is the state transition matrix\ntheta_x_t are the parameters of the emission distribution for state x_t","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The emission model can take many forms: Gaussian, Poisson, Bernoulli, categorical, etc... In the case of a Gaussian emission distribution, this becomes:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) sim mathcalN(mu_k Sigma_k)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"mu_k is the mean of the emission distribution for state k\nSigma_k is the covariance of the emission distribution for state k","category":"page"},{"location":"HiddenMarkovModels/#What-is-a-Generalized-Linear-Model-Hidden-Markov-Model","page":"Hidden Markov Models","title":"What is a Generalized Linear Model - Hidden Markov Model","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"A Hidden Markov Model - Generalized Linear Model (GLM-HMM) - also known as Switching Regression Model - is an extension to classic HMMs where the emission models are state-dependent GLMs that link an observed input to an observed output. This formulation allows each hidden state to define its own regression relationship between inputs and outputs, enabling the model to capture complex, state-dependent dynamics in the data. Currently, StateSpaceDynamics.jl support Gaussian, Bernoulli, Poisson, and Autoregressive GLMs as emission models.","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The generative model is as follows:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beginalign*\n    x_1 sim textCat(pi) \n    x_t mid x_t-1 sim textCat(A_x_t-1 ) \n    y_t mid x_t u_t sim p(y_t mid theta_x_t u_t)\nendalign*","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"x_t is the hidden (discrete) state at time t\ny_t is the observed output at time t\nu_t is the observed input (covariate) at time t\ntheta_x_t are the parameters of the GLM emission model for state x_t","category":"page"},{"location":"HiddenMarkovModels/#Example-Emission-Models","page":"Hidden Markov Models","title":"Example Emission Models","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For example, if the emission is a Gaussian GLM:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) u_t sim mathcalN(mu_k + beta_k^top u_t sigma_k^2)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k are the regression weights for state k\nsigma_k^2 is the state-dependent variance\nmu_k is the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"If the emission is Bernoulli (for binary outputs):","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"y_t mid (x_t = k) u_t sim textBernoulli left( sigma left( mu_k + beta_k^top u_t right) right)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Where:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"beta_k are the regression weights for state k\nsigma(cdot) is the logistic sigmoid function for binary outputs\nmu_k is the state-dependent bias","category":"page"},{"location":"HiddenMarkovModels/#Sampling-from-an-HMM","page":"Hidden Markov Models","title":"Sampling from an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"You can generate synthetic data from an HMM:","category":"page"},{"location":"HiddenMarkovModels/#Base.rand","page":"Hidden Markov Models","title":"Base.rand","text":"Random.rand(\n    rng::AbstractRNG,\n    model::HiddenMarkovModel,\n    X::Union{Matrix{<:Real}, Nothing}=nothing;\n    n::Int,\n    autoregressive::Bool=false)\n\nGenerate n samples from a Hidden Markov Model. Returns a tuple of the state sequence and the observation sequence.\n\nArguments\n\nrng::AbstractRNG: The seed.\nmodel::HiddenMarkovModel: The Hidden Markov Model to sample from.\nX: The input data for switching regression models.\nn::Int: The number of samples to generate.\n\nReturns\n\nstate_sequence::Vector{Int}: The state sequence, where each element is an integer 1:K.\nobservation_sequence::Matrix{Float64}: The observation sequence. This takes the form of the emission model's output.\n\n\n\n\n\n","category":"function"},{"location":"HiddenMarkovModels/#Learning-in-an-HMM","page":"Hidden Markov Models","title":"Learning in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"StateSpaceDynamics.jl implements Expectation-Maximization (EM) for parameter learning in both HMMs and GLM-HMMs. EM is an iterative method for finding maximum likelihood estimates of the parameters in graphical models with hidden variables. ","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.fit!-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{<:Real}}}} where T<:Real","page":"Hidden Markov Models","title":"StateSpaceDynamics.fit!","text":"fit!(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real}, Nothing}=nothing; max_iters::Int=100, tol::Float64=1e-6)\n\nFit the Hidden Markov Model using the EM algorithm.\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data.\nX::Union{Matrix{<:Real}, Nothing}=nothing: Optional input data for fitting Switching Regression Models\nmax_iters::Int=100: The maximum number of iterations to run the EM algorithm.\ntol::Float64=1e-6: When the log likelihood is improving by less than this value, the algorithm will stop.\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#Expectation-Step-(E-step)","page":"Hidden Markov Models","title":"Expectation Step (E-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the expectation step (E-step), we calculate the posterior distribution of the latent states given the current parameters of the model:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"p(X mid Y theta_textold)","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"We use dynamic programming to efficiently calculate this posterior using the forward and backward recursions for HMMs. This posterior is then used to construct the expectation of the complete data log-likelihood, also known as the Q-function:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"Q(theta theta_textold) = sum_X p(X mid Y theta_textold) ln p(Y X mid theta)","category":"page"},{"location":"HiddenMarkovModels/#Maximization-Step-(M-step)","page":"Hidden Markov Models","title":"Maximization Step (M-step)","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"In the maximization step (M-step), we maximize this expectation with respect to the parameters theta. Specifically:","category":"page"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For the initial state distribution and the transition matrix, we use analytical updates for the parameters, derived using Lagrange multipliers.\nFor emission models in the case of HMMs, we also implement analytical updates.\nIf the emission model is a GLM, we use Optim.jl to numerically optimize the objective function.","category":"page"},{"location":"HiddenMarkovModels/#Inference-in-an-HMM","page":"Hidden Markov Models","title":"Inference in an HMM","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For state inference in Hidden Markov Models (HMMs), we implement two common algorithms:","category":"page"},{"location":"HiddenMarkovModels/#Forward-Backward-Algorithm","page":"Hidden Markov Models","title":"Forward-Backward Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Forward-Backward algorithm is used to compute the posterior state probabilities at each time step. Given the observed data, it calculates the probability of being in each possible hidden state at each time step, marginalizing over all possible state sequences.","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.class_probabilities-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{<:Real}}}} where T<:Real","page":"Hidden Markov Models","title":"StateSpaceDynamics.class_probabilities","text":"function class_probabilities(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nCalculate the class probabilities at each time point using forward backward algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nclass_probabilities::Matrix{Float64}: The class probabilities at each timepoint\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#Viterbi-Algorithm","page":"Hidden Markov Models","title":"Viterbi Algorithm","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"The Viterbi algorithm is used for best state sequence labeling. It finds the most likely sequence of hidden states given the observed data. This is done by dynamically computing the highest probability path through the state space, which maximizes the likelihood of the observed sequence.","category":"page"},{"location":"HiddenMarkovModels/#StateSpaceDynamics.viterbi-Union{Tuple{T}, Tuple{HiddenMarkovModel, AbstractMatrix{T}}, Tuple{HiddenMarkovModel, AbstractMatrix{T}, Union{Nothing, AbstractMatrix{<:Real}}}} where T<:Real","page":"Hidden Markov Models","title":"StateSpaceDynamics.viterbi","text":"viterbi(model::HiddenMarkovModel, Y::Matrix{<:Real}, X::Union{Matrix{<:Real},Nothing}=nothing;)\n\nGet most likely class labels using the Viterbi algorithm\n\nArguments\n\nmodel::HiddenMarkovModel: The Hidden Markov Model to fit.\nY::Matrix{<:Real}: The emission data\nX::Union{Matrix{<:Real},Nothing}=nothing: Optional input data for fitting Switching Regression Models\n\nReturns\n\nbest_path::Vector{Float64}: The most likely state label at each timepoint\n\n\n\n\n\n","category":"method"},{"location":"HiddenMarkovModels/#Reference","page":"Hidden Markov Models","title":"Reference","text":"","category":"section"},{"location":"HiddenMarkovModels/","page":"Hidden Markov Models","title":"Hidden Markov Models","text":"For a complete mathematical formulation of the relevant HMM and HMM-GLM learning and inference algorithms, we recommend Pattern Recognition and Machine Learning, Chapter 13 by Christopher Bishop.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Simulating-and-Fitting-a-Gaussian-Mixture-Model","page":"Probabilistic PCA Example","title":"Simulating and Fitting a Gaussian Mixture Model","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create a Probabilistic PCA model and fit it using the EM algorithm.","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Load-Packages","page":"Probabilistic PCA Example","title":"Load Packages","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing StatsPlots\nusing StableRNGs\nusing Distributions","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Create-a-State-Space-Model","page":"Probabilistic PCA Example","title":"Create a State-Space Model","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"D = 2\nk = 2","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"define true parameters for the model to sample from","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"W_true = [\n   -1.64   0.2;\n    0.9  -2.8\n]\n\nσ²_true = 0.5\nμ_true = [1.65, -1.3]\n\nppca = ProbabilisticPCA(W_true, σ²_true, μ_true)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Sample data from the model","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"num_obs = 500\nX, z = rand(rng, ppca, num_obs)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Plot-sampled-data-from-the-model","page":"Probabilistic PCA Example","title":"Plot sampled data from the model","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"x1 = X[1, :]\nx2 = X[2, :]\nlabels = map(i -> (abs(z[1,i]) > abs(z[2,i]) ? 1 : 2), 1:size(z,2))\n\np = plot()\n\nscatter!(\n    p, x1, x2;\n    group      = labels,\n    xlabel     = \"X₁\",\n    ylabel     = \"X₂\",\n    title      = \"Samples grouped by dominant latent factor\",\n    label= [\"Latent 1\" \"Latent 2\"],\n    legend     = :topright,\n    markersize = 5,\n)\n\np","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Paramter-recovery:-Initialize-a-new-model-with-default-parameters-and-fit-to-the-data-using-EM.","page":"Probabilistic PCA Example","title":"Paramter recovery: Initialize a new model with default parameters and fit to the data using EM.","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"#define default parameters\nD = 2\nk = 2\nW = randn(rng, D, k)\nσ² = 0.5\nμ_vector = randn(rng, 2)\n\nfit_ppca = ProbabilisticPCA(W, σ², μ_vector)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Fit-model-using-EM-Algorithm","page":"Probabilistic PCA Example","title":"Fit model using EM Algorithm","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"lls = fit!(fit_ppca, X)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Confirm-model-convergence-using-log-likelihoods","page":"Probabilistic PCA Example","title":"Confirm model convergence using log likelihoods","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"ll_plot = plot(\n    lls;\n    xlabel=\"Iteration\",\n    ylabel=\"Log-Likelihood\",\n    title=\"EM Convergence (PPCA)\",\n    marker=:circle,\n    label=\"log_likelihood\",\n    reuse=false,\n)\n\nll_plot","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/#Plot-the-learned-lower-dimension-latent-space-over-the-Data","page":"Probabilistic PCA Example","title":"Plot the learned lower dimension latent space over the Data","text":"","category":"section"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"x1, x2 = X[1, :], X[2, :]\nμ1, μ2  = fit_ppca.μ\nW_fit = fit_ppca.W\nw1      = W_fit[:, 1]\nw2      = W_fit[:, 2]\n\nP = plot()\n\nscatter!(\n    P, x1, x2;\n    xlabel     = \"X₁\",\n    ylabel     = \"X₂\",\n    title      = \"Data with PPCA loading directions\",\n    label      = \"Data\",\n    alpha      = 0.5,\n    markersize = 4,\n)\n\nk = size(W_fit, 2)\nbases_x = repeat([μ1], k)\nbases_y = repeat([μ2], k)","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"Add the component arrows in both directions","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"quiver!(\n  P, [μ1], [μ2];\n  quiver      = ([ w1[1]], [ w1[2]]),\n  arrow       = :arrow, lw=3,\n  color       = :red,\n  label       = \"W₁\"\n)\nquiver!(\n  P, [μ1], [μ2];\n  quiver      = ([-w1[1]], [-w1[2]]),\n  arrow       = :arrow, lw=3,\n  color       = :red,\n  label       = \"\"\n)\n\nquiver!(\n  P, [μ1], [μ2];\n  quiver      = ([ w2[1]], [ w2[2]]),\n  arrow       = :arrow, lw=3,\n  color       = :green,\n  label       = \"W₂\"\n)\nquiver!(\n  P, [μ1], [μ2];\n  quiver      = ([-w2[1]], [-w2[2]]),\n  arrow       = :arrow, lw=3,\n  color       = :green,\n  label       = \"\"\n)\n\nP","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"","category":"page"},{"location":"tutorials/Probabilistic_PCA_example/","page":"Probabilistic PCA Example","title":"Probabilistic PCA Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Simulating-and-Fitting-a-Hidden-Markov-Model","page":"Hidden Markov Model Example","title":"Simulating and Fitting a Hidden Markov Model","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create, sample from, and fit Hidden Markov Models (HMMs).","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Load-Packages","page":"Hidden Markov Model Example","title":"Load Packages","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"using LinearAlgebra\nusing Plots\nusing Random\nusing StateSpaceDynamics\nusing StableRNGs","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"rng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Create-an-HMM","page":"Hidden Markov Model Example","title":"Create an HMM","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"output_dim = 2\n\nA = [0.99 0.01; 0.05 0.95];\nπₖ = [0.5; 0.5]\n\nμ_1 = [-1.0, -1.0]\nΣ_1 = 0.1 * Matrix{Float64}(I, output_dim, output_dim)\nemission_1 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1)\n\nμ_2 = [1.0, 1.0]\nΣ_2 = 0.2 * Matrix{Float64}(I, output_dim, output_dim)\nemission_2 = GaussianEmission(output_dim=output_dim, μ=μ_2, Σ=Σ_2)\n\nmodel = HiddenMarkovModel(K=2, B=[emission_1, emission_2], A=A, πₖ=πₖ)","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Sample-from-the-HMM","page":"Hidden Markov Model Example","title":"Sample from the HMM","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"num_samples = 10000\ntrue_labels, data = rand(rng, model, n=num_samples)","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Visualize-the-sampled-dataset","page":"Hidden Markov Model Example","title":"Visualize the sampled dataset","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"x_vals = data[1, 1:num_samples]\ny_vals = data[2, 1:num_samples]\nlabels_slice = true_labels[1:num_samples]\n\nstate_colors = [:dodgerblue, :crimson]\n\nplt = plot()\nfor state in 1:2\n    idx = findall(labels_slice .== state)\n    scatter!(x_vals[idx], y_vals[idx];\n        color=state_colors[state],\n        label=\"State $state\",\n        markersize=6)\nend\n\nplot!(x_vals, y_vals;\n    color=:gray,\n    lw=1.5,\n    linealpha=0.4,\n    label=\"\")\n\nscatter!([x_vals[1]], [y_vals[1]];\n    color=:green,\n    markershape=:star5,\n    markersize=10,\n    label=\"Start\")\n\nscatter!([x_vals[end]], [y_vals[end]];\n    color=:black,\n    markershape=:diamond,\n    markersize=8,\n    label=\"End\")\n\nxlabel!(\"Output dim 1\")\nylabel!(\"Output dim 2\")\ntitle!(\"Emissions from HMM (First 100 Points)\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Initialize-and-fit-a-new-HMM-to-the-sampled-data","page":"Hidden Markov Model Example","title":"Initialize and fit a new HMM to the sampled data","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"μ_1 = [-0.25, -0.25]\nΣ_1 = 0.3 * Matrix{Float64}(I, output_dim, output_dim)\nemission_1 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1)\n\nμ_2 = [0.25, 0.25]\nΣ_2 = 0.5 * Matrix{Float64}(I, output_dim, output_dim)\nemission_2 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1)\n\nA = [0.8 0.2; 0.05 0.95]\nπₖ = [0.6,0.4]\ntest_model = HiddenMarkovModel(K=2, B=[emission_1, emission_2], A=A, πₖ=πₖ)\n\nlls = fit!(test_model, data)\n\nplot(lls)\ntitle!(\"Log-likelihood over EM Iterations\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Visualize-the-latent-state-predictions-using-Viterbi","page":"Hidden Markov Model Example","title":"Visualize the latent state predictions using Viterbi","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"pred_labels= viterbi(test_model, data);\n\ntrue_mat = reshape(true_labels[1:1000], 1, :)\npred_mat = reshape(pred_labels[1:1000], 1, :)\n\np1 = heatmap(true_mat;\n    colormap = :roma50,\n    title = \"True State Labels\",\n    xlabel = \"\",\n    ylabel = \"\",\n    xticks = false,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\np2 = heatmap(pred_mat;\n    colormap = :roma50,\n    title = \"Predicted State Labels\",\n    xlabel = \"Timepoints\",\n    ylabel = \"\",\n    xticks = 0:200:1000,\n    yticks = false,\n    colorbar = false,\n    framestyle = :box)\n\nplot(p1, p2;\n    layout = (2, 1),\n    size = (700, 500),\n    margin = 5Plots.mm)","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Sampling-multiple,-independent-trials-of-data-from-an-HMM","page":"Hidden Markov Model Example","title":"Sampling multiple, independent trials of data from an HMM","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"n_trials = 100\nn_samples = 1000\n\nall_true_labels = Vector{Vector{Int}}(undef, n_trials)\nall_data = Vector{Matrix{Float64}}(undef, n_trials)\n\nfor i in 1:n_trials\n    true_labels, data = rand(rng, model, n=n_samples)\n    all_true_labels[i] = true_labels\n    all_data[i] = data\nend","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Fitting-an-HMM-to-multiple,-independent-trials-of-data","page":"Hidden Markov Model Example","title":"Fitting an HMM to multiple, independent trials of data","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"μ_1 = [-0.25, -0.25]\nΣ_1 = 0.3 * Matrix{Float64}(I, output_dim, output_dim)\nemission_1 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1)\n\nμ_2 = [0.25, 0.25]\nΣ_2 = 0.5 * Matrix{Float64}(I, output_dim, output_dim)\nemission_2 = GaussianEmission(output_dim=output_dim, μ=μ_1, Σ=Σ_1)\n\nA = [0.8 0.2; 0.05 0.95]\nπₖ = [0.6,0.4]\ntest_model = HiddenMarkovModel(K=2, B=[emission_1, emission_2], A=A, πₖ=πₖ)\n\nlls = fit!(test_model, all_data)\n\nplot(lls)\ntitle!(\"Log-likelihood over EM Iterations\")\nxlabel!(\"EM Iteration\")\nylabel!(\"Log-Likelihood\")","category":"page"},{"location":"tutorials/hidden_markov_model_example/#Visualize-latent-state-predictions-for-multiple-trials-of-data-using-Viterbi","page":"Hidden Markov Model Example","title":"Visualize latent state predictions for multiple trials of data using Viterbi","text":"","category":"section"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"all_pred_labels_vec = viterbi(test_model, all_data)\nall_pred_labels = hcat(all_pred_labels_vec...)'\nall_true_labels_matrix = hcat(all_true_labels...)'\n\nstate_colors = [:dodgerblue, :crimson]\ntrue_subset = all_true_labels_matrix[1:10, 1:500]\npred_subset = all_pred_labels[1:10, 1:500]\n\np1 = heatmap(\n    true_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"True State Labels\",\n    xlabel = \"\",\n    ylabel = \"Trials\",\n    xticks = false,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\np2 = heatmap(\n    pred_subset,\n    colormap = :roma50,\n    colorbar = false,\n    title = \"Predicted State Labels\",\n    xlabel = \"Timepoints\",\n    ylabel = \"Trials\",\n    xticks = true,\n    yticks = true,\n    margin = 5Plots.mm,\n    legend = false\n)\n\nfinal_plot = plot(\n    p1, p2,\n    layout = (2, 1),\n    size = (850, 550),\n    margin = 5Plots.mm,\n    legend = false,\n)\n\ndisplay(final_plot)","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"","category":"page"},{"location":"tutorials/hidden_markov_model_example/","page":"Hidden Markov Model Example","title":"Hidden Markov Model Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Simulating-and-Fitting-a-Poisson-Linear-Dynamical-System","page":"Poisson LDS Example","title":"Simulating and Fitting a Poisson Linear Dynamical System","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to simulate and fit a Linear Dynamical System (LDS) with Poisson observations using the Laplace-EM algorithm.","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Load-Packages","page":"Poisson LDS Example","title":"Load Packages","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing LaTeXStrings\nusing StableRNGs","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"rng = StableRNG(123);\nnothing #hide","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Create-a-Poisson-Linear-Dynamical-System","page":"Poisson LDS Example","title":"Create a Poisson Linear Dynamical System","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"obs_dim = 10\nlatent_dim = 2\n\nA = 0.95 * [cos(0.25) -sin(0.25); sin(0.25) cos(0.25)]\nQ = Matrix(0.1 * I(latent_dim))\nx0 = zeros(latent_dim)\nP0 = Matrix(0.1 * I(latent_dim))\n\nlog_d = log.(fill(0.1, obs_dim))\nC = permutedims([abs.(randn(rng, obs_dim))'; abs.(randn(rng, obs_dim))'])\n\nstate_model = GaussianStateModel(; A, Q, x0, P0)\nobs_model = PoissonObservationModel(; C, log_d)\n\ntrue_plds = LinearDynamicalSystem(;\n    state_model=state_model,\n    obs_model=obs_model,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)\n)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Simulate-Latent-States-and-Observations","page":"Poisson LDS Example","title":"Simulate Latent States and Observations","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"tSteps = 500\nlatents, observations = rand(rng, true_plds; tsteps=tSteps, ntrials=1)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Plot-Vector-Field-of-Latent-Dynamics","page":"Poisson LDS Example","title":"Plot Vector Field of Latent Dynamics","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"x = y = -3:0.5:3\nX = repeat(x', length(y), 1)\nY = repeat(y, 1, length(x))\nU = zeros(size(X))\nV = zeros(size(Y))\n\nfor i in 1:size(X, 1), j in 1:size(X, 2)\n    v = A * [X[i,j], Y[i,j]]\n    U[i,j] = v[1] - X[i,j]\n    V[i,j] = v[2] - Y[i,j]\nend\n\nmagnitude = @. sqrt(U^2 + V^2)\nU_norm = U ./ magnitude\nV_norm = V ./ magnitude\n\np = quiver(X, Y, quiver=(U_norm, V_norm), color=:blue, alpha=0.3,\n           linewidth=1, arrow=arrow(:closed, :head, 0.1, 0.1))\nplot!(latents[1, :, 1], latents[2, :, 1], xlabel=\"x₁\", ylabel=\"x₂\",\n      color=:black, linewidth=1.5, title=\"Latent Dynamics\", legend=false)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Plot-Latent-States-and-Observations","page":"Poisson LDS Example","title":"Plot Latent States and Observations","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"states = latents[:, :, 1]\nemissions = observations[:, :, 1]\ntime_bins = size(states, 2)\n\nplot(size=(800, 600), layout=@layout[a{0.3h}; b])\n\nlim_states = maximum(abs.(states))\nfor d in 1:latent_dim\n    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black,\n          linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, time_bins), title=\"Simulated Latent States\",\n      yformatter=y->\"\", tickfontsize=12)\n\ncolors = palette(:default, obs_dim)\nfor f in 1:obs_dim\n    spike_times = findall(x -> x > 0, emissions[f, :])\n    for t in spike_times\n        plot!([t, t], [f-0.4, f+0.4], color=colors[f], linewidth=1, label=\"\", subplot=2)\n    end\nend\n\nplot!(subplot=2, yticks=(1:obs_dim, [L\"y_{%$d}\" for d in 1:obs_dim]),\n      xlims=(0, time_bins), ylims=(0.5, obs_dim + 0.5), title=\"Simulated Emissions\",\n      xlabel=\"Time\", tickfontsize=12, grid=false)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Initialize-Model-and-Smooth","page":"Poisson LDS Example","title":"Initialize Model and Smooth","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"Initialize with random parameters","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"A_init = random_rotation_matrix(latent_dim, rng)\nQ_init = Matrix(0.1 * I(latent_dim))\nC_init = randn(rng, obs_dim, latent_dim)\nlog_d_init = log.(fill(0.1, obs_dim))\nx0_init = zeros(latent_dim)\nP0_init = Matrix(0.1 * I(latent_dim))\n\nsm_init = GaussianStateModel(; A=A_init, Q=Q_init, x0=x0_init, P0=P0_init)\nom_init = PoissonObservationModel(; C=C_init, log_d=log_d_init)\n\nnaive_plds = LinearDynamicalSystem(;\n    state_model=sm_init,\n    obs_model=om_init,\n    latent_dim=latent_dim,\n    obs_dim=obs_dim,\n    fit_bool=fill(true, 6)\n)\n\nsmoothed_x, smoothed_p, _ = smooth(naive_plds, observations)\n\nplot()\nfor d in 1:latent_dim\n    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:time_bins, smoothed_x[d, :, 1] .+ lim_states * (d-1), color=:red, linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, time_bins), title=\"True vs. Predicted Latent States (Pre-EM)\",\n      yformatter=y->\"\", tickfontsize=12)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#Fit-the-Poisson-LDS-Using-Laplace-EM","page":"Poisson LDS Example","title":"Fit the Poisson LDS Using Laplace EM","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"elbo, _ = fit!(naive_plds, observations; max_iter=25, tol=1e-6)\n\nsmoothed_x, smoothed_p, _ = smooth(naive_plds, observations)\n\nplot()\nfor d in 1:latent_dim\n    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label=\"\", subplot=1)\n    plot!(1:time_bins, smoothed_x[d, :, 1] .+ lim_states * (d-1), color=:red, linewidth=2, label=\"\", subplot=1)\nend\n\nplot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L\"x_%$d\" for d in 1:latent_dim]),\n      xticks=[], xlims=(0, time_bins), title=\"True vs. Predicted Latent States (Post-EM)\",\n      yformatter=y->\"\", tickfontsize=12)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/#ELBO-Convergence","page":"Poisson LDS Example","title":"ELBO Convergence","text":"","category":"section"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"plot(elbo, xlabel=\"iteration\", ylabel=\"ELBO\", title=\"ELBO over Iterations\", legend=false)","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"","category":"page"},{"location":"tutorials/poisson_latent_dynamics_example/","page":"Poisson LDS Example","title":"Poisson LDS Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Simulating-and-Fitting-a-Gaussian-Mixture-Model","page":"Gaussian Mixture Model Example","title":"Simulating and Fitting a Gaussian Mixture Model","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"This tutorial demonstrates how to use StateSpaceDynamics.jl to create a Gaussian Mixture Model and fit it using the EM algorithm.","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"using StateSpaceDynamics\nusing LinearAlgebra\nusing Random\nusing Plots\nusing StableRNGs\nusing Distributions\nusing StatsPlots\n\nrng = StableRNG(1234);\nnothing #hide","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Create-a-true-GaussianMixtureModel-to-simulate-from","page":"Gaussian Mixture Model Example","title":"Create a true GaussianMixtureModel to simulate from","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"k = 3\nD = 2  # data dimension\n\ntrue_μs = [\n    -1.0  1.0  0.0;\n    -1.0 -1.5  2.0\n]  # shape (D, K)\n\ntrue_Σs = [Matrix{Float64}(0.3 * I(2)) for _ in 1:k]\ntrue_πs = [0.5, 0.2, 0.3]\n\ntrue_gmm = GaussianMixtureModel(k, true_μs, true_Σs, true_πs)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Sample-data-from-the-true-GMM","page":"Gaussian Mixture Model Example","title":"Sample data from the true GMM","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"n = 500","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"generate component labels (for plotting)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"labels = rand(rng, Categorical(true_πs), n)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"generate samples from the GMM","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"X = Matrix{Float64}(undef, D, n)\nfor i in 1:n\n    X[:, i] = rand(rng, MvNormal(true_μs[:, labels[i]], true_Σs[labels[i]]))\nend\n\np1 = scatter(\n    X[1, :], X[2, :];\n    group=labels,\n    title=\"GMM Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    markersize=4,\n    alpha=0.8,\n    legend=false,\n)\np1","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Fit-a-new-GaussianMixtureModel-to-the-data","page":"Gaussian Mixture Model Example","title":"Fit a new GaussianMixtureModel to the data","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"fit_gmm = GaussianMixtureModel(k, D)\n\nclass_probabilities, lls = fit!(fit_gmm, X;\n    maxiter=100, tol=1e-6, initialize_kmeans=true)","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Plot-log-likelihoods-to-visualize-EM-convergence","page":"Gaussian Mixture Model Example","title":"Plot log-likelihoods to visualize EM convergence","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"p2 = plot(\n    lls;\n    xlabel=\"Iteration\",\n    ylabel=\"Log-Likelihood\",\n    title=\"EM Convergence\",\n    label=\"log_likelihood\",\n    marker=:circle,\n)\np2","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/#Visualize-model-contours-over-the-data","page":"Gaussian Mixture Model Example","title":"Visualize model contours over the data","text":"","category":"section"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"xs = collect(range(minimum(X[1, :]) - 1, stop=maximum(X[1, :]) + 1, length=150))\nys = collect(range(minimum(X[2, :]) - 1, stop=maximum(X[2, :]) + 1, length=150))\n\np3 = scatter(\n    X[1, :], X[2, :];\n    markersize=3, alpha=0.5,\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    title=\"Data & Fitted GMM Contours by Component\",\n    legend=:topright,\n)\n\ncolors = [:red, :green, :blue]\n\nfor i in 1:fit_gmm.k\n    comp_dist = MvNormal(fit_gmm.μₖ[:, i], fit_gmm.Σₖ[i])\n    Z_i = [fit_gmm.πₖ[i] * pdf(comp_dist, [x, y]) for y in ys, x in xs]\n\n    contour!(\n        p3, xs, ys, Z_i;\n        levels=10,\n        linewidth=2,\n        c=colors[i],\n        label=\"Comp $i\",\n    )\nend\n\np3","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"","category":"page"},{"location":"tutorials/gaussian_mixture_model_example/","page":"Gaussian Mixture Model Example","title":"Gaussian Mixture Model Example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl is a comprehensive Julia package for state space modeling, designed specifically with neuroscientific applications in mind. The package provides efficient implementations of various state space models along with tools for parameter estimation, state inference, and model selection.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install StateSpaceDynamics.jl, start up Julia and type the following code-snipped into the REPL. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"StateSpaceDynamics\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"or alternatively, you can enter the package manager by typing ] and then run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add StateSpaceDynamics","category":"page"},{"location":"#What-are-State-Space-Models?","page":"Home","title":"What are State Space Models?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"State space models are a class of probabilistic models that describe the evolution of a system through two main components - a latent and observation process. The latent process is a stochastic process that is not directly observed, but is used to generate the observed data. The observation process is a conditional distribution that describes how the observed data is generated from the latent process.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In their most general form, state space models can be written as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim p(x_t+1  x_t) \n    y_t sim p(y_t  x_t)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x_t is the latent state at time t and y_t is the observed data at time t.","category":"page"},{"location":"#Example:-Linear-Dynamical-Systems","page":"Home","title":"Example: Linear Dynamical Systems","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A fundamental example is the Linear Dynamical System (LDS), which combines linear dynamics with Gaussian noise. The LDS can be expressed in two equivalent forms:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Equation form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 = A x_t + b + epsilon_t \n    y_t = C x_t + d + delta_t\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where:","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathbfA is the state transition matrix\nmathbfC is the observation matrix  \nmathbfb and mathbfd are bias terms\nboldsymbolepsilon_t and boldsymboldelta_t are Gaussian noise terms with covariances mathbfQ and mathbfR respectively","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distributional form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginalign*\n    x_t+1 sim mathcalN(A x_t + b Q) \n    y_t sim mathcalN(C x_t + d R)\nendalign*","category":"page"},{"location":"","page":"Home","title":"Home","text":"where mathbfQ and mathbfR are the state and observation noise covariance matrices, respectively.","category":"page"},{"location":"#Models-Implemented","page":"Home","title":"Models Implemented","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"StateSpaceDynamics.jl implements several types of state space models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Linear Dynamical Systems (LDS)\nGaussian LDS\nPoisson LDS\nHidden Markov Models (HMM)\nGaussian emissions\nRegression-based emissions\nGaussian regression\nBernoulli regression\nPoisson regression\nAutoregressive emissions","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here's a simple example on how to create a Gaussian SSM.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using StateSpaceDynamics\nusing LinearAlgebra\n\n# Define model dimensions\nlatent_dim = 3\nobs_dim = 10\n\n# Define state model parameters\nA = 0.95 * I(latent_dim)\nQ = 0.01 * I(latent_dim)\nx0 = zeros(latent_dim)\nP0 = I(latent_dim)\nstate_model = GaussianStateModel(A, Q, x0, P0)\n\n# Define observation model parameters\nC = randn(obs_dim, latent_dim)\nR = 0.1 * I(obs_dim)\nobs_model = GaussianObservationModel(C, R)\n\n# Construct the LDS\nlds = LinearDynamicalSystem(state_model, obs_model, latent_dim, obs_dim, fill(true, 6))","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you encounter a bug or would like to contribute to the package, please open an issue on our GitHub repository. Once the suggested change has received positive feedback feel free to submit a PR adhering to the blue style guide.","category":"page"},{"location":"#Citing-StateSpaceDynamics.jl","page":"Home","title":"Citing StateSpaceDynamics.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Our work is currently under review in the Journal of Open Source Software. For now, if you use StateSpaceDynamics.jl in your research, please use the following bibtex citation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@software{Senne_Zenodo_SSD,\n  author       = {Ryan Senne and Zachary Loschinskey and James Fourie and Carson Loughridge and Brian DePasquale},\n  title        = {StateSpaceDynamics.jl},\n  month        = jun,\n  year         = 2025,\n  publisher    = {Zenodo},\n  version      = {v1.0.0},\n  doi          = {10.5281/zenodo.15668420},\n  url          = {https://doi.org/10.5281/zenodo.15668420}\n}","category":"page"}]
}
