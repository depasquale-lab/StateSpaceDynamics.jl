<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Mixture Models · StateSpaceDynamics.jl</title><meta name="title" content="Mixture Models · StateSpaceDynamics.jl"/><meta property="og:title" content="Mixture Models · StateSpaceDynamics.jl"/><meta property="twitter:title" content="Mixture Models · StateSpaceDynamics.jl"/><meta name="description" content="Documentation for StateSpaceDynamics.jl."/><meta property="og:description" content="Documentation for StateSpaceDynamics.jl."/><meta property="twitter:description" content="Documentation for StateSpaceDynamics.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="StateSpaceDynamics.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">StateSpaceDynamics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../LinearDynamicalSystems/">Linear Dynamical Systems</a></li><li><a class="tocitem" href="../HiddenMarkovModels/">Hidden Markov Models</a></li><li><a class="tocitem" href="../SLDS/">Switching Linear Dynamical Systems</a></li><li><a class="tocitem" href="../EmissionModels/">Emission Models</a></li><li class="is-active"><a class="tocitem" href>Mixture Models</a><ul class="internal"><li><a class="tocitem" href="#Generative-Process"><span>Generative Process</span></a></li><li><a class="tocitem" href="#Key-Properties"><span>Key Properties</span></a></li><li><a class="tocitem" href="#Gaussian-Mixture-Model"><span>Gaussian Mixture Model</span></a></li><li><a class="tocitem" href="#Mathematical-Formulation"><span>Mathematical Formulation</span></a></li><li><a class="tocitem" href="#Applications"><span>Applications</span></a></li><li><a class="tocitem" href="#Poisson-Mixture-Model"><span>Poisson Mixture Model</span></a></li><li><a class="tocitem" href="#Learning-in-Mixture-Models"><span>Learning in Mixture Models</span></a></li><li><a class="tocitem" href="#Convergence-and-Initialization"><span>Convergence and Initialization</span></a></li><li><a class="tocitem" href="#Model-Evaluation-and-Selection"><span>Model Evaluation and Selection</span></a></li><li><a class="tocitem" href="#Information-Criteria"><span>Information Criteria</span></a></li><li><a class="tocitem" href="#Inference-and-Applications"><span>Inference and Applications</span></a></li><li><a class="tocitem" href="#Sampling"><span>Sampling</span></a></li><li><a class="tocitem" href="#Practical-Considerations"><span>Practical Considerations</span></a></li><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li></ul></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/gaussian_latent_dynamics_example/">Gaussian LDS Example</a></li><li><a class="tocitem" href="../tutorials/poisson_latent_dynamics_example/">Poisson LDS Example</a></li><li><a class="tocitem" href="../tutorials/lds_model_selection_example/">LDS Model Selection Example</a></li><li><a class="tocitem" href="../tutorials/lds_identifiability_example/">Non-Identifiability in LDS Models</a></li><li><a class="tocitem" href="../tutorials/hidden_markov_model_example/">Hidden Markov Model Example</a></li><li><a class="tocitem" href="../tutorials/hmm_model_selection_example/">HMM Model Selection</a></li><li><a class="tocitem" href="../tutorials/gaussian_glm_hmm_example/">Gaussian GLM-HMM Example</a></li><li><a class="tocitem" href="../tutorials/hmm_identifiability_example/">HMM Identifiability</a></li><li><a class="tocitem" href="../tutorials/gaussian_mixture_model_example/">Gaussian Mixture Model Example</a></li><li><a class="tocitem" href="../tutorials/poisson_mixture_model_example/">Poisson Mixture Model Example</a></li><li><a class="tocitem" href="../tutorials/Probabilistic_PCA_example/">Probabilistic PCA Example</a></li><li><a class="tocitem" href="../tutorials/switching_linear_dynamical_system_example/">Switching Linear Dynamical System Example</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Models</a></li><li class="is-active"><a href>Mixture Models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Mixture Models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/docs/src/MixtureModels.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="What-is-a-Mixture-Model?"><a class="docs-heading-anchor" href="#What-is-a-Mixture-Model?">What is a Mixture Model?</a><a id="What-is-a-Mixture-Model?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-Mixture-Model?" title="Permalink"></a></h1><p>A <strong>mixture model</strong> is a probabilistic model that represents a population consisting of multiple subpopulations, where each observation is assumed to come from one of several component distributions. Mixture models are particularly useful for modeling data that exhibits multimodal behavior or comes from heterogeneous sources.</p><p>Formally, a mixture model with <span>$K$</span> components is defined as:</p><p class="math-container">\[f_{mix}(x; \Theta, \pi) = \sum_{k=1}^K \pi_k f_k(x; \theta_k)\]</p><p>Where:</p><ul><li><span>$f_k(x; \theta_k)$</span> is the <span>$k$</span>-th <strong>component distribution</strong> with parameters <span>$\theta_k$</span></li><li><span>$\pi_k$</span> is the <strong>mixing coefficient</strong> (or mixing weight) for component <span>$k$</span></li><li><span>$\Theta = \{\theta_1, \ldots, \theta_K\}$</span> are the component parameters</li><li><span>$\pi = \{\pi_1, \ldots, \pi_K\}$</span> are the mixing coefficients with <span>$\sum_{k=1}^K \pi_k = 1$</span> and <span>$\pi_k \geq 0$</span></li></ul><h2 id="Generative-Process"><a class="docs-heading-anchor" href="#Generative-Process">Generative Process</a><a id="Generative-Process-1"></a><a class="docs-heading-anchor-permalink" href="#Generative-Process" title="Permalink"></a></h2><p>The generative process for a mixture model can be described as follows:</p><p class="math-container">\[\begin{align*}
    z_i &amp;\sim \text{Cat}(\pi) \\
    x_i &amp;\mid z_i = k \sim f_k(x; \theta_k)
\end{align*}\]</p><p>Where:</p><ul><li><span>$z_i$</span> is a <strong>latent assignment variable</strong> indicating which component generated observation <span>$x_i$</span></li><li><span>$z_i = k$</span> means observation <span>$x_i$</span> was generated by component <span>$k$</span></li><li><span>$\text{Cat}(\pi)$</span> is the categorical distribution with probabilities <span>$\pi$</span></li></ul><p>This two-step process first selects a component according to the mixing weights, then generates an observation from that component&#39;s distribution.</p><h2 id="Key-Properties"><a class="docs-heading-anchor" href="#Key-Properties">Key Properties</a><a id="Key-Properties-1"></a><a class="docs-heading-anchor-permalink" href="#Key-Properties" title="Permalink"></a></h2><p><strong>Identifiability:</strong> Mixture models are generally identifiable up to permutation of the component labels. This means that swapping component labels and their associated parameters yields an equivalent model.</p><p><strong>Model Selection:</strong> The number of components <span>$K$</span> is typically unknown and must be determined using model selection criteria such as AIC, BIC, or cross-validation.</p><p><strong>Latent Structure:</strong> The latent variables <span>$z_i$</span> provide a natural clustering interpretation, where observations are probabilistically assigned to components.</p><h2 id="Gaussian-Mixture-Model"><a class="docs-heading-anchor" href="#Gaussian-Mixture-Model">Gaussian Mixture Model</a><a id="Gaussian-Mixture-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-Mixture-Model" title="Permalink"></a></h2><p>A <strong>Gaussian Mixture Model (GMM)</strong> is a mixture model where each component is a multivariate Gaussian distribution. GMMs are among the most widely used mixture models due to their mathematical tractability and flexibility in modeling continuous data.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.GaussianMixtureModel" href="#StateSpaceDynamics.GaussianMixtureModel"><code>StateSpaceDynamics.GaussianMixtureModel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GaussianMixtureModel</code></pre><p>A Gaussian Mixture Model for clustering and density estimation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L4-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.GaussianMixtureModel-Tuple{Int64, Int64}" href="#StateSpaceDynamics.GaussianMixtureModel-Tuple{Int64, Int64}"><code>StateSpaceDynamics.GaussianMixtureModel</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GaussianMixtureModel(k::Int, data_dim::Int)</code></pre><p>Constructor for GaussianMixtureModel. Initializes Σₖ&#39;s covariance matrices to the  identity, πₖ to a uniform distribution, and μₖ&#39;s means to zeros.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L16-L21">source</a></section></article><h2 id="Mathematical-Formulation"><a class="docs-heading-anchor" href="#Mathematical-Formulation">Mathematical Formulation</a><a id="Mathematical-Formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Mathematical-Formulation" title="Permalink"></a></h2><p>For a GMM with <span>$K$</span> components in <span>$D$</span> dimensions, the mixture density is:</p><p class="math-container">\[f_{GMM}(x; \Theta, \pi) = \sum_{k=1}^K \pi_k \mathcal{N}(x; \mu_k, \Sigma_k)\]</p><p>Where each component is a multivariate Gaussian:</p><p class="math-container">\[\mathcal{N}(x; \mu_k, \Sigma_k) = \frac{1}{(2\pi)^{D/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\right)\]</p><p>The parameters for each component <span>$k$</span> are:</p><ul><li><span>$\mu_k \in \mathbb{R}^D$</span>: the mean vector</li><li><span>$\Sigma_k \in \mathbb{R}^{D \times D}$</span>: the covariance matrix (positive definite)</li></ul><h2 id="Applications"><a class="docs-heading-anchor" href="#Applications">Applications</a><a id="Applications-1"></a><a class="docs-heading-anchor-permalink" href="#Applications" title="Permalink"></a></h2><p>GMMs are particularly effective for:</p><ul><li><strong>Density estimation</strong> for continuous multimodal data</li><li><strong>Soft clustering</strong> where observations can belong to multiple clusters with different probabilities</li><li><strong>Dimensionality reduction</strong> when combined with factor analysis</li><li><strong>Anomaly detection</strong> by identifying low-probability regions</li></ul><h2 id="Poisson-Mixture-Model"><a class="docs-heading-anchor" href="#Poisson-Mixture-Model">Poisson Mixture Model</a><a id="Poisson-Mixture-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Poisson-Mixture-Model" title="Permalink"></a></h2><p>A <strong>Poisson Mixture Model (PMM)</strong> is designed for modeling count data that exhibits overdispersion or multimodality. Each component follows a Poisson distribution, making it suitable for discrete, non-negative integer observations.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.PoissonMixtureModel" href="#StateSpaceDynamics.PoissonMixtureModel"><code>StateSpaceDynamics.PoissonMixtureModel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PoissonMixtureModel</code></pre><p>A Poisson Mixture Model for clustering and density estimation.</p><p><strong>Fields</strong></p><ul><li><code>k::Int</code>: Number of poisson-distributed clusters.</li><li><code>λₖ::Vector{Float64}</code>: Means of each cluster.</li><li><code>πₖ::Vector{Float64}</code>: Mixing coefficients for each cluster.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L163-L172">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.PoissonMixtureModel-Tuple{Int64}" href="#StateSpaceDynamics.PoissonMixtureModel-Tuple{Int64}"><code>StateSpaceDynamics.PoissonMixtureModel</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PoissonMixtureModel(k::Int)</code></pre><p>Constructor for PoissonMixtureModel. Initializes λₖ&#39;s means to  ones and πₖ to a uniform distribution.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L180-L185">source</a></section></article><h3 id="Mathematical-Formulation-2"><a class="docs-heading-anchor" href="#Mathematical-Formulation-2">Mathematical Formulation</a><a class="docs-heading-anchor-permalink" href="#Mathematical-Formulation-2" title="Permalink"></a></h3><p>For a PMM with <span>$K$</span> components, the mixture density is:</p><p class="math-container">\[f_{PMM}(x; \lambda, \pi) = \sum_{k=1}^K \pi_k \text{Poisson}(x; \lambda_k)\]</p><p>Where each component is a Poisson distribution:</p><p class="math-container">\[\text{Poisson}(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}\]</p><p>The parameter for each component <span>$k$</span> is:</p><ul><li><p class="math-container">\[\lambda_k &gt; 0\]</p>: the rate parameter (both mean and variance of the Poisson distribution)</li></ul><h3 id="Applications-2"><a class="docs-heading-anchor" href="#Applications-2">Applications</a><a class="docs-heading-anchor-permalink" href="#Applications-2" title="Permalink"></a></h3><p>PMMs are commonly used for:</p><ul><li><strong>Count data modeling</strong> with heterogeneous populations</li><li><strong>Modeling overdispersed count data</strong> where variance exceeds the mean</li><li><strong>Analyzing arrival processes</strong> with multiple underlying rates</li><li><strong>Biological applications</strong> such as modeling gene expression counts</li></ul><h2 id="Learning-in-Mixture-Models"><a class="docs-heading-anchor" href="#Learning-in-Mixture-Models">Learning in Mixture Models</a><a id="Learning-in-Mixture-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-in-Mixture-Models" title="Permalink"></a></h2><p><code>StateSpaceDynamics.jl</code> implements the <strong>Expectation-Maximization (EM) algorithm</strong> for parameter estimation in mixture models. EM is an iterative algorithm that finds maximum likelihood estimates in the presence of latent variables.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.fit!-Tuple{GaussianMixtureModel, Matrix{&lt;:Real}}" href="#StateSpaceDynamics.fit!-Tuple{GaussianMixtureModel, Matrix{&lt;:Real}}"><code>StateSpaceDynamics.fit!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fit!(gmm::GaussianMixtureModel, data::AbstractMatrix{&lt;:Real}; &lt;keyword arguments&gt;)</code></pre><p>Fits a Gaussian Mixture Model (GMM) to the given data using the Expectation-Maximization (EM) algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>gmm::GaussianMixtureModel</code>: The Gaussian Mixture Model to be fitted.</li><li><code>data::AbstractMatrix{&lt;:Real}</code>: The dataset on which the model will be fitted, where each row represents a data point.</li><li><code>maxiter::Int=50</code>: The maximum number of iterations for the EM algorithm (default: 50).</li><li><code>tol::Float64=1e-3</code>: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).</li><li><code>initialize_kmeans::Bool=false</code>: If true, initializes the means of the GMM using K-means++ initialization (default: false).</li></ul><p><strong>Returns</strong></p><ul><li><code>class_probabilities</code>: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L124-L138">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.fit!-Tuple{PoissonMixtureModel, Matrix{Int64}}" href="#StateSpaceDynamics.fit!-Tuple{PoissonMixtureModel, Matrix{Int64}}"><code>StateSpaceDynamics.fit!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">fit!(pmm::PoissonMixtureModel, data::AbstractMatrix{&lt;:Integer}; maxiter::Int=50, tol::Float64=1e-3, initialize_kmeans::Bool=false)</code></pre><p>Fits a Poisson Mixture Model (PMM) to the given data using the Expectation-Maximization (EM) algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>pmm::PoissonMixtureModel</code>: The Poisson Mixture Model to be fitted.</li><li><code>data::AbstractMatrix{&lt;:Integer}</code>: The dataset on which the model will be fitted, where each row represents a data point.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>maxiter::Int=50</code>: The maximum number of iterations for the EM algorithm (default: 50).</li><li><code>tol::Float64=1e-3</code>: The tolerance for convergence. The algorithm stops if the change in log-likelihood between iterations is less than this value (default: 1e-3).</li><li><code>initialize_kmeans::Bool=false</code>: If true, initializes the means of the PMM using K-means++ initialization (default: false).</li></ul><p><strong>Returns</strong></p><ul><li><code>class_probabilities</code>: A matrix where each entry (i, k) represents the probability of the i-th data point belonging to the k-th component of the mixture model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L276-L292">source</a></section></article><h3 id="Expectation-Maximization-Algorithm"><a class="docs-heading-anchor" href="#Expectation-Maximization-Algorithm">Expectation-Maximization Algorithm</a><a id="Expectation-Maximization-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Expectation-Maximization-Algorithm" title="Permalink"></a></h3><p>The EM algorithm alternates between two steps:</p><h3 id="Expectation-Step-(E-step)"><a class="docs-heading-anchor" href="#Expectation-Step-(E-step)">Expectation Step (E-step)</a><a id="Expectation-Step-(E-step)-1"></a><a class="docs-heading-anchor-permalink" href="#Expectation-Step-(E-step)" title="Permalink"></a></h3><p>Calculate the <strong>posterior probabilities</strong> (responsibilities) that each observation belongs to each component:</p><p class="math-container">\[\gamma_{ik} = p(z_i = k \mid x_i, \theta^{(t)}) = \frac{\pi_k^{(t)} f_k(x_i; \theta_k^{(t)})}{\sum_{j=1}^K \pi_j^{(t)} f_j(x_i; \theta_j^{(t)})}\]</p><p>Where <span>$\gamma_{ik}$</span> represents the responsibility of component <span>$k$</span> for observation <span>$x_i$</span>.</p><h3 id="Maximization-Step-(M-step)"><a class="docs-heading-anchor" href="#Maximization-Step-(M-step)">Maximization Step (M-step)</a><a id="Maximization-Step-(M-step)-1"></a><a class="docs-heading-anchor-permalink" href="#Maximization-Step-(M-step)" title="Permalink"></a></h3><p>Update the parameters by maximizing the expected complete data log-likelihood:</p><p><strong>Mixing coefficients:</strong></p><p class="math-container">\[\pi_k^{(t+1)} = \frac{1}{N} \sum_{i=1}^N \gamma_{ik}\]</p><p><strong>For Gaussian components:</strong></p><p class="math-container">\[\begin{align*}
\mu_k^{(t+1)} &amp;= \frac{\sum_{i=1}^N \gamma_{ik} x_i}{\sum_{i=1}^N \gamma_{ik}} \\
\Sigma_k^{(t+1)} &amp;= \frac{\sum_{i=1}^N \gamma_{ik} (x_i - \mu_k^{(t+1)})(x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^N \gamma_{ik}}
\end{align*}\]</p><p><strong>For Poisson components:</strong></p><p class="math-container">\[\lambda_k^{(t+1)} = \frac{\sum_{i=1}^N \gamma_{ik} x_i}{\sum_{i=1}^N \gamma_{ik}}\]</p><h2 id="Convergence-and-Initialization"><a class="docs-heading-anchor" href="#Convergence-and-Initialization">Convergence and Initialization</a><a id="Convergence-and-Initialization-1"></a><a class="docs-heading-anchor-permalink" href="#Convergence-and-Initialization" title="Permalink"></a></h2><p>The EM algorithm is guaranteed to converge to a local maximum of the likelihood function. However, the quality of the solution depends heavily on initialization:</p><ul><li><strong>Random initialization:</strong> Parameters are randomly initialized</li><li><strong>K-means initialization:</strong> Use K-means clustering to initialize component means and mixing weights</li><li><strong>Multiple random starts:</strong> Run EM from multiple random initializations and select the best solution</li></ul><h2 id="Model-Evaluation-and-Selection"><a class="docs-heading-anchor" href="#Model-Evaluation-and-Selection">Model Evaluation and Selection</a><a id="Model-Evaluation-and-Selection-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Evaluation-and-Selection" title="Permalink"></a></h2><h3 id="Log-Likelihood"><a class="docs-heading-anchor" href="#Log-Likelihood">Log-Likelihood</a><a id="Log-Likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Log-Likelihood" title="Permalink"></a></h3><p>The log-likelihood of the data under the fitted model provides a measure of model fit:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.loglikelihood-Tuple{GaussianMixtureModel, Matrix{&lt;:Real}}" href="#StateSpaceDynamics.loglikelihood-Tuple{GaussianMixtureModel, Matrix{&lt;:Real}}"><code>StateSpaceDynamics.loglikelihood</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">loglikelihood(gmm::GaussianMixtureModel, data::AbstractMatrix{&lt;:Real})</code></pre><p>Compute the log-likelihood of the data given the Gaussian Mixture Model (GMM). The data matrix should be of shape (# observations, # features).</p><p><strong>Arguments</strong></p><ul><li><code>gmm::GaussianMixtureModel</code>: The Gaussian Mixture Model instance </li><li><code>data::AbstractMatrix{&lt;:Real}</code>: data matrix to calculate the Log-Likelihood </li></ul><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The log-likelihood of the data given the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L99-L110">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StateSpaceDynamics.loglikelihood-Tuple{PoissonMixtureModel, Matrix{Int64}}" href="#StateSpaceDynamics.loglikelihood-Tuple{PoissonMixtureModel, Matrix{Int64}}"><code>StateSpaceDynamics.loglikelihood</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">loglikelihood(pmm::PoissonMixtureModel, data::AbstractMatrix{&lt;:Integer})</code></pre><p>Compute the log-likelihood of the data given the Poisson Mixture Model (PMM). The data matrix should be of shape (# features, # obs).</p><p><strong>Returns</strong></p><ul><li><code>Float64</code>: The log-likelihood of the data given the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/9d28cad11478581b4a361ac41a28decd844376a2/src/MixtureModels.jl#L255-L262">source</a></section></article><p class="math-container">\[\ell(\Theta, \pi) = \sum_{i=1}^N \log \left( \sum_{k=1}^K \pi_k f_k(x_i; \theta_k) \right)\]</p><h2 id="Information-Criteria"><a class="docs-heading-anchor" href="#Information-Criteria">Information Criteria</a><a id="Information-Criteria-1"></a><a class="docs-heading-anchor-permalink" href="#Information-Criteria" title="Permalink"></a></h2><p>For model selection (choosing the optimal number of components <span>$K$</span>):</p><p><strong>Akaike Information Criterion (AIC):</strong></p><p class="math-container">\[\text{AIC} = -2\ell(\hat{\Theta}, \hat{\pi}) + 2p\]</p><p><strong>Bayesian Information Criterion (BIC):</strong></p><p class="math-container">\[\text{BIC} = -2\ell(\hat{\Theta}, \hat{\pi}) + p \log(N)\]</p><p>Where <span>$p$</span> is the number of free parameters in the model.</p><h2 id="Inference-and-Applications"><a class="docs-heading-anchor" href="#Inference-and-Applications">Inference and Applications</a><a id="Inference-and-Applications-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-and-Applications" title="Permalink"></a></h2><h3 id="Posterior-Probabilities"><a class="docs-heading-anchor" href="#Posterior-Probabilities">Posterior Probabilities</a><a id="Posterior-Probabilities-1"></a><a class="docs-heading-anchor-permalink" href="#Posterior-Probabilities" title="Permalink"></a></h3><p>After fitting, the posterior probability that observation <span>$x_i$</span> belongs to component <span>$k$</span> is:</p><p class="math-container">\[p(z_i = k \mid x_i, \hat{\Theta}, \hat{\pi}) = \frac{\hat{\pi}_k f_k(x_i; \hat{\theta}_k)}{\sum_{j=1}^K \hat{\pi}_j f_j(x_i; \hat{\theta}_j)}\]</p><h3 id="Hard-Assignment"><a class="docs-heading-anchor" href="#Hard-Assignment">Hard Assignment</a><a id="Hard-Assignment-1"></a><a class="docs-heading-anchor-permalink" href="#Hard-Assignment" title="Permalink"></a></h3><p>For clustering applications, observations can be assigned to their most likely component:</p><p class="math-container">\[\hat{z}_i = \arg\max_{k} p(z_i = k \mid x_i, \hat{\Theta}, \hat{\pi})\]</p><h2 id="Sampling"><a class="docs-heading-anchor" href="#Sampling">Sampling</a><a id="Sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling" title="Permalink"></a></h2><p>New observations can be generated from the fitted mixture model by:</p><ol><li>Sampling a component <span>$k \sim \text{Cat}(\hat{\pi})$</span></li><li>Sampling from that component: <span>$x \sim f_k(x; \hat{\theta}_k)$</span></li></ol><h2 id="Practical-Considerations"><a class="docs-heading-anchor" href="#Practical-Considerations">Practical Considerations</a><a id="Practical-Considerations-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-Considerations" title="Permalink"></a></h2><h3 id="Computational-Complexity"><a class="docs-heading-anchor" href="#Computational-Complexity">Computational Complexity</a><a id="Computational-Complexity-1"></a><a class="docs-heading-anchor-permalink" href="#Computational-Complexity" title="Permalink"></a></h3><ul><li><strong>Time complexity:</strong> <span>$O(NKD \cdot \text{iterations})$</span> where <span>$N$</span> is the number of observations, <span>$K$</span> is the number of components, and <span>$D$</span> is the dimensionality</li><li><strong>Space complexity:</strong> <span>$O(NK + KD^2)$</span> for storing responsibilities and covariance matrices (GMM)</li></ul><h3 id="Common-Issues"><a class="docs-heading-anchor" href="#Common-Issues">Common Issues</a><a id="Common-Issues-1"></a><a class="docs-heading-anchor-permalink" href="#Common-Issues" title="Permalink"></a></h3><p><strong>Singular covariance matrices:</strong> In GMMs, components may collapse to single points, leading to singular covariance matrices. Regularization techniques include adding a small value to the diagonal.</p><p><strong>Empty components:</strong> Some components may receive very little probability mass during EM iterations. This can be addressed by reinitializing empty components.</p><p><strong>Convergence to local optima:</strong> EM is sensitive to initialization. Multiple random starts and careful initialization strategies are recommended.</p><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><p>For comprehensive mathematical foundations and advanced topics in mixture models, we recommend:</p><ul><li><strong>Pattern Recognition and Machine Learning, Chapter 9</strong> by <strong>Christopher Bishop</strong></li><li><strong>The Elements of Statistical Learning, Chapter 6</strong> by <strong>Hastie, Tibshirani, and Friedman</strong></li><li><strong>Finite Mixture Models</strong> by <strong>Geoffrey McLachlan and David Peel</strong></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../EmissionModels/">« Emission Models</a><a class="docs-footer-nextpage" href="../tutorials/gaussian_latent_dynamics_example/">Gaussian LDS Example »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Friday 19 September 2025 22:36">Friday 19 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
