# # Simulating and Fitting a Poisson Linear Dynamical System

# This tutorial demonstrates how to use `StateSpaceDynamics.jl` to simulate and fit a
# Linear Dynamical System (LDS) with Poisson observations using the Laplace-EM algorithm.
# Unlike the standard Gaussian LDS, this model is designed for count data (e.g., neural
# spike counts, customer arrivals, or discrete event data) where observations are
# non-negative integers following Poisson distributions.

# The key insight is that while latent dynamics remain continuous and Gaussian, the
# observations are discrete counts whose rates depend on the latent state through
# an exponential link function: $\lambda_i(t) = \exp(\mathbf{C}_i^T \mathbf{x}_t + d_i)$.

# ## Load Required Packages

using StateSpaceDynamics
using LinearAlgebra
using Random
using Plots
using LaTeXStrings
using StableRNGs

# Set up reproducible random number generation
rng = StableRNG(1234);

# ## Create a Poisson Linear Dynamical System

# We define a system where continuous latent dynamics generate discrete count observations.
# This is particularly relevant in neuroscience (neural spike trains) and other domains
# where discrete events are generated by underlying continuous processes.

obs_dim = 10       # Number of observed count variables (e.g., neurons)
latent_dim = 2;    # Number of latent state dimensions

# Define latent dynamics: same spiral structure as Gaussian LDS
# Latent states evolve smoothly according to linear dynamics
A = 0.95 * [cos(0.25) -sin(0.25); sin(0.25) cos(0.25)]  # Rotation with contraction
Q = Matrix(0.1 * I(latent_dim))     # Process noise covariance
x0 = zeros(latent_dim)              # Initial state mean
P0 = Matrix(0.1 * I(latent_dim));   # Initial state covariance

# Poisson observation model parameters:
# For Poisson observations, the rate parameter $\lambda_i$ is modeled as:
# $$\log(\lambda_i) = \mathbf{C}_i^T \mathbf{x}_t + d_i$$
# where $\mathbf{C}$ maps latent states to log-rates and $d_i$ provides baseline log-rates

log_d = log.(fill(0.1, obs_dim));    # Log baseline rates (small positive rates)

# Observation matrix C: maps 2D latent states to log-rates for each observed dimension
# Use positive values so latent activity increases firing rates
C = permutedims([abs.(randn(rng, obs_dim))'; abs.(randn(rng, obs_dim))']);

# Construct the model components
state_model = GaussianStateModel(; A, Q, x0, P0)          # Gaussian latent dynamics
obs_model = PoissonObservationModel(; C, log_d);           # Poisson observations

# Create the complete Poisson Linear Dynamical System
true_plds = LinearDynamicalSystem(;
    state_model=state_model,
    obs_model=obs_model,
    latent_dim=latent_dim,
    obs_dim=obs_dim,
    fit_bool=fill(true, 6)  # Learn all parameters: A, Q, C, log_d, x0, P0
);

# ## Simulate Latent States and Count Observations

# Generate synthetic data from our Poisson LDS. Latent states evolve according
# to linear dynamics, while observations are drawn from Poisson distributions
# whose rates depend exponentially on the current latent state.

tSteps = 500

print("Simulating $tSteps time steps...")

# Generate both latent trajectories and count observations
latents, observations = rand(rng, true_plds; tsteps=tSteps, ntrials=1);

print("Latent range: [$(round(minimum(latents), digits=2)), $(round(maximum(latents), digits=2))]\n")
print("Count range: [$(minimum(observations)), $(maximum(observations))]\n");

# ## Visualize Latent Dynamics

# Show the underlying continuous dynamics that drive discrete observations.
# This vector field illustrates how latent states evolve deterministically (ignoring noise).

# Create grid for vector field
x = y = -3:0.5:3
X = repeat(x', length(y), 1)
Y = repeat(y, 1, length(x))
U = zeros(size(X))  # Flow in x-direction
V = zeros(size(Y))  # Flow in y-direction

for i in 1:size(X, 1), j in 1:size(X, 2)
    v = A * [X[i,j], Y[i,j]]
    U[i,j] = v[1] - X[i,j] 
    V[i,j] = v[2] - Y[i,j] 
end

magnitude = @. sqrt(U^2 + V^2)  # Normalize arrow lengths for cleaner visualization
U_norm = U ./ magnitude
V_norm = V ./ magnitude

# Plot vector field with simulated trajectory
p1 = quiver(X, Y, quiver=(U_norm, V_norm), color=:blue, alpha=0.3,
           linewidth=1, arrow=arrow(:closed, :head, 0.1, 0.1))
plot!(latents[1, :, 1], latents[2, :, 1], xlabel=L"x_1", ylabel=L"x_2",
      color=:black, linewidth=1.5, title="Latent Dynamics", legend=false)

# ## Visualize Latent States and Spike Observations

# Create visualizations highlighting the contrast between continuous latent
# dynamics and discrete count observations (spike trains).

states = latents[:, :, 1]
emissions = observations[:, :, 1]

# Two-panel layout: continuous latent states above, discrete spike rasters below
lim_states = maximum(abs.(states))

p2 = plot(size=(800, 600), layout=@layout[a{0.3h}; b])

for d in 1:latent_dim
    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black,
          linewidth=2, label="", subplot=1) # Plot smooth latent state trajectories

end

plot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L"x_%$d" for d in 1:latent_dim]),
      xticks=[], xlims=(0, tSteps), title="Simulated Latent States",
      yformatter=y->"", tickfontsize=12)

# Plot discrete observations as spike rasters
# Each row represents one observed dimension, spikes shown as vertical lines
colors = palette(:default, obs_dim)
for f in 1:obs_dim
    spike_times = findall(x -> x > 0, emissions[f, :])
    for t in spike_times
        plot!([t, t], [f-0.4, f+0.4], color=colors[f], linewidth=1, label="", subplot=2)
    end
end

plot!(subplot=2, yticks=(1:obs_dim, [L"y_{%$d}" for d in 1:obs_dim]),
      xlims=(0, tSteps), ylims=(0.5, obs_dim + 0.5), title="Spike Raster Plot",
      xlabel="Time", tickfontsize=12, grid=false)

# ## Initialize and Fit Poisson LDS

# In practice, we only observe spike counts, not latent states. Our goal is to infer
# both latent dynamics and the mapping from latent states to firing rates.
# Start with randomly initialized model.

# Random initialization (simulating lack of prior knowledge)
A_init = random_rotation_matrix(latent_dim, rng)  # Random rotation matrix
Q_init = Matrix(0.1 * I(latent_dim))              # Process noise guess
C_init = randn(rng, obs_dim, latent_dim)          # Random observation mapping
log_d_init = log.(fill(0.1, obs_dim))             # Baseline log-rate guess
x0_init = zeros(latent_dim)                       # Start from origin
P0_init = Matrix(0.1 * I(latent_dim))             # Initial uncertainty

# Construct naive model
sm_init = GaussianStateModel(; A=A_init, Q=Q_init, x0=x0_init, P0=P0_init)
om_init = PoissonObservationModel(; C=C_init, log_d=log_d_init)

naive_plds = LinearDynamicalSystem(;
    state_model=sm_init,
    obs_model=om_init,
    latent_dim=latent_dim,
    obs_dim=obs_dim,
    fit_bool=fill(true, 6)
);

# Perform initial smoothing with random parameters
print("Initial smoothing with random parameters...")

# For Poisson observations, this requires Laplace approximations since
# the posterior is no longer Gaussian (unlike linear-Gaussian case)
smoothed_x_pre, smoothed_p_pre = smooth(naive_plds, observations);

# Compare true vs. initial estimated latent states
p3 = plot()
for d in 1:latent_dim
    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black, 
          linewidth=2, label=(d==1 ? "True" : ""), alpha=0.8)
    plot!(1:tSteps, smoothed_x_pre[d, :, 1] .+ lim_states * (d-1), color=:red, 
          linewidth=2, label=(d==1 ? "Initial Est." : ""), alpha=0.8)
end

plot!(yticks=(lim_states .* (0:latent_dim-1), [L"x_%$d" for d in 1:latent_dim]),
      xlabel="Time", xlims=(0, tSteps), title="Pre-EM: True vs. Initial Estimates",
      yformatter=y->"", tickfontsize=12, legend=:topright)

# ## Fit Using Laplace-EM Algorithm

# Use Laplace-EM to learn parameters. This is more complex than standard EM because:
# 1. E-step requires Laplace approximations for non-Gaussian posteriors
# 2. M-step updates account for Poisson likelihood structure  
# 3. Convergence can be slower due to non-conjugate nature

print("Starting Laplace-EM algorithm...")
print("Note: Poisson LDS fitting is computationally intensive\n")

# Fit the model - using fewer iterations due to computational cost
elbo, _ = fit!(naive_plds, observations; max_iter=25, tol=1e-6);

print("Laplace-EM completed in $(length(elbo)) iterations\n")

# Perform smoothing with learned parameters
smoothed_x_post, smoothed_p_post = smooth(naive_plds, observations);

# Compare true vs. learned latent state estimates
p4 = plot()
for d in 1:latent_dim
    plot!(1:tSteps, states[d, :] .+ lim_states * (d-1), color=:black, 
          linewidth=2, label=(d==1 ? "True" : ""), alpha=0.8)
    plot!(1:tSteps, smoothed_x_post[d, :, 1] .+ lim_states * (d-1), color=:red, 
          linewidth=2, label=(d==1 ? "Post-EM Est." : ""), alpha=0.8)
end

plot!(yticks=(lim_states .* (0:latent_dim-1), [L"x_%$d" for d in 1:latent_dim]),
      xlabel="Time", xlims=(0, tSteps), title="Post-EM: True vs. Learned Estimates",
      yformatter=y->"", tickfontsize=12, legend=:topright)

# ## Monitor ELBO Convergence

# The Evidence Lower Bound (ELBO) tracks algorithm progress. For Poisson LDS,
# ELBO includes both data likelihood and Laplace approximation terms.
# Convergence may be less smooth than Gaussian case due to approximations.

p5 = plot(elbo, xlabel="Iteration", ylabel="ELBO",  
          title="Laplace-EM Convergence", legend=false,
          linewidth=2, marker=:circle, markersize=3, color=:darkgreen)

# Add convergence annotation
if length(elbo) > 1
    improvement = elbo[end] - elbo[1]
    annotate!(p5, length(elbo)*0.7, elbo[end]*0.95, 
        text("Improvement: $(round(improvement, digits=1))", 10))
end

# ## Parameter Recovery Assessment

print("\n=== Parameter Recovery Assessment ===\n")

# Compare key learned parameters with ground truth
print("Dynamics Matrix A Recovery:\n")
A_error = norm(A - naive_plds.state_model.A) / norm(A)
print("Relative error: $(round(A_error, digits=3))\n")

print("Observation Matrix C Recovery:\n") 
C_error = norm(C - naive_plds.obs_model.C) / norm(C)
print("Relative error: $(round(C_error, digits=3))\n")

print("Process Noise Q Recovery:\n")
Q_error = norm(Q - naive_plds.state_model.Q) / norm(Q)
print("Relative error: $(round(Q_error, digits=3))\n");

# ## Computational Complexity Notes

print("\n=== Computational Considerations ===\n")
print("Poisson LDS fitting challenges:\n")
print("• Non-conjugate Poisson-Gaussian combination requires approximations\n")
print("• Laplace approximation adds computational overhead per E-step\n") 
print("• Optimization landscape can be more complex than Gaussian case\n")
print("• Convergence typically slower than standard EM\n")
print("\nBenefits:\n")
print("• Principled handling of count/spike data\n")
print("• Maintains interpretable continuous latent dynamics\n")
print("• Extends LDS framework to discrete observations\n");

# ## Summary
#
# This tutorial demonstrated fitting a Poisson Linear Dynamical System:
# 
# **Key Concepts:**
# - **Hybrid model**: Continuous Gaussian latent dynamics generate discrete Poisson observations
# - **Exponential link**: $\log(\lambda_i) = \mathbf{C}_i^T \mathbf{x}_t + d_i$ connects latent states to count rates
# - **Laplace-EM**: Handles non-conjugate Poisson-Gaussian combination through approximations
# - **Count data modeling**: Extends LDS framework to spike trains and event sequences
#
# **Applications:**
# - Neural spike train analysis and decoding
# - Customer arrival modeling and forecasting  
# - Event sequence analysis in discrete-time systems
# - Any scenario with latent continuous dynamics generating discrete observations
#
# **Technical Insights:**
# - More computationally intensive than Gaussian LDS due to required approximations
# - Convergence can be slower and less smooth than conjugate models
# - Parameter recovery quality depends on observation density and latent state separation
# - Laplace approximations become more accurate with higher count rates
#
# **Advantages:**
# - Principled probabilistic framework for count data
# - Maintains interpretable continuous latent dynamics
# - Enables simultaneous state estimation and parameter learning
# - Provides uncertainty quantification for both states and parameters
#
# The Poisson LDS successfully bridges continuous dynamical systems and discrete observation
# models, enabling principled analysis of count data with underlying temporal structure.