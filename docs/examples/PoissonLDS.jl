# # Simulating and Fitting a Poisson Linear Dynamical System

# This tutorial demonstrates how to use `StateSpaceDynamics.jl` to simulate and fit a
# Linear Dynamical System (LDS) with Poisson observations using the Laplace-EM algorithm.
# Unlike the standard Gaussian LDS, this model is designed for count data (e.g., neural
# spike counts, customer arrivals, or any discrete event data) where observations are
# non-negative integers that follow Poisson distributions.

# ## Load Required Packages

# We begin by loading the necessary packages. The core difference from the Gaussian
# case is that we'll be working with Poisson observation models, which require more
# sophisticated inference algorithms (Laplace approximations) due to the non-conjugate
# nature of Poisson likelihoods with Gaussian latent states.

using StateSpaceDynamics
using LinearAlgebra
using Random
using Plots
using LaTeXStrings
using StableRNGs

# Set up reproducible random number generation
rng = StableRNG(123);

# ## Create a Poisson Linear Dynamical System

# We define a system where continuous latent dynamics generate discrete count observations.
# This is particularly relevant in neuroscience (modeling neural spike trains) and other
# domains where we observe discrete events generated by underlying continuous processes.

obs_dim = 10      # Number of observed count variables (e.g., neurons)
latent_dim = 2    # Number of latent state dimensions

# Define the latent dynamics: same spiral structure as the Gaussian case
# The latent states evolve smoothly and continuously according to linear dynamics
A = 0.95 * [cos(0.25) -sin(0.25); sin(0.25) cos(0.25)]  # Rotation with contraction
Q = Matrix(0.1 * I(latent_dim))     # Process noise covariance
x0 = zeros(latent_dim)              # Initial state mean
P0 = Matrix(0.1 * I(latent_dim))    # Initial state covariance

# Poisson observation model parameters:
# For Poisson observations, the rate parameter λ is typically modeled as:
# log(λ_i) = C_i^T * x_t + log_d_i
# where C maps latent states to log-rates and log_d provides baseline log-rates

log_d = log.(fill(0.1, obs_dim))    # Log baseline rates (small positive rates)

# Observation matrix C: maps 2D latent states to log-rates for each observed dimension
# We use positive values to ensure that latent activity increases firing rates
C = permutedims([abs.(randn(rng, obs_dim))'; abs.(randn(rng, obs_dim))'])

# Construct the model components
state_model = GaussianStateModel(; A, Q, x0, P0)          # Gaussian latent dynamics
obs_model = PoissonObservationModel(; C, log_d)           # Poisson observations

# Create the complete Poisson Linear Dynamical System
true_plds = LinearDynamicalSystem(;
    state_model=state_model,
    obs_model=obs_model,
    latent_dim=latent_dim,
    obs_dim=obs_dim,
    fit_bool=fill(true, 6)  # Learn all parameters: A, Q, C, log_d, x0, P0
)

# ## Simulate Latent States and Observations

# Generate synthetic data from our Poisson LDS. The latent states evolve according
# to the linear dynamics, while observations are drawn from Poisson distributions
# whose rates depend on the current latent state.

tSteps = 500
println("Simulating $tSteps time steps of Poisson LDS data...")

# Generate both latent trajectories and count observations
latents, observations = rand(rng, true_plds; tsteps=tSteps, ntrials=1)

println("Generated latent states with range: [$(minimum(latents)), $(maximum(latents))]")
println("Generated count observations with range: [$(minimum(observations)), $(maximum(observations))]")

# ## Plot Vector Field of Latent Dynamics

# Visualize the underlying continuous dynamics that drive the discrete observations.
# This vector field shows how latent states evolve deterministically (ignoring noise).

# Create a grid of starting points in latent space
x = y = -3:0.5:3
X = repeat(x', length(y), 1)
Y = repeat(y, 1, length(x))
U = zeros(size(X))  # Flow in x-direction
V = zeros(size(Y))  # Flow in y-direction

# Compute the deterministic flow at each grid point
for i in 1:size(X, 1), j in 1:size(X, 2)
    v = A * [X[i,j], Y[i,j]]
    U[i,j] = v[1] - X[i,j] 
    V[i,j] = v[2] - Y[i,j] 
end

# Normalize arrow lengths for cleaner visualization
magnitude = @. sqrt(U^2 + V^2)
U_norm = U ./ magnitude
V_norm = V ./ magnitude

# Plot the vector field with the actual simulated trajectory
p = quiver(X, Y, quiver=(U_norm, V_norm), color=:blue, alpha=0.3,
           linewidth=1, arrow=arrow(:closed, :head, 0.1, 0.1))
plot!(latents[1, :, 1], latents[2, :, 1], xlabel="x₁", ylabel="x₂",
      color=:black, linewidth=1.5, title="Latent Dynamics", legend=false)

# ## Plot Latent States and Observations

# Create visualizations that highlight the key difference between continuous latent
# dynamics and discrete count observations. The latent states are smooth curves,
# while observations are spike trains (discrete events over time).

states = latents[:, :, 1]
emissions = observations[:, :, 1]
time_bins = size(states, 2)

# Two-panel layout: latent states above, spike rasters below
plot(size=(800, 600), layout=@layout[a{0.3h}; b])

# Plot smooth latent state trajectories
lim_states = maximum(abs.(states))
for d in 1:latent_dim
    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black,
          linewidth=2, label="", subplot=1)
end

plot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L"x_%$d" for d in 1:latent_dim]),
      xticks=[], xlims=(0, time_bins), title="Simulated Latent States",
      yformatter=y->"", tickfontsize=12)

# Plot discrete observations as spike rasters
# Each row represents one observed dimension, spikes shown as vertical lines
colors = palette(:default, obs_dim)
for f in 1:obs_dim
    spike_times = findall(x -> x > 0, emissions[f, :])
    for t in spike_times
        plot!([t, t], [f-0.4, f+0.4], color=colors[f], linewidth=1, label="", subplot=2)
    end
end

plot!(subplot=2, yticks=(1:obs_dim, [L"y_{%$d}" for d in 1:obs_dim]),
      xlims=(0, time_bins), ylims=(0.5, obs_dim + 0.5), title="Simulated Emissions (Spike Raster)",
      xlabel="Time", tickfontsize=12, grid=false)

println("Total spike count across all dimensions: $(sum(emissions))")
println("Average firing rate: $(sum(emissions)/(obs_dim * time_bins)) spikes per time bin")

# ## Initialize Model and Perform Initial Smoothing

# In practice, we only observe the spike counts, not the latent states. Our goal
# is to infer both the latent dynamics and the mapping from latent states to
# observed firing rates. We start with a randomly initialized model.

# Random initialization (simulating lack of prior knowledge)
A_init = random_rotation_matrix(latent_dim, rng)  # Random rotation matrix
Q_init = Matrix(0.1 * I(latent_dim))              # Process noise guess
C_init = randn(rng, obs_dim, latent_dim)          # Random observation mapping
log_d_init = log.(fill(0.1, obs_dim))             # Baseline log-rate guess
x0_init = zeros(latent_dim)                       # Start from origin
P0_init = Matrix(0.1 * I(latent_dim))             # Initial uncertainty

# Construct the naive model
sm_init = GaussianStateModel(; A=A_init, Q=Q_init, x0=x0_init, P0=P0_init)
om_init = PoissonObservationModel(; C=C_init, log_d=log_d_init)

naive_plds = LinearDynamicalSystem(;
    state_model=sm_init,
    obs_model=om_init,
    latent_dim=latent_dim,
    obs_dim=obs_dim,
    fit_bool=fill(true, 6)
)

# Perform smoothing with the randomly initialized model
# For Poisson observations, this requires Laplace approximations since the
# posterior is no longer Gaussian (unlike the linear-Gaussian case)
println("Performing initial smoothing with random parameters...")
smoothed_x, smoothed_p = smooth(naive_plds, observations)

# Compare true vs. initial estimated latent states
plot()
for d in 1:latent_dim
    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label="", subplot=1)
    plot!(1:time_bins, smoothed_x[d, :, 1] .+ lim_states * (d-1), color=:red, linewidth=2, label="", subplot=1)
end

plot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L"x_%$d" for d in 1:latent_dim]),
      xticks=[], xlims=(0, time_bins), title="True vs. Predicted Latent States (Pre-EM)",
      yformatter=y->"", tickfontsize=12)

# ## Fit the Poisson LDS Using Laplace-EM

# Now we use the Laplace-EM algorithm to learn the parameters. This is more complex
# than standard EM because:
# 1. The E-step requires Laplace approximations to handle non-Gaussian posteriors
# 2. The M-step updates must account for the Poisson likelihood structure
# 3. Convergence can be slower due to the non-conjugate nature of the model

println("Starting Laplace-EM algorithm...")
println("Note: Poisson LDS fitting is more computationally intensive than Gaussian LDS")

# Fit the model - using fewer iterations than Gaussian case due to computational cost
elbo, _ = fit!(naive_plds, observations; max_iter=25, tol=1e-6)

println("Laplace-EM completed after $(length(elbo)) iterations")

# Perform smoothing with the learned parameters
smoothed_x, smoothed_p = smooth(naive_plds, observations)

# Compare true vs. learned latent state estimates
plot()
for d in 1:latent_dim
    plot!(1:time_bins, states[d, :] .+ lim_states * (d-1), color=:black, linewidth=2, label="", subplot=1)
    plot!(1:time_bins, smoothed_x[d, :, 1] .+ lim_states * (d-1), color=:red, linewidth=2, label="", subplot=1)
end

plot!(subplot=1, yticks=(lim_states .* (0:latent_dim-1), [L"x_%$d" for d in 1:latent_dim]),
      xticks=[], xlims=(0, time_bins), title="True vs. Predicted Latent States (Post-EM)",
      yformatter=y->"", tickfontsize=12)

# ## Monitor ELBO Convergence

# The Evidence Lower BOund (ELBO) tracks the algorithm's progress. For Poisson LDS,
# the ELBO includes both the data likelihood and the Laplace approximation terms.
# Convergence may be less smooth than in the Gaussian case due to the approximations.

plot(elbo, xlabel="iteration", ylabel="ELBO", title="ELBO over Iterations", legend=false)

println("Initial ELBO: $(elbo[1])")
println("Final ELBO: $(elbo[end])")
println("ELBO improvement: $(elbo[end] - elbo[1])")

# ## Model Comparison and Validation

# Let's examine how well we recovered the true parameters
println("\n=== Parameter Recovery Assessment ===")

# Compare true vs learned observation matrix C
C_error = norm(true_plds.obs_model.C - naive_plds.obs_model.C) / norm(true_plds.obs_model.C)
println("Relative error in observation matrix C: $(round(C_error, digits=3))")

# Compare true vs learned dynamics matrix A  
A_error = norm(true_plds.state_model.A - naive_plds.state_model.A) / norm(true_plds.state_model.A)
println("Relative error in dynamics matrix A: $(round(A_error, digits=3))")

# Compare latent state trajectories
state_error = norm(states - smoothed_x[:,:,1]) / norm(states)
println("Relative error in latent state estimation: $(round(state_error, digits=3))")

# ## Summary
#
# This tutorial demonstrated fitting a Poisson Linear Dynamical System:
# 
# 1. **Model Structure**: Continuous Gaussian latent dynamics generate discrete Poisson observations
# 2. **Applications**: Ideal for count data like neural spikes, customer arrivals, or event sequences  
# 3. **Algorithm**: Laplace-EM handles the non-conjugate Poisson-Gaussian combination
# 4. **Challenges**: More computationally intensive than Gaussian LDS due to required approximations
# 5. **Results**: Successfully recovered both latent dynamics and observation parameters from spike data
#
# The Poisson LDS extends the linear dynamical system framework to discrete observation models,
# enabling state-space modeling of count data while maintaining interpretable latent dynamics.