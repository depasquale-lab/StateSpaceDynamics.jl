@article{Dalle2024, 
  doi = {10.21105/joss.06436}, 
  url = {https://doi.org/10.21105/joss.06436}, 
  year = {2024}, 
  publisher = {The Open Journal}, 
  volume = {9}, 
  number = {96}, 
  pages = {6436}, 
  author = {Guillaume Dalle}, 
  title = {HiddenMarkovModels.jl: generic, fast and reliable state space modeling}, 
  journal = {Journal of Open Source Software}
}

@software{PySSM2022,
  title = {SSM: Bayesian learning and inference for state space models},
  author = {Linderman, Scott},
  date = {2022},
  url = {https://github.com/lindermanlab/ssm},
  abstract = {SSM is a python library for fitting state space models},
  keywords = {SSM}
}

@software{Turing2018,
  title = {Turing.jl: Bayesian inference with probabilistic programming},
  author = {{Ge, H.}, {Xu, K.}, {Ghahramani, Z.}},
  date = {2018},
  url = {https://github.com/TuringLang/Turing.jl},
  abstract = {Bayesian inference with probabilistic programming},
  keywords = {Turing}
}

@software{HMMjl2024,
  title = {HiddenMarkovModels.jl},
  author = {Dalle, Guillaume},
  date = {2024},
  url = {https://github.com/gdalle/HiddenMarkovModels.jl},
  abstract = {A Julia package for simulation, inference and learning of Hidden Markov Models},
  keywords = {HMM}
}

@software{SSMjl2024,
  title = {StateSpaceModels.jl},
  author = {{Bodin, Guilherme}, {Saavedra, Raphael}},
  date = {2024},
  url = {https://github.com/LAMPSPUC/StateSpaceModels.jl},
  abstract = {StateSpaceModels.jl is a package for modeling, forecasting, 
    and simulating time series in a state-space framework. Implementations were made based on the book 
    "Time Series Analysis by State Space Methods" (2012) by James Durbin and Siem Jan Koopman. 
    The notation of the variables in the code also follows the book.},
  keywords = {SSM}
}

@software{SSLjl2024,
  title = {StateSpaceLearning.jl},
  author = {Ramos, Andre},
  date = {2024},
  url = {https://github.com/LAMPSPUC/StateSpaceLearning.jl},
  abstract = {StateSpaceLearning.jl is a package for modeling and forecasting time series in a high-dimension regression framework},
  keywords = {SSM}
}

@article{NIPS2011_7143d7fb,
 author = {Macke, Jakob H and Buesing, Lars and Cunningham, John P and Yu, Byron M and Shenoy, Krishna V and Sahani, Maneesh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Empirical models of spiking in neural populations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},
 volume = {24},
 year = {2011}
}

@article{Linderman_Dynamax_A_Python_2025,
author = {Linderman, Scott W. and Chang, Peter and Harper-Donnelly, Giles and Kara, Aleyna and Li, Xinglong and Duran-Martin, Gerardo and Murphy, Kevin},
doi = {10.21105/joss.07069},
journal = {Journal of Open Source Software},
month = apr,
number = {108},
pages = {7069},
title = {{Dynamax: A Python package for probabilistic state space modeling with JAX}},
url = {https://joss.theoj.org/papers/10.21105/joss.07069},
volume = {10},
year = {2025}}

@article{Paninski2010-ns,
  title        = {A new look at state-space models for neural data},
  author       = {Paninski, Liam and Ahmadian, Yashar and Ferreira, Daniel Gil
                  and Koyama, Shinsuke and Rahnama Rad, Kamiar and Vidne,
                  Michael and Vogelstein, Joshua and Wu, Wei},
  doi          = {10.1007/s10827-009-0179-x},
  journaltitle = {J. Comput. Neurosci.},
  publisher    = {Springer Science and Business Media LLC},
  volume       = {29},
  issue        = {1-2},
  pages        = {107--126},
  date         = {2010-08},
  abstract     = {State space methods have proven indispensable in neural data
                  analysis. However, common methods for performing inference in
                  state-space models with non-Gaussian observations rely on
                  certain approximations which are not always accurate. Here we
                  review direct optimization methods that avoid these
                  approximations, but that nonetheless retain the computational
                  efficiency of the approximate methods. We discuss a variety of
                  examples, applying these direct optimization techniques to
                  problems in spike train smoothing, stimulus decoding,
                  parameter estimation, and inference of synaptic properties.
                  Along the way, we point out connections to some related
                  standard statistical methods, including spline smoothing and
                  isotonic regression. Finally, we note that the computational
                  methods reviewed here do not in fact depend on the state-space
                  setting at all; instead, the key property we are exploiting
                  involves the bandedness of certain matrices. We close by
                  discussing some applications of this more general point of
                  view, including Markov chain Monte Carlo methods for neural
                  decoding and efficient estimation of spatially-varying firing
                  rates.},
  urldate      = {2024-09-11},
  language     = {en}
}

@software{SSDjl2024,
  title = {StateSpaceDynamics.jl: A Julia package for probabilistic
state space models},
  author = {Senne, Ryan and Loschinskey, Zachary and Loughridge, Carson  and DePasquale, Brian},
  date = {2025},
  howpublished = {\url{https://github.com/depasquale-lab/StateSpaceDynamics.jl}},
  url = {https://github.com/depasquale-lab/StateSpaceDynamics.jl},
  abstract = {A Julia package for probabilistic state space models},
  keywords = {SSM}
}

@article{bezanson2017julia,
  title={Julia: A fresh approach to numerical computing},
  author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  journal={SIAM review},
  volume={59},
  number={1},
  pages={65--98},
  year={2017},
  doi={10.48550/arXiv.1411.1607},
  publisher={SIAM}
}

@ARTICLE{Linderman2016-xe,
  title        = {Recurrent switching linear dynamical systems},
  author       = {Linderman, Scott W and Miller, Andrew C and Adams, Ryan P and
                  Blei, David M and Paninski, Liam and Johnson, Matthew J},
  journaltitle = {arXiv [stat.ML]},
  date         = {2016-10-26},
  eprintclass  = {stat.ML},
  doi          = {10.48550/arXiv.1610.08466},
  abstract     = {Many natural systems, such as neurons firing in the brain or
                  basketball teams traversing a court, give rise to time series
                  data with complex, nonlinear dynamics. We can gain insight
                  into these systems by decomposing the data into segments that
                  are each explained by simpler dynamic units. Building on
                  switching linear dynamical systems (SLDS), we present a new
                  model class that not only discovers these dynamical units, but
                  also explains how their switching behavior depends on
                  observations or continuous latent states. These "recurrent"
                  switching linear dynamical systems provide further insight by
                  discovering the conditions under which each unit is deployed,
                  something that traditional SLDS models fail to do. We leverage
                  recent algorithmic advances in approximate inference to make
                  Bayesian inference in these models easy, fast, and scalable.}
}

@REPORT{Murphy1998-bk,
  type   = {techreport},
  title  = {Switching Kalman Filters},
  author = {Murphy, Kevin P},
  date   = {1998}
}

@MISC{Rybicki1990-ky,
  title  = {Fast Solution for the Diagonal Elements of the Inverse of a
            Tridiagonal Matrix},
  author = {Rybicki, G B and Hummer, D G},
  date   = {1990}
}

@BOOK{Bishop2006-kv,
  title     = {Pattern Recognition and Machine Learning},
  author    = {Bishop, Christopher M},
  publisher = {Springer},
  location  = {New York, NY},
  date      = {2006-11-01},
  pagetotal = {738},
  series    = {Information Science and Statistics},
  language  = {en}
}

@article{slds,
    author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
    title = {Variational Learning for Switching State-Space Models},
    journal = {Neural Computation},
    volume = {12},
    number = {4},
    pages = {831-864},
    year = {2000},
    month = {04},
    abstract = {We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models—hidden Markov models and linear dynamical systems—and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, \&amp; Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
    issn = {0899-7667},
    doi = {10.1162/089976600300015619},
    url = {https://doi.org/10.1162/089976600300015619},
    eprint = {https://direct.mit.edu/neco/article-pdf/12/4/831/814443/089976600300015619.pdf},
}

@inproceedings{NIPS1994_8065d07d,
 author = {Bengio, Yoshua and Frasconi, Paolo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 pages = {},
 publisher = {MIT Press},
 title = {An Input Output HMM Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},
 volume = {7},
 year = {1994}
}

@article{Ashwood2022,
  title     = {Mice alternate between discrete strategies during perceptual decision-making},
  author    = {Ashwood, Zoe C. and Roy, Nicholas A. and Stone, Iris R. and Urai, Anne E. and Churchland, Anne K. and Pouget, Alexandre and Pillow, Jonathan W.},
  journal   = {Nature Neuroscience},
  volume    = {25},
  number    = {2},
  pages     = {201--212},
  year      = {2022},
  doi       = {10.1038/s41593-021-01007-z},
  url       = {https://www.nature.com/articles/s41593-021-01007-z}
}

@software{pythoncall,
  title    = {PythonCall.jl},
  author   = {Christopher Doris},
  date     = {2021},
  howpublished = {\url{https://github.com/JuliaPy/PythonCall.jl}}
}

@software{bench,
  title    = {BenchmarkTools.jl},
  author   = {Jarrett Revels},
  date     = {2017},
  howpublished = {\url{https://github.com/JuliaCI/BenchmarkTools.jl}}
}

@article{ForwardDiff,
    title = {Forward-Mode Automatic Differentiation in {J}ulia},
   author = {{Revels}, J. and {Lubin}, M. and {Papamarkou}, T.},
  journal = {arXiv:1607.07892 [cs.MS]},
     year = {2016},
      url = {https://arxiv.org/abs/1607.07892}
}

@article{Zygote,
  author       = {Michael Innes},
  title        = {Don't Unroll Adjoint: Differentiating SSA-Form Programs},
  journal      = {CoRR},
  volume       = {abs/1810.07951},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.07951},
  eprinttype    = {arXiv},
  eprint       = {1810.07951},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-07951.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@software{DiffInterface,
      author={Dalle, Guillaume and Hill, Adrian},
      title={Differentiation{I}nterface.jl},
      year={2024},
      publisher={Zenodo},
      doi={10.5281/zenodo.11092033},
      url={https://doi.org/10.5281/zenodo.11092033},
}