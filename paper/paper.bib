@software{PySSM2022,
  title = {SSM: Bayesian learning and inference for state space models},
  author = {Linderman, Scott},
  date = {2022},
  url = {https://github.com/lindermanlab/ssm},
  abstract = {SSM is a python library for fitting state space models},
  keywords = {SSM}
}

@article{10.1145/3711897,
author = {Fjelde, Tor Erlend and Xu, Kai and Widmann, David and Tarek, Mohamed and Pfiffer, Cameron and Trapp, Martin and Axen, Seth D. and Sun, Xianda and Hauru, Markus and Yong, Penelope and Tebbutt, Will and Ghahramani, Zoubin and Ge, Hong},
title = {Turing.jl: a general-purpose probabilistic programming language},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711897},
doi = {10.1145/3711897},
note = {Just Accepted},
journal = {ACM Trans. Probab. Mach. Learn.},
month = feb,
}

@InProceedings{pmlr-v84-ge18b,
  title = 	 {Turing: A Language for Flexible Probabilistic Inference},
  author = 	 {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1682--1690},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/ge18b/ge18b.pdf},
  url = 	 {https://proceedings.mlr.press/v84/ge18b.html},
}

@article{Dalle2024,
    doi = {10.21105/joss.06436},
    url = {https://doi.org/10.21105/joss.06436},
    year = {2024},
    publisher = {The Open Journal},
    volume = {9},
    number = {96},
    pages = {6436},
    author = {Guillaume Dalle},
    title = {HiddenMarkovModels.jl: generic, fast and reliable state space modeling},
    journal = {Journal of Open Source Software}
}

@article{SaavedraBodinSouto2019,
  title={StateSpaceModels.jl: a Julia Package for Time-Series Analysis in a State-Space Framework},
  author={Raphael Saavedra and Guilherme Bodin and Mario Souto},
  journal={arXiv preprint arXiv:1908.01757},
  year={2019}
}

@software{SSLjl2024,
  title = {StateSpaceLearning.jl},
  author = {Ramos, Andre},
  date = {2024},
  url = {https://github.com/LAMPSPUC/StateSpaceLearning.jl},
  abstract = {StateSpaceLearning.jl is a package for modeling and forecasting time series in a high-dimension regression framework},
  keywords = {SSM}
}

@article{NIPS2011_7143d7fb,
 author = {Macke, Jakob H and Buesing, Lars and Cunningham, John P and Yu, Byron M and Shenoy, Krishna V and Sahani, Maneesh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Empirical models of spiking in neural populations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},
 volume = {24},
 year = {2011}
}

@article{Linderman_Dynamax_A_Python_2025,
author = {Linderman, Scott W. and Chang, Peter and Harper-Donnelly, Giles and Kara, Aleyna and Li, Xinglong and Duran-Martin, Gerardo and Murphy, Kevin},
doi = {10.21105/joss.07069},
journal = {Journal of Open Source Software},
month = apr,
number = {108},
pages = {7069},
title = {{Dynamax: A Python package for probabilistic state space modeling with JAX}},
url = {https://joss.theoj.org/papers/10.21105/joss.07069},
volume = {10},
year = {2025}}

@article{Paninski2010-ns,
  title        = {A new look at state-space models for neural data},
  author       = {Paninski, Liam and Ahmadian, Yashar and Ferreira, Daniel Gil
                  and Koyama, Shinsuke and Rahnama Rad, Kamiar and Vidne,
                  Michael and Vogelstein, Joshua and Wu, Wei},
  doi          = {10.1007/s10827-009-0179-x},
  journaltitle = {J. Comput. Neurosci.},
  publisher    = {Springer Science and Business Media LLC},
  volume       = {29},
  issue        = {1-2},
  pages        = {107--126},
  date         = {2010-08},
  abstract     = {State space methods have proven indispensable in neural data
                  analysis. However, common methods for performing inference in
                  state-space models with non-Gaussian observations rely on
                  certain approximations which are not always accurate. Here we
                  review direct optimization methods that avoid these
                  approximations, but that nonetheless retain the computational
                  efficiency of the approximate methods. We discuss a variety of
                  examples, applying these direct optimization techniques to
                  problems in spike train smoothing, stimulus decoding,
                  parameter estimation, and inference of synaptic properties.
                  Along the way, we point out connections to some related
                  standard statistical methods, including spline smoothing and
                  isotonic regression. Finally, we note that the computational
                  methods reviewed here do not in fact depend on the state-space
                  setting at all; instead, the key property we are exploiting
                  involves the bandedness of certain matrices. We close by
                  discussing some applications of this more general point of
                  view, including Markov chain Monte Carlo methods for neural
                  decoding and efficient estimation of spatially-varying firing
                  rates.},
  urldate      = {2024-09-11},
  language     = {en}
}

@software{SSDjl2024,
  title = {StateSpaceDynamics.jl: A Julia package for probabilistic
state space models},
  author = {Senne, Ryan and Loschinskey, Zachary and Loughridge, Carson  and DePasquale, Brian},
  date = {2025},
  howpublished = {\url{https://github.com/depasquale-lab/StateSpaceDynamics.jl}},
  url = {https://github.com/depasquale-lab/StateSpaceDynamics.jl},
  abstract = {A Julia package for probabilistic state space models},
  keywords = {SSM}
}

@article{bezanson2017julia,
  title={Julia: A fresh approach to numerical computing},
  author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  journal={SIAM review},
  volume={59},
  number={1},
  pages={65--98},
  year={2017},
  doi={10.48550/arXiv.1411.1607},
  publisher={SIAM}
}

@ARTICLE{Linderman2016-xe,
  title        = {Recurrent switching linear dynamical systems},
  author       = {Linderman, Scott W and Miller, Andrew C and Adams, Ryan P and
                  Blei, David M and Paninski, Liam and Johnson, Matthew J},
  journaltitle = {arXiv [stat.ML]},
  date         = {2016-10-26},
  eprintclass  = {stat.ML},
  doi          = {10.48550/arXiv.1610.08466},
  abstract     = {Many natural systems, such as neurons firing in the brain or
                  basketball teams traversing a court, give rise to time series
                  data with complex, nonlinear dynamics. We can gain insight
                  into these systems by decomposing the data into segments that
                  are each explained by simpler dynamic units. Building on
                  switching linear dynamical systems (SLDS), we present a new
                  model class that not only discovers these dynamical units, but
                  also explains how their switching behavior depends on
                  observations or continuous latent states. These "recurrent"
                  switching linear dynamical systems provide further insight by
                  discovering the conditions under which each unit is deployed,
                  something that traditional SLDS models fail to do. We leverage
                  recent algorithmic advances in approximate inference to make
                  Bayesian inference in these models easy, fast, and scalable.}
}

@REPORT{Murphy1998-bk,
  type   = {techreport},
  title  = {Switching Kalman Filters},
  author = {Murphy, Kevin P},
  date   = {1998}
}

@MISC{Rybicki1990-ky,
  title  = {Fast Solution for the Diagonal Elements of the Inverse of a
            Tridiagonal Matrix},
  author = {Rybicki, G B and Hummer, D G},
  date   = {1990}
}

@BOOK{Bishop2006-kv,
  title     = {Pattern Recognition and Machine Learning},
  author    = {Bishop, Christopher M},
  publisher = {Springer},
  location  = {New York, NY},
  date      = {2006-11-01},
  pagetotal = {738},
  series    = {Information Science and Statistics},
  language  = {en}
}

@article{slds,
    author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
    title = {Variational Learning for Switching State-Space Models},
    journal = {Neural Computation},
    volume = {12},
    number = {4},
    pages = {831-864},
    year = {2000},
    month = {04},
    abstract = {We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models—hidden Markov models and linear dynamical systems—and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, \&amp; Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
    issn = {0899-7667},
    doi = {10.1162/089976600300015619},
    url = {https://doi.org/10.1162/089976600300015619},
    eprint = {https://direct.mit.edu/neco/article-pdf/12/4/831/814443/089976600300015619.pdf},
}

@inproceedings{NIPS1994_8065d07d,
 author = {Bengio, Yoshua and Frasconi, Paolo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {G. Tesauro and D. Touretzky and T. Leen},
 pages = {},
 publisher = {MIT Press},
 title = {An Input Output HMM Architecture},
 url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},
 volume = {7},
 year = {1994}
}

@article{Ashwood2022,
  title     = {Mice alternate between discrete strategies during perceptual decision-making},
  author    = {Ashwood, Zoe C. and Roy, Nicholas A. and Stone, Iris R. and Urai, Anne E. and Churchland, Anne K. and Pouget, Alexandre and Pillow, Jonathan W.},
  journal   = {Nature Neuroscience},
  volume    = {25},
  number    = {2},
  pages     = {201--212},
  year      = {2022},
  doi       = {10.1038/s41593-021-01007-z},
  url       = {https://www.nature.com/articles/s41593-021-01007-z}
}

@software{pythoncall,
  title    = {PythonCall.jl},
  author   = {Christopher Doris},
  date     = {2021},
  howpublished = {\url{https://github.com/JuliaPy/PythonCall.jl}}
}

@ARTICLE{BenchmarkTools.jl-2016,
  author =	 {{Chen}, Jiahao and {Revels}, Jarrett},
  title =	 "{Robust benchmarking in noisy environments}",
  journal =	 {arXiv e-prints},
  keywords =	 {Computer Science - Performance, 68N30, B.8.1, D.2.5},
  year =	 2016,
  month =	 "Aug",
  eid =		 {arXiv:1608.04295},
  archivePrefix ={arXiv},
  eprint =	 {1608.04295},
  primaryClass = {cs.PF},
  adsurl =	 {https://ui.adsabs.harvard.edu/abs/2016arXiv160804295C},
  adsnote =	 {Provided by the SAO/NASA Astrophysics Data System}
}

@article{ForwardDiff,
    title = {Forward-Mode Automatic Differentiation in {J}ulia},
   author = {{Revels}, J. and {Lubin}, M. and {Papamarkou}, T.},
  journal = {arXiv:1607.07892 [cs.MS]},
     year = {2016},
      url = {https://arxiv.org/abs/1607.07892}
}

@article{Zygote,
  author       = {Michael Innes},
  title        = {Don't Unroll Adjoint: Differentiating SSA-Form Programs},
  journal      = {CoRR},
  volume       = {abs/1810.07951},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.07951},
  eprinttype    = {arXiv},
  eprint       = {1810.07951},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-07951.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{dalle2025commoninterfaceautomaticdifferentiation,
      title={A Common Interface for Automatic Differentiation},
      author={Guillaume Dalle and Adrian Hill},
      year={2025},
      eprint={2505.05542},
      archivePrefix={arXiv},
      primaryClass={cs.MS},
      url={https://arxiv.org/abs/2505.05542},
}

@InProceedings{pmlr-v119-zoltowski20a,
  title = 	 {A general recurrent state space framework for modeling neural dynamics during decision-making},
  author =       {Zoltowski, David and Pillow, Jonathan and Linderman, Scott},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11680--11691},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zoltowski20a/zoltowski20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zoltowski20a.html},
  abstract = 	 {An open question in systems and computational neuroscience is how neural circuits accumulate evidence towards a decision. Fitting models of decision-making theory to neural activity helps answer this question, but current approaches limit the number of these models that we can fit to neural data. Here we propose a general framework for modeling neural activity during decision-making. The framework includes the canonical drift-diffusion model and enables extensions such as multi-dimensional accumulators, variable and collapsing boundaries, and discrete jumps. Our framework is based on constraining the parameters of recurrent state space models, for which we introduce a scalable variational Laplace EM inference algorithm. We applied the modeling approach to spiking responses recorded from monkey parietal cortex during two decision-making tasks. We found that a two-dimensional accumulator better captured the responses of a set of parietal neurons than a single accumulator model, and we identified a variable lower boundary in the responses of a parietal neuron during a random dot motion task. We expect this framework will be useful for modeling neural dynamics in a variety of decision-making settings.}
}
