@article{Dalle2024, 
  doi = {10.21105/joss.06436}, 
  url = {https://doi.org/10.21105/joss.06436}, 
  year = {2024}, 
  publisher = {The Open Journal}, 
  volume = {9}, 
  number = {96}, 
  pages = {6436}, 
  author = {Guillaume Dalle}, 
  title = {HiddenMarkovModels.jl: generic, fast and reliable state space modeling}, 
  journal = {Journal of Open Source Software}
}

@software{PySSM2022,
  title = {SSM: Bayesian learning and inference for state space models},
  author = {Linderman, Scott},
  date = {2022},
  url = {https://github.com/lindermanlab/ssm},
  abstract = {SSM is a python library for fitting state space models},
  keywords = {SSM}
}

@software{HMMjl2024,
  title = {HiddenMarkovModels.jl},
  author = {Dalle, Guillaume},
  date = {2024},
  url = {https://github.com/gdalle/HiddenMarkovModels.jl},
  abstract = {A Julia package for simulation, inference and learning of Hidden Markov Models},
  keywords = {HMM}
}

@software{SSMjl2024,
  title = {StateSpaceModels.jl},
  author = {{Bodin, Guilherme}, {Saavedra, Raphael}},
  date = {2024},
  url = {https://github.com/LAMPSPUC/StateSpaceModels.jl},
  abstract = {StateSpaceModels.jl is a package for modeling, forecasting, 
    and simulating time series in a state-space framework. Implementations were made based on the book 
    "Time Series Analysis by State Space Methods" (2012) by James Durbin and Siem Jan Koopman. 
    The notation of the variables in the code also follows the book.},
  keywords = {SSM}
}

@software{SSLjl2024,
  title = {StateSpaceLearning.jl},
  author = {Ramos, Andre},
  date = {2024},
  url = {https://github.com/LAMPSPUC/StateSpaceLearning.jl},
  abstract = {StateSpaceLearning.jl is a package for modeling and forecasting time series in a high-dimension regression framework},
  keywords = {SSM}
}

@article{NIPS2011_7143d7fb,
 author = {Macke, Jakob H and Buesing, Lars and Cunningham, John P and Yu, Byron M and Shenoy, Krishna V and Sahani, Maneesh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Empirical models of spiking in neural populations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},
 volume = {24},
 year = {2011}
}

@software{ChangUnknown-wn,
  type        = {software},
  year        = {2024},
  title       = {dynamax: State Space Models library in {JAX}},
  author      = {Chang, Peter and Harper-Donnelly, Giles and Kara, Aleyna and
                 Li, Xinglong and Linderman, Scott and Murphy, Kevin},
  institution = {Github},
  abstract    = {State Space Models library in JAX. Contribute to probml/dynamax
                 development by creating an account on GitHub.},
  language    = {en}
}

@article{Paninski2010-ns,
  title        = {A new look at state-space models for neural data},
  author       = {Paninski, Liam and Ahmadian, Yashar and Ferreira, Daniel Gil
                  and Koyama, Shinsuke and Rahnama Rad, Kamiar and Vidne,
                  Michael and Vogelstein, Joshua and Wu, Wei},
  journaltitle = {J. Comput. Neurosci.},
  publisher    = {Springer Science and Business Media LLC},
  volume       = {29},
  issue        = {1-2},
  pages        = {107--126},
  date         = {2010-08},
  abstract     = {State space methods have proven indispensable in neural data
                  analysis. However, common methods for performing inference in
                  state-space models with non-Gaussian observations rely on
                  certain approximations which are not always accurate. Here we
                  review direct optimization methods that avoid these
                  approximations, but that nonetheless retain the computational
                  efficiency of the approximate methods. We discuss a variety of
                  examples, applying these direct optimization techniques to
                  problems in spike train smoothing, stimulus decoding,
                  parameter estimation, and inference of synaptic properties.
                  Along the way, we point out connections to some related
                  standard statistical methods, including spline smoothing and
                  isotonic regression. Finally, we note that the computational
                  methods reviewed here do not in fact depend on the state-space
                  setting at all; instead, the key property we are exploiting
                  involves the bandedness of certain matrices. We close by
                  discussing some applications of this more general point of
                  view, including Markov chain Monte Carlo methods for neural
                  decoding and efficient estimation of spatially-varying firing
                  rates.},
  urldate      = {2024-09-11},
  language     = {en}
}

@software{SSDjl2024,
  title = {StateSpaceDynamics.jl: A Julia package for probabilistic
state space models},
  author = {Senne, Ryan and Loughridge, Carson and Loschinsky, Zach and DePasquale, Brian},
  date = {2024},
  howpublished = {\url{https://github.com/depasquale-lab/StateSpaceDynamics.jl}},
  url = {https://github.com/depasquale-lab/StateSpaceDynamics.jl},
  abstract = {A Julia package for probabilistic state space models},
  keywords = {SSM}
}

@article{bezanson2017julia,
  title={Julia: A fresh approach to numerical computing},
  author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
  journal={SIAM review},
  volume={59},
  number={1},
  pages={65--98},
  year={2017},
  publisher={SIAM}
}

@ARTICLE{Linderman2016-xe,
  title        = {Recurrent switching linear dynamical systems},
  author       = {Linderman, Scott W and Miller, Andrew C and Adams, Ryan P and
                  Blei, David M and Paninski, Liam and Johnson, Matthew J},
  journaltitle = {arXiv [stat.ML]},
  date         = {2016-10-26},
  eprintclass  = {stat.ML},
  abstract     = {Many natural systems, such as neurons firing in the brain or
                  basketball teams traversing a court, give rise to time series
                  data with complex, nonlinear dynamics. We can gain insight
                  into these systems by decomposing the data into segments that
                  are each explained by simpler dynamic units. Building on
                  switching linear dynamical systems (SLDS), we present a new
                  model class that not only discovers these dynamical units, but
                  also explains how their switching behavior depends on
                  observations or continuous latent states. These "recurrent"
                  switching linear dynamical systems provide further insight by
                  discovering the conditions under which each unit is deployed,
                  something that traditional SLDS models fail to do. We leverage
                  recent algorithmic advances in approximate inference to make
                  Bayesian inference in these models easy, fast, and scalable.}
}

@REPORT{Murphy1998-bk,
  type   = {techreport},
  title  = {Switching Kalman Filters},
  author = {Murphy, Kevin P},
  date   = {1998}
}

@MISC{Rybicki1990-ky,
  title  = {Fast Solution for the Diagonal Elements of the Inverse of a
            Tridiagonal Matrix},
  author = {Rybicki, G B and Hummer, D G},
  date   = {1990}
}

@BOOK{Bishop2006-kv,
  title     = {Pattern Recognition and Machine Learning},
  author    = {Bishop, Christopher M},
  publisher = {Springer},
  location  = {New York, NY},
  date      = {2006-11-01},
  pagetotal = {738},
  series    = {Information Science and Statistics},
  language  = {en}
}
